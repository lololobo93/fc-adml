{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapper de elementos de la página del SSN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero que debemos hacer es obtener el registro de sismos para un determinado periodo. Para eso, vamos a hacer uso de la biblioteca `requests` de Python, y despues de `beautifulsoup4` para hacer el scrapping de la página que se obtiene con `requests`, y con `pandas` un DataFrame para poder escribir todo a un csv y trabajar con él más adelante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests #Para mandar mensajes POST a un URL\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de __perder un buen rato por la falta de documentación del API del SSN__, he encontrado los campos que se deben mandar para obtener información en una tabla de html (nota: el servicio es __superlento__, Así que usaré pequeños rangos de fechas para no saturar la página del api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote backup file 1990\n",
      "Wrote backup file 1991\n",
      "Wrote backup file 1992\n",
      "Wrote backup file 1993\n",
      "Wrote backup file 1994\n",
      "Wrote backup file 1995\n",
      "Wrote backup file 1996\n",
      "Wrote backup file 1997\n",
      "Wrote backup file 1998\n",
      "Wrote backup file 1999\n",
      "Wrote backup file 2000\n",
      "Wrote backup file 2001\n",
      "Wrote backup file 2002\n",
      "Wrote backup file 2003\n",
      "Wrote backup file 2004\n",
      "Wrote backup file 2005\n",
      "Wrote backup file 2006\n",
      "Wrote backup file 2007\n",
      "Wrote backup file 2008\n",
      "Wrote backup file 2009\n",
      "Wrote backup file 2010\n",
      "Wrote backup file 2011\n",
      "Wrote backup file 2012\n",
      "Wrote backup file 2013\n",
      "Wrote backup file 2014\n",
      "Wrote backup file 2015\n",
      "Wrote backup file 2016\n",
      "Wrote backup file 2017\n"
     ]
    }
   ],
   "source": [
    "for year in range(1990,2018):\n",
    "    # Información en formato JSON para ser pedida por POST\n",
    "    mData = {'ano1': str(year),\n",
    "        'ano2': str(year),\n",
    "        'dia1': '1',\n",
    "        'dia2': '31',\n",
    "        'estado': '',\n",
    "        'filtrado': 'estado',\n",
    "        'latmax': '33',\n",
    "        'latmin': '13',\n",
    "        'lonmax': '-85',\n",
    "        'lonmin': '-118',\n",
    "        'magnitud1': '0',\n",
    "        'magnitud2': '9.5',\n",
    "        'mes1': '1',\n",
    "        'mes2': '12',\n",
    "        'submit': 'BUSCAR'}\n",
    "    r = requests.post(\"http://www2.ssn.unam.mx:8080/catalogo/\",data = mData)\n",
    "    with open('htmlb-'+str(year)+'.html','w') as file:\n",
    "        file.write(r.text)\n",
    "        print(\"Wrote backup file \" + str(year))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Añadimos los sismos que van del 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote backup file 2018\n"
     ]
    }
   ],
   "source": [
    "mData = {'ano1': '2018', 'ano2': '2018', 'dia1': '1', 'dia2': '17',\n",
    "        'estado': '', 'filtrado': 'estado', 'latmax': '33', 'latmin': '13',\n",
    "        'lonmax': '-85', 'lonmin': '-118', 'magnitud1': '0', 'magnitud2': '9.5',\n",
    "        'mes1': '1', 'mes2': '2', 'submit': 'BUSCAR'}\n",
    "r = requests.post(\"http://www2.ssn.unam.mx:8080/catalogo/\",data = mData)\n",
    "with open('htmlb-2018.html','w') as file:\n",
    "    file.write(r.text)\n",
    "    print(\"Wrote backup file 2018\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora creamos una _sopa_ con el resultado de nuestro request. Además de eso, de estos archivos vamos a crear una tabla con Pandas. Primero sacamos la tabla de datos del html, cuyos elementos se identifican por ser del tipo 'input'. El SSN pone 10 categorías, por así decirlo: archivo (0), evento (1), fecha (2), hora (3), latitud (4), longitud (5), profundidad (6), magnitud (7) y epicentro (8) (hay dos más, 'regresar' y 'titulo' pero son funciones de la página, no datos). Ahora, de manera no tan compleja, preparamos el DataFrame de Pandas (este código es LEEEEEENTOOOO):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating soup and building DataFrame (2018)\n",
      "Done\n",
      "Creating soup and building DataFrame (2017)\n",
      "Done\n",
      "Creating soup and building DataFrame (2016)\n",
      "Done\n",
      "Creating soup and building DataFrame (2015)\n",
      "Done\n",
      "Creating soup and building DataFrame (2014)\n",
      "Done\n",
      "Creating soup and building DataFrame (2013)\n",
      "Done\n",
      "Creating soup and building DataFrame (2012)\n",
      "Done\n",
      "Creating soup and building DataFrame (2011)\n",
      "Done\n",
      "Creating soup and building DataFrame (2010)\n",
      "Done\n",
      "Creating soup and building DataFrame (2009)\n",
      "Done\n",
      "Creating soup and building DataFrame (2008)\n",
      "Done\n",
      "Creating soup and building DataFrame (2007)\n",
      "Done\n",
      "Creating soup and building DataFrame (2006)\n",
      "Done\n",
      "Creating soup and building DataFrame (2005)\n",
      "Done\n",
      "Creating soup and building DataFrame (2004)\n",
      "Done\n",
      "Creating soup and building DataFrame (2003)\n",
      "Done\n",
      "Creating soup and building DataFrame (2002)\n",
      "Done\n",
      "Creating soup and building DataFrame (2001)\n",
      "Done\n",
      "Creating soup and building DataFrame (2000)\n",
      "Done\n",
      "Creating soup and building DataFrame (1999)\n",
      "Done\n",
      "Creating soup and building DataFrame (1998)\n",
      "Done\n",
      "Creating soup and building DataFrame (1997)\n",
      "Done\n",
      "Creating soup and building DataFrame (1996)\n",
      "Done\n",
      "Creating soup and building DataFrame (1995)\n",
      "Done\n",
      "Creating soup and building DataFrame (1994)\n",
      "Done\n",
      "Creating soup and building DataFrame (1993)\n",
      "Done\n",
      "Creating soup and building DataFrame (1992)\n",
      "Done\n",
      "Creating soup and building DataFrame (1991)\n",
      "Done\n",
      "Creating soup and building DataFrame (1990)\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "resultingDF = pd.DataFrame(columns = ['Magnitud','Latitud','Longitud','Profundidad (km)', 'Fecha y Hora'])\n",
    "for year in range(1990,2019)[::-1]:\n",
    "    with open('htmlb-'+str(year)+'.html','r') as file:\n",
    "        print(\"Creating soup and building DataFrame (\" + str(year) + \")\")\n",
    "        soup = BeautifulSoup(file, 'html5lib')\n",
    "        #con el parámetro {'name':'evento'} se puede pre-filtrar\n",
    "        allTags = soup.find_all('input')\n",
    "\n",
    "        #Ponemos en una lambda una funión para interpretar dos campos como un Timestamp\n",
    "        dateparser = lambda x: pd.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        for i in range(0,len(allTags)//11):\n",
    "            series = {'Magnitud':float(allTags[i*11+7]['value']),\n",
    "                      'Latitud':allTags[i*11+4]['value'],\n",
    "                      'Longitud':allTags[i*11+5]['value'],\n",
    "                      'Profundidad (km)': allTags[i*11+6]['value'],\n",
    "                     'Fecha y Hora': dateparser(allTags[i*11+2]['value'] + \" \" + allTags[i*11+3]['value'])}\n",
    "            resultingDF = resultingDF.append(series, ignore_index=True)\n",
    "        print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último salvamos el Data Frame a un archivo csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resultingDF.to_csv('earthquakes.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
