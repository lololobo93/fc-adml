{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ernesto Carro \n",
      "last updated: 2019-02-09 \n",
      "\n",
      "CPython 3.7.1\n",
      "IPython 7.2.0\n",
      "\n",
      "numpy 1.15.4\n",
      "scipy 1.1.0\n",
      "matplotlib 3.0.2\n",
      "torch 1.0.0\n",
      "sklearn 0.20.1\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a \"Ernesto Carro\" -u -d -v -p numpy,scipy,matplotlib,torch,sklearn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación de galaxias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de esta tarea es hacer una red neuronal que pueda diferenciar entre tres tipos de galaxia: espiral, irregular y elíptica. Se va a hacer por medio de una red convolucional (para disminuír el número de parámetros de la red.\n",
    "\n",
    "El ejercicio está basado en el [tutorial](https://www.tensorflow.org/tutorials/layers) de tensorflow para capas en una red convolutiva.\n",
    "\n",
    "Las fotos de estas galaxias las necesitan sacar del folder zip que se llame 'source' (basta con darle click derecho y luego descomprimir, en realidad.\n",
    "\n",
    "Primero lo primero, cargar las bibliotecas necesarias. Recuerden, si una les aparece que no está instalada, basta con que utilicen el ambiente virtual para instalar cosas. En particular, tensorflow y PIL (cuyo paquete se llama Pillow, es decir, `pip install pillow`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image\n",
    "import os\n",
    "import zipfile\n",
    "import io\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a cargar en RAM todas las fotos. Para ello primero cargamos los nombres y sus tipos (lo único que nos interesa). Me aprovecho de ello para de una vez guardar en un data frame sólo las columnas de interés. No sólo eso pero de una vez vamos a convertir las etiquetas en las tres posibilidades de galaxia que nos interesa clasificar: Elíptica ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('EFIGI_data.txt', sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver aquí los primeros 5 renglones de este dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhotoName</th>\n",
       "      <th>HStage</th>\n",
       "      <th>Type</th>\n",
       "      <th>NType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PGC0009530</td>\n",
       "      <td>10</td>\n",
       "      <td>I</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PGC0025524</td>\n",
       "      <td>-5</td>\n",
       "      <td>E</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PGC0002149</td>\n",
       "      <td>-3</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PGC0004363</td>\n",
       "      <td>-3</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PGC0004540</td>\n",
       "      <td>6</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    PhotoName  HStage Type  NType\n",
       "0  PGC0009530      10    I      2\n",
       "1  PGC0025524      -5    E      0\n",
       "2  PGC0002149      -3    S      1\n",
       "3  PGC0004363      -3    S      1\n",
       "4  PGC0004540       6    S      1"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esperemos tener una distribución masomenos equitativa de cada galaxia. Usando `groupby` podremos verificar esto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhotoName</th>\n",
       "      <th>HStage</th>\n",
       "      <th>NType</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>E</th>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S</th>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      PhotoName  HStage  NType\n",
       "Type                          \n",
       "E           200     200    200\n",
       "I           200     200    200\n",
       "S           200     200    200"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby(\"Type\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pues... Resulta que no. En fin, A la mejor el problema fue dar un rango tan grande a las espirales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HStage\n",
       "-6      14\n",
       "-5     163\n",
       "-4      23\n",
       "-3      18\n",
       "-2      14\n",
       "-1       7\n",
       " 0       6\n",
       " 1      19\n",
       " 2      14\n",
       " 3      18\n",
       " 4      14\n",
       " 5       8\n",
       " 6      31\n",
       " 7      15\n",
       " 8      27\n",
       " 9       9\n",
       " 10    156\n",
       " 11     44\n",
       "Name: Type, dtype: int64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby(\"HStage\").count().Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si pues, un poco más homogeneo... Tenemos aún que cargar 82MB de imágenes en RAM. Esto puede ser un grave problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 65025)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = []\n",
    "def ProcImage(row):\n",
    "    archive = zipfile.ZipFile('source.zip', 'r')\n",
    "    img_data = archive.read(os.path.join('source',row.PhotoName+'.png'))\n",
    "    bytes_io = io.BytesIO(img_data)\n",
    "    return Image.open(bytes_io).convert('L')\n",
    "images = data.apply(lambda row: ProcImage(row), axis = 1)\n",
    "shape = [images[0].height, images[0].width]\n",
    "# To normalized numpy array\n",
    "images = np.array(\n",
    "    list(map(lambda pic: np.reshape(np.array(pic), [1, pic.width * pic.height])[0] , images)), dtype = 'float32'\n",
    ") / 255\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=5,\n",
    "        sharex=True, sharey=True)\n",
    "ax = ax.flatten()\n",
    "to_selec = np.random.randint(600, size = 10)\n",
    "for i in range(10):\n",
    "    img = images[to_selec[i]].reshape(255, 255)\n",
    "    ax[i].imshow(img, cmap='Greys')\n",
    "    ax[i].set_title('Type: ' + data.iloc[i].Type)\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividimos los datos en tres partes,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.iloc[:400].NType.values\n",
    "train_images = images[:400]\n",
    "\n",
    "test_data = data.iloc[400:500].NType.values\n",
    "test_images = images[400:500]\n",
    "\n",
    "validation_data = data.iloc[500:].NType.values\n",
    "validation_images = images[500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder comparar nuestra red neuronal, colocaremos como punto de referencia la precisión obtenida al usar el SVM clasificador de sckit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline classifier using an SVM.\n",
      "52 of 100 values correct.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "def svm_baseline():\n",
    "    clf = svm.SVC(gamma = 'auto')\n",
    "    clf.fit(train_images, train_data)\n",
    "    # test\n",
    "    predictions = [int(a) for a in clf.predict(validation_images)]\n",
    "    num_correct = sum(int(a == y) for a, y in zip(predictions, validation_data))\n",
    "    print(\"Baseline classifier using an SVM.\")\n",
    "    print(\"%s of %s values correct.\" % (num_correct, len(validation_data)))\n",
    "    \n",
    "svm_baseline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que el SVM clasificador tiene una precisión del 52%, record que debemos de romper con nuestra red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "# Parameters\n",
    "params = {'batch_size': 10,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 1, \n",
    "          'pin_memory': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_data = map(\n",
    "    torch.tensor, (train_images, train_data)\n",
    ")\n",
    "train_ds = TensorDataset(train_images, train_data)\n",
    "train_dl = DataLoader(train_ds, **params)\n",
    "\n",
    "test_images, test_data = map(\n",
    "    torch.tensor, (test_images, test_data)\n",
    ")\n",
    "test_ds = TensorDataset(test_images, test_data)\n",
    "test_dl = DataLoader(test_ds, **params)\n",
    "\n",
    "validation_images, validation_data = map(\n",
    "    torch.tensor, (validation_images, validation_data)\n",
    ")\n",
    "validation_ds = TensorDataset(validation_images, validation_data)\n",
    "validation_dl = DataLoader(validation_ds, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos nuestra red neuronal convolutiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 3, 6)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(3 * 125 * 125, 100)\n",
    "        self.fc2 = nn.Linear(100, 30)\n",
    "        self.fc3 = nn.Linear(30, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 255, 255)\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = x.view(-1, 3 * 125* 125)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.softmax(self.fc3(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch import NeuralNetClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetClassifier(\n",
    "    Net,\n",
    "    max_epochs=50,\n",
    "    lr=0.01,\n",
    "    batch_size = 10,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing module because the following parameters were re-set: .\n",
      "Re-initializing optimizer because the following parameters were re-set: .\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0952\u001b[0m       \u001b[32m0.3519\u001b[0m        \u001b[35m1.0929\u001b[0m  0.3778\n",
      "      2        \u001b[36m1.0868\u001b[0m       0.3519        \u001b[35m1.0839\u001b[0m  0.3409\n",
      "      3        \u001b[36m1.0731\u001b[0m       \u001b[32m0.4815\u001b[0m        \u001b[35m1.0699\u001b[0m  0.3343\n",
      "      4        \u001b[36m1.0492\u001b[0m       \u001b[32m0.5370\u001b[0m        \u001b[35m1.0411\u001b[0m  0.3461\n",
      "      5        \u001b[36m0.9975\u001b[0m       \u001b[32m0.5926\u001b[0m        \u001b[35m0.9791\u001b[0m  0.3337\n",
      "      6        \u001b[36m0.9090\u001b[0m       \u001b[32m0.6481\u001b[0m        \u001b[35m0.8899\u001b[0m  0.3326\n",
      "      7        \u001b[36m0.8264\u001b[0m       0.6111        \u001b[35m0.8355\u001b[0m  0.3342\n",
      "      8        \u001b[36m0.7818\u001b[0m       0.6296        \u001b[35m0.8099\u001b[0m  0.3370\n",
      "      9        \u001b[36m0.7501\u001b[0m       \u001b[32m0.6667\u001b[0m        \u001b[35m0.7816\u001b[0m  0.3405\n",
      "     10        \u001b[36m0.7175\u001b[0m       \u001b[32m0.6852\u001b[0m        \u001b[35m0.7622\u001b[0m  0.3348\n",
      "     11        \u001b[36m0.6864\u001b[0m       \u001b[32m0.7222\u001b[0m        \u001b[35m0.7376\u001b[0m  0.3384\n",
      "     12        \u001b[36m0.6556\u001b[0m       0.7222        \u001b[35m0.7094\u001b[0m  0.3399\n",
      "     13        \u001b[36m0.6264\u001b[0m       \u001b[32m0.7593\u001b[0m        \u001b[35m0.6824\u001b[0m  0.3318\n",
      "     14        \u001b[36m0.5963\u001b[0m       \u001b[32m0.7778\u001b[0m        \u001b[35m0.6580\u001b[0m  0.3352\n",
      "     15        \u001b[36m0.5690\u001b[0m       0.7778        \u001b[35m0.6310\u001b[0m  0.3336\n",
      "     16        \u001b[36m0.5430\u001b[0m       0.7593        \u001b[35m0.6117\u001b[0m  0.3342\n",
      "     17        \u001b[36m0.5169\u001b[0m       0.7593        \u001b[35m0.5985\u001b[0m  0.3342\n",
      "     18        \u001b[36m0.4921\u001b[0m       0.7407        \u001b[35m0.5885\u001b[0m  0.3333\n",
      "     19        \u001b[36m0.4703\u001b[0m       0.7407        \u001b[35m0.5800\u001b[0m  0.3331\n",
      "     20        \u001b[36m0.4530\u001b[0m       0.7407        \u001b[35m0.5779\u001b[0m  0.3367\n",
      "     21        \u001b[36m0.4334\u001b[0m       0.7593        0.5829  0.3516\n",
      "     22        \u001b[36m0.4159\u001b[0m       0.7593        0.5827  0.3478\n",
      "     23        \u001b[36m0.3946\u001b[0m       0.7593        0.5905  0.3348\n",
      "     24        \u001b[36m0.3793\u001b[0m       0.7778        0.5995  0.3350\n",
      "     25        \u001b[36m0.3613\u001b[0m       0.7778        0.6019  0.3337\n",
      "     26        \u001b[36m0.3448\u001b[0m       0.7778        0.6063  0.3365\n",
      "     27        \u001b[36m0.3252\u001b[0m       0.7778        0.6089  0.3363\n",
      "     28        \u001b[36m0.3093\u001b[0m       0.7778        0.6037  0.3346\n",
      "     29        \u001b[36m0.2931\u001b[0m       0.7778        0.6066  0.3350\n",
      "     30        \u001b[36m0.2788\u001b[0m       0.7778        0.6064  0.4002\n",
      "     31        \u001b[36m0.2639\u001b[0m       0.7778        0.6047  0.3369\n",
      "     32        \u001b[36m0.2521\u001b[0m       0.7593        0.6125  0.3336\n",
      "     33        \u001b[36m0.2371\u001b[0m       0.7593        0.6156  0.3355\n",
      "     34        \u001b[36m0.2237\u001b[0m       0.7407        0.6231  0.3344\n",
      "     35        \u001b[36m0.2119\u001b[0m       0.7222        0.6265  0.3341\n",
      "     36        \u001b[36m0.1965\u001b[0m       0.7222        0.6337  0.3354\n",
      "     37        \u001b[36m0.1906\u001b[0m       0.7222        0.6453  0.3338\n",
      "     38        \u001b[36m0.1765\u001b[0m       0.7222        0.6423  0.3353\n",
      "     39        \u001b[36m0.1719\u001b[0m       0.7407        0.6374  0.3775\n",
      "     40        \u001b[36m0.1592\u001b[0m       0.7222        0.6383  0.3423\n",
      "     41        \u001b[36m0.1453\u001b[0m       0.7222        0.6417  0.3373\n",
      "     42        \u001b[36m0.1332\u001b[0m       0.7222        0.6401  0.3535\n",
      "     43        \u001b[36m0.1237\u001b[0m       0.7222        0.6469  0.3809\n",
      "     44        \u001b[36m0.1055\u001b[0m       0.7222        0.6473  0.3569\n",
      "     45        0.1200       0.7222        0.6458  0.3373\n",
      "     46        \u001b[36m0.0959\u001b[0m       0.7222        0.6682  0.3461\n",
      "     47        \u001b[36m0.0876\u001b[0m       0.7222        0.6679  0.3370\n",
      "     48        0.1057       0.7222        0.6682  0.3348\n",
      "     49        \u001b[36m0.0824\u001b[0m       0.7222        0.6785  0.4118\n",
      "     50        \u001b[36m0.0817\u001b[0m       0.7222        0.6930  0.3364\n",
      "Re-initializing module because the following parameters were re-set: .\n",
      "Re-initializing optimizer because the following parameters were re-set: .\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0992\u001b[0m       \u001b[32m0.3273\u001b[0m        \u001b[35m1.0741\u001b[0m  0.3344\n",
      "      2        \u001b[36m1.0723\u001b[0m       \u001b[32m0.4364\u001b[0m        \u001b[35m1.0334\u001b[0m  0.3347\n",
      "      3        \u001b[36m1.0371\u001b[0m       \u001b[32m0.4545\u001b[0m        \u001b[35m0.9767\u001b[0m  0.3553\n",
      "      4        \u001b[36m0.9880\u001b[0m       \u001b[32m0.5091\u001b[0m        \u001b[35m0.8932\u001b[0m  0.3958\n",
      "      5        \u001b[36m0.9238\u001b[0m       \u001b[32m0.6545\u001b[0m        \u001b[35m0.7953\u001b[0m  0.3405\n",
      "      6        \u001b[36m0.8579\u001b[0m       0.6545        \u001b[35m0.7164\u001b[0m  0.3415\n",
      "      7        \u001b[36m0.8084\u001b[0m       0.6545        \u001b[35m0.6772\u001b[0m  0.3421\n",
      "      8        \u001b[36m0.7692\u001b[0m       \u001b[32m0.6727\u001b[0m        \u001b[35m0.6643\u001b[0m  0.3376\n",
      "      9        \u001b[36m0.7323\u001b[0m       \u001b[32m0.7091\u001b[0m        \u001b[35m0.6510\u001b[0m  0.3404\n",
      "     10        \u001b[36m0.6954\u001b[0m       \u001b[32m0.7273\u001b[0m        \u001b[35m0.6305\u001b[0m  0.3364\n",
      "     11        \u001b[36m0.6569\u001b[0m       0.7273        \u001b[35m0.6082\u001b[0m  0.3760\n",
      "     12        \u001b[36m0.6195\u001b[0m       \u001b[32m0.7455\u001b[0m        \u001b[35m0.5884\u001b[0m  0.3801\n",
      "     13        \u001b[36m0.5853\u001b[0m       0.7455        \u001b[35m0.5525\u001b[0m  0.3416\n",
      "     14        \u001b[36m0.5559\u001b[0m       \u001b[32m0.8000\u001b[0m        \u001b[35m0.5362\u001b[0m  0.3419\n",
      "     15        \u001b[36m0.5291\u001b[0m       0.8000        \u001b[35m0.5264\u001b[0m  0.4009\n",
      "     16        \u001b[36m0.5055\u001b[0m       0.8000        \u001b[35m0.5231\u001b[0m  0.3647\n",
      "     17        \u001b[36m0.4853\u001b[0m       0.8000        0.5234  0.3425\n",
      "     18        \u001b[36m0.4660\u001b[0m       \u001b[32m0.8182\u001b[0m        0.5237  0.3656\n",
      "     19        \u001b[36m0.4492\u001b[0m       0.8182        0.5254  0.3537\n",
      "     20        \u001b[36m0.4327\u001b[0m       0.8182        0.5285  0.3543\n",
      "     21        \u001b[36m0.4179\u001b[0m       0.8182        0.5325  0.3946\n",
      "     22        \u001b[36m0.4039\u001b[0m       0.8182        0.5365  0.3548\n",
      "     23        \u001b[36m0.3918\u001b[0m       0.8182        0.5409  0.4112\n",
      "     24        \u001b[36m0.3777\u001b[0m       0.8182        0.5461  0.4024\n",
      "     25        \u001b[36m0.3664\u001b[0m       0.8182        0.5527  0.4026\n",
      "     26        \u001b[36m0.3542\u001b[0m       0.8182        0.5557  0.4028\n",
      "     27        \u001b[36m0.3429\u001b[0m       0.8182        0.5600  0.3580\n",
      "     28        \u001b[36m0.3319\u001b[0m       0.8000        0.5653  0.3370\n",
      "     29        \u001b[36m0.3211\u001b[0m       0.7818        0.5708  0.3401\n",
      "     30        \u001b[36m0.3113\u001b[0m       0.7636        0.5756  0.3369\n",
      "     31        \u001b[36m0.3002\u001b[0m       0.7636        0.5801  0.3361\n",
      "     32        \u001b[36m0.2909\u001b[0m       0.7455        0.5826  0.3372\n",
      "     33        \u001b[36m0.2792\u001b[0m       0.7273        0.5864  0.3360\n",
      "     34        \u001b[36m0.2707\u001b[0m       0.7273        0.5901  0.3349\n",
      "     35        \u001b[36m0.2602\u001b[0m       0.7273        0.5913  0.3388\n",
      "     36        \u001b[36m0.2502\u001b[0m       0.7455        0.5941  0.3377\n",
      "     37        \u001b[36m0.2422\u001b[0m       0.7818        0.5983  0.3360\n",
      "     38        \u001b[36m0.2336\u001b[0m       0.7818        0.6018  0.3339\n",
      "     39        \u001b[36m0.2255\u001b[0m       0.7636        0.6036  0.3334\n",
      "     40        \u001b[36m0.2147\u001b[0m       0.7636        0.6075  0.3488\n",
      "     41        \u001b[36m0.2070\u001b[0m       0.7636        0.6113  0.3475\n",
      "     42        \u001b[36m0.1970\u001b[0m       0.7636        0.6149  0.3429\n",
      "     43        \u001b[36m0.1872\u001b[0m       0.7636        0.6188  0.3416\n",
      "     44        \u001b[36m0.1782\u001b[0m       0.7636        0.6243  0.3369\n",
      "     45        \u001b[36m0.1671\u001b[0m       0.7636        0.6324  0.3354\n",
      "     46        \u001b[36m0.1588\u001b[0m       0.7636        0.6437  0.3336\n",
      "     47        \u001b[36m0.1462\u001b[0m       0.7818        0.6518  0.3351\n",
      "     48        \u001b[36m0.1378\u001b[0m       0.8000        0.6620  0.3338\n",
      "     49        \u001b[36m0.1238\u001b[0m       0.8000        0.6745  0.3389\n",
      "     50        \u001b[36m0.1163\u001b[0m       0.8000        0.6854  0.3406\n",
      "Re-initializing module because the following parameters were re-set: .\n",
      "Re-initializing optimizer because the following parameters were re-set: .\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0811\u001b[0m       \u001b[32m0.5556\u001b[0m        \u001b[35m1.0358\u001b[0m  0.3405\n",
      "      2        \u001b[36m1.0207\u001b[0m       0.5556        \u001b[35m0.9372\u001b[0m  0.3417\n",
      "      3        \u001b[36m0.9380\u001b[0m       \u001b[32m0.6296\u001b[0m        \u001b[35m0.8040\u001b[0m  0.3408\n",
      "      4        \u001b[36m0.8592\u001b[0m       \u001b[32m0.6852\u001b[0m        \u001b[35m0.7113\u001b[0m  0.3396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        \u001b[36m0.8115\u001b[0m       0.6852        \u001b[35m0.6964\u001b[0m  0.3369\n",
      "      6        \u001b[36m0.7793\u001b[0m       0.6111        0.7534  0.3350\n",
      "      7        \u001b[36m0.7492\u001b[0m       0.6296        0.7532  0.3343\n",
      "      8        \u001b[36m0.7182\u001b[0m       0.6852        0.7477  0.3352\n",
      "      9        \u001b[36m0.6936\u001b[0m       \u001b[32m0.7037\u001b[0m        0.7342  0.3333\n",
      "     10        \u001b[36m0.6649\u001b[0m       \u001b[32m0.7222\u001b[0m        0.7166  0.3337\n",
      "     11        \u001b[36m0.6392\u001b[0m       0.7222        0.7091  0.3338\n",
      "     12        \u001b[36m0.6150\u001b[0m       0.7222        0.7055  0.3337\n",
      "     13        \u001b[36m0.5918\u001b[0m       0.7037        0.7062  0.3344\n",
      "     14        \u001b[36m0.5748\u001b[0m       0.7037        0.7105  0.3329\n",
      "     15        \u001b[36m0.5524\u001b[0m       0.7037        0.7006  0.3331\n",
      "     16        \u001b[36m0.5293\u001b[0m       0.6852        0.6970  0.3334\n",
      "     17        \u001b[36m0.5109\u001b[0m       0.6852        \u001b[35m0.6813\u001b[0m  0.3339\n",
      "     18        \u001b[36m0.4956\u001b[0m       0.6852        0.6857  0.3353\n",
      "     19        \u001b[36m0.4739\u001b[0m       0.6852        0.6995  0.3356\n",
      "     20        \u001b[36m0.4606\u001b[0m       0.6852        0.6872  0.3349\n",
      "     21        \u001b[36m0.4443\u001b[0m       0.6852        \u001b[35m0.6727\u001b[0m  0.3342\n",
      "     22        \u001b[36m0.4297\u001b[0m       0.6852        0.6913  0.3349\n",
      "     23        \u001b[36m0.4094\u001b[0m       0.7222        0.6781  0.3346\n",
      "     24        \u001b[36m0.3889\u001b[0m       0.7222        \u001b[35m0.6659\u001b[0m  0.3340\n",
      "     25        \u001b[36m0.3771\u001b[0m       0.7222        \u001b[35m0.6598\u001b[0m  0.3341\n",
      "     26        \u001b[36m0.3578\u001b[0m       0.7222        0.6612  0.3359\n",
      "     27        \u001b[36m0.3435\u001b[0m       0.7037        0.6628  0.3345\n",
      "     28        \u001b[36m0.3392\u001b[0m       0.7037        0.6695  0.3336\n",
      "     29        \u001b[36m0.3169\u001b[0m       0.7037        0.6767  0.3358\n",
      "     30        \u001b[36m0.3041\u001b[0m       0.7037        0.6882  0.3347\n",
      "     31        \u001b[36m0.2884\u001b[0m       0.7037        0.7021  0.3343\n",
      "     32        \u001b[36m0.2781\u001b[0m       0.7037        0.7017  0.3351\n",
      "     33        \u001b[36m0.2632\u001b[0m       0.7222        0.7254  0.3350\n",
      "     34        \u001b[36m0.2557\u001b[0m       0.7222        0.7502  0.3357\n",
      "     35        \u001b[36m0.2312\u001b[0m       \u001b[32m0.7593\u001b[0m        1.1070  0.3344\n",
      "     36        0.2846       0.7407        0.8224  0.3350\n",
      "     37        0.2331       0.7407        1.1600  0.3354\n",
      "     38        0.2477       0.7407        1.0974  0.3359\n",
      "     39        \u001b[36m0.2241\u001b[0m       0.7037        0.7690  0.3359\n",
      "     40        \u001b[36m0.2105\u001b[0m       0.6852        0.9434  0.3375\n",
      "     41        \u001b[36m0.1966\u001b[0m       0.7407        1.2765  0.3547\n",
      "     42        0.2036       0.7222        1.3871  0.3563\n",
      "     43        \u001b[36m0.1841\u001b[0m       0.7037        1.1306  0.3530\n",
      "     44        0.2042       0.7037        1.1398  0.3458\n",
      "     45        \u001b[36m0.1648\u001b[0m       0.7037        1.0603  0.3393\n",
      "     46        0.1667       0.7037        1.1018  0.3368\n",
      "     47        \u001b[36m0.1596\u001b[0m       0.7037        1.1125  0.3505\n",
      "     48        \u001b[36m0.1448\u001b[0m       0.7037        1.1139  0.3359\n",
      "     49        0.1464       0.6667        1.1803  0.3401\n",
      "     50        \u001b[36m0.1306\u001b[0m       0.6852        1.1724  0.3387\n",
      "Re-initializing module because the following parameters were re-set: .\n",
      "Re-initializing optimizer because the following parameters were re-set: .\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1061\u001b[0m       \u001b[32m0.3148\u001b[0m        \u001b[35m1.0971\u001b[0m  0.3336\n",
      "      2        \u001b[36m1.0813\u001b[0m       \u001b[32m0.5185\u001b[0m        \u001b[35m1.0630\u001b[0m  0.3345\n",
      "      3        \u001b[36m1.0149\u001b[0m       \u001b[32m0.6111\u001b[0m        \u001b[35m0.9684\u001b[0m  0.3341\n",
      "      4        \u001b[36m0.8951\u001b[0m       0.5926        \u001b[35m0.8472\u001b[0m  0.3347\n",
      "      5        \u001b[36m0.8225\u001b[0m       \u001b[32m0.6481\u001b[0m        \u001b[35m0.8383\u001b[0m  0.3347\n",
      "      6        \u001b[36m0.7739\u001b[0m       \u001b[32m0.7222\u001b[0m        \u001b[35m0.7798\u001b[0m  0.3338\n",
      "      7        \u001b[36m0.7208\u001b[0m       \u001b[32m0.7593\u001b[0m        \u001b[35m0.7146\u001b[0m  0.3343\n",
      "      8        \u001b[36m0.6791\u001b[0m       \u001b[32m0.7963\u001b[0m        \u001b[35m0.6971\u001b[0m  0.3348\n",
      "      9        \u001b[36m0.6386\u001b[0m       0.7407        \u001b[35m0.6886\u001b[0m  0.3341\n",
      "     10        \u001b[36m0.6026\u001b[0m       0.7407        \u001b[35m0.6800\u001b[0m  0.3348\n",
      "     11        \u001b[36m0.5638\u001b[0m       0.7593        \u001b[35m0.6678\u001b[0m  0.3352\n",
      "     12        \u001b[36m0.5207\u001b[0m       0.7593        0.6729  0.3345\n",
      "     13        \u001b[36m0.4934\u001b[0m       0.7407        \u001b[35m0.6615\u001b[0m  0.3334\n",
      "     14        \u001b[36m0.4686\u001b[0m       0.7593        0.6688  0.3429\n",
      "     15        \u001b[36m0.4391\u001b[0m       0.7593        0.6631  0.3402\n",
      "     16        \u001b[36m0.4124\u001b[0m       0.7593        \u001b[35m0.6594\u001b[0m  0.3398\n",
      "     17        \u001b[36m0.4012\u001b[0m       0.7222        \u001b[35m0.6580\u001b[0m  0.3407\n",
      "     18        \u001b[36m0.3742\u001b[0m       0.7593        \u001b[35m0.6429\u001b[0m  0.3361\n",
      "     19        \u001b[36m0.3402\u001b[0m       0.7593        \u001b[35m0.6392\u001b[0m  0.3369\n",
      "     20        \u001b[36m0.3105\u001b[0m       0.7407        \u001b[35m0.6338\u001b[0m  0.3346\n",
      "     21        \u001b[36m0.2801\u001b[0m       0.7593        \u001b[35m0.6102\u001b[0m  0.3342\n",
      "     22        \u001b[36m0.2612\u001b[0m       0.7407        0.7040  0.3343\n",
      "     23        \u001b[36m0.2271\u001b[0m       0.7407        0.6260  0.3399\n",
      "     24        \u001b[36m0.2050\u001b[0m       0.7407        0.6297  0.3398\n",
      "     25        0.2077       0.7222        0.6465  0.3379\n",
      "     26        0.2488       0.7593        0.6626  0.3383\n",
      "     27        \u001b[36m0.1681\u001b[0m       0.7963        \u001b[35m0.5779\u001b[0m  0.3399\n",
      "     28        \u001b[36m0.1582\u001b[0m       0.7407        0.6580  0.3383\n",
      "     29        \u001b[36m0.1332\u001b[0m       0.7778        0.6413  0.3414\n",
      "     30        \u001b[36m0.1220\u001b[0m       0.7778        \u001b[35m0.5772\u001b[0m  0.3516\n",
      "     31        0.1937       0.7407        0.6397  0.3546\n",
      "     32        \u001b[36m0.1162\u001b[0m       0.7407        0.6556  0.3441\n",
      "     33        \u001b[36m0.1156\u001b[0m       0.7407        0.6381  0.3456\n",
      "     34        0.1198       0.7593        0.6441  0.3397\n",
      "     35        \u001b[36m0.1099\u001b[0m       0.7593        0.6454  0.3404\n",
      "     36        \u001b[36m0.1023\u001b[0m       0.7407        0.6600  0.3422\n",
      "     37        \u001b[36m0.0947\u001b[0m       0.7593        0.6490  0.3927\n",
      "     38        \u001b[36m0.0796\u001b[0m       0.7593        0.6431  0.3626\n",
      "     39        \u001b[36m0.0725\u001b[0m       0.7407        0.6754  0.3451\n",
      "     40        0.0772       0.7037        0.6717  0.3526\n",
      "     41        0.0858       0.7222        0.6576  0.3466\n",
      "     42        \u001b[36m0.0565\u001b[0m       0.7407        0.7077  0.3472\n",
      "     43        0.0616       0.7407        0.7292  0.3966\n",
      "     44        \u001b[36m0.0492\u001b[0m       0.7407        0.7665  0.3442\n",
      "     45        \u001b[36m0.0396\u001b[0m       0.7222        0.7691  0.3660\n",
      "     46        \u001b[36m0.0292\u001b[0m       0.7222        0.7636  0.3392\n",
      "     47        0.1956       0.7037        0.7714  0.3587\n",
      "     48        0.1412       0.6667        0.7251  0.3571\n",
      "     49        0.0764       0.6852        0.7171  0.3422\n",
      "     50        0.0464       0.7222        0.7586  0.3660\n",
      "Re-initializing module because the following parameters were re-set: .\n",
      "Re-initializing optimizer because the following parameters were re-set: .\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0995\u001b[0m       \u001b[32m0.3273\u001b[0m        \u001b[35m1.0761\u001b[0m  0.3373\n",
      "      2        \u001b[36m1.0474\u001b[0m       \u001b[32m0.6000\u001b[0m        \u001b[35m0.9543\u001b[0m  0.3617\n",
      "      3        \u001b[36m0.9501\u001b[0m       \u001b[32m0.6182\u001b[0m        \u001b[35m0.8017\u001b[0m  0.3395\n",
      "      4        \u001b[36m0.8639\u001b[0m       0.6000        \u001b[35m0.7952\u001b[0m  0.3767\n",
      "      5        \u001b[36m0.7883\u001b[0m       0.6000        \u001b[35m0.7726\u001b[0m  0.3679\n",
      "      6        \u001b[36m0.7075\u001b[0m       \u001b[32m0.7273\u001b[0m        \u001b[35m0.6490\u001b[0m  0.3660\n",
      "      7        \u001b[36m0.6563\u001b[0m       0.7273        \u001b[35m0.5803\u001b[0m  0.3610\n",
      "      8        \u001b[36m0.6045\u001b[0m       \u001b[32m0.8000\u001b[0m        \u001b[35m0.5692\u001b[0m  0.3628\n",
      "      9        \u001b[36m0.5776\u001b[0m       0.7455        \u001b[35m0.5602\u001b[0m  0.3643\n",
      "     10        \u001b[36m0.5315\u001b[0m       0.7818        \u001b[35m0.5600\u001b[0m  0.3630\n",
      "     11        \u001b[36m0.5010\u001b[0m       0.7818        0.5625  0.3569\n",
      "     12        \u001b[36m0.4729\u001b[0m       0.8000        0.5663  0.3617\n",
      "     13        \u001b[36m0.4526\u001b[0m       \u001b[32m0.8182\u001b[0m        0.5675  0.3777\n",
      "     14        \u001b[36m0.4275\u001b[0m       0.8182        0.5712  0.3678\n",
      "     15        \u001b[36m0.4090\u001b[0m       0.8000        0.5796  0.3477\n",
      "     16        \u001b[36m0.3967\u001b[0m       0.8000        0.5826  0.3390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17        \u001b[36m0.3749\u001b[0m       0.8000        0.5905  0.3395\n",
      "     18        0.3792       0.8000        0.6067  0.3395\n",
      "     19        \u001b[36m0.3642\u001b[0m       0.8000        0.5998  0.3396\n",
      "     20        \u001b[36m0.3329\u001b[0m       0.8000        0.6154  0.3392\n",
      "     21        \u001b[36m0.3074\u001b[0m       0.7818        0.6181  0.3383\n",
      "     22        \u001b[36m0.2837\u001b[0m       0.8000        0.6239  0.3395\n",
      "     23        \u001b[36m0.2802\u001b[0m       0.7818        0.6198  0.3388\n",
      "     24        \u001b[36m0.2564\u001b[0m       0.7818        0.6228  0.3387\n",
      "     25        \u001b[36m0.2432\u001b[0m       0.7636        0.6424  0.3398\n",
      "     26        \u001b[36m0.2252\u001b[0m       0.7455        0.6507  0.3395\n",
      "     27        0.2471       0.7455        0.6199  0.3387\n",
      "     28        \u001b[36m0.1918\u001b[0m       0.7455        0.6419  0.3466\n",
      "     29        0.1921       0.7455        0.6534  0.3468\n",
      "     30        0.2229       0.7636        0.6166  0.3366\n",
      "     31        \u001b[36m0.1827\u001b[0m       0.7636        0.6301  0.3359\n",
      "     32        \u001b[36m0.1484\u001b[0m       0.7636        0.6744  0.3358\n",
      "     33        \u001b[36m0.1366\u001b[0m       0.7455        0.7019  0.3366\n",
      "     34        0.2202       0.7818        0.6636  0.3354\n",
      "     35        0.1544       0.7455        0.6822  0.3353\n",
      "     36        \u001b[36m0.1181\u001b[0m       0.7455        0.7513  0.3345\n",
      "     37        \u001b[36m0.0990\u001b[0m       0.7636        0.7682  0.3364\n",
      "     38        0.1990       0.7636        0.8370  0.3350\n",
      "     39        0.1864       0.7455        0.7638  0.3366\n",
      "     40        0.1153       0.7636        0.7628  0.3355\n",
      "     41        \u001b[36m0.0807\u001b[0m       0.7636        0.7889  0.3356\n",
      "     42        \u001b[36m0.0654\u001b[0m       0.7636        0.8210  0.3368\n",
      "     43        \u001b[36m0.0631\u001b[0m       0.7818        0.8466  0.3408\n",
      "     44        \u001b[36m0.0563\u001b[0m       0.7636        0.8578  0.3390\n",
      "     45        0.0642       0.7818        0.8560  0.3393\n",
      "     46        \u001b[36m0.0538\u001b[0m       0.7636        0.8719  0.3387\n",
      "     47        \u001b[36m0.0536\u001b[0m       0.7818        0.8839  0.3392\n",
      "     48        \u001b[36m0.0474\u001b[0m       0.7636        0.9054  0.3354\n",
      "     49        \u001b[36m0.0417\u001b[0m       0.7636        0.9183  0.3365\n",
      "     50        \u001b[36m0.0396\u001b[0m       0.7636        0.9206  0.3367\n",
      "Re-initializing module because the following parameters were re-set: .\n",
      "Re-initializing optimizer because the following parameters were re-set: .\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1018\u001b[0m       \u001b[32m0.4815\u001b[0m        \u001b[35m1.0885\u001b[0m  0.3437\n",
      "      2        \u001b[36m1.0920\u001b[0m       0.3333        \u001b[35m1.0786\u001b[0m  0.3531\n",
      "      3        \u001b[36m1.0846\u001b[0m       0.3333        \u001b[35m1.0664\u001b[0m  0.3513\n",
      "      4        \u001b[36m1.0743\u001b[0m       0.3333        \u001b[35m1.0488\u001b[0m  0.3508\n",
      "      5        \u001b[36m1.0588\u001b[0m       0.3333        \u001b[35m1.0242\u001b[0m  0.3511\n",
      "      6        \u001b[36m1.0361\u001b[0m       0.3889        \u001b[35m0.9903\u001b[0m  0.3476\n",
      "      7        \u001b[36m1.0033\u001b[0m       \u001b[32m0.5370\u001b[0m        \u001b[35m0.9409\u001b[0m  0.3475\n",
      "      8        \u001b[36m0.9621\u001b[0m       \u001b[32m0.6111\u001b[0m        \u001b[35m0.8648\u001b[0m  0.3471\n",
      "      9        \u001b[36m0.9188\u001b[0m       \u001b[32m0.6296\u001b[0m        \u001b[35m0.7942\u001b[0m  0.3483\n",
      "     10        \u001b[36m0.8827\u001b[0m       0.6296        \u001b[35m0.7496\u001b[0m  0.3480\n",
      "     11        \u001b[36m0.8498\u001b[0m       0.6296        \u001b[35m0.7222\u001b[0m  0.3411\n",
      "     12        \u001b[36m0.8114\u001b[0m       \u001b[32m0.6481\u001b[0m        \u001b[35m0.7009\u001b[0m  0.3361\n",
      "     13        \u001b[36m0.7690\u001b[0m       \u001b[32m0.7037\u001b[0m        \u001b[35m0.6744\u001b[0m  0.3354\n",
      "     14        \u001b[36m0.7348\u001b[0m       0.7037        \u001b[35m0.6644\u001b[0m  0.3357\n",
      "     15        \u001b[36m0.6998\u001b[0m       \u001b[32m0.7222\u001b[0m        \u001b[35m0.6411\u001b[0m  0.3405\n",
      "     16        \u001b[36m0.6652\u001b[0m       0.7222        \u001b[35m0.6251\u001b[0m  0.3401\n",
      "     17        \u001b[36m0.6275\u001b[0m       \u001b[32m0.7407\u001b[0m        \u001b[35m0.6032\u001b[0m  0.3397\n",
      "     18        \u001b[36m0.6024\u001b[0m       0.7407        0.6118  0.3354\n",
      "     19        \u001b[36m0.5664\u001b[0m       0.7407        0.6070  0.3347\n",
      "     20        \u001b[36m0.5651\u001b[0m       0.7222        0.6138  0.3405\n",
      "     21        \u001b[36m0.5390\u001b[0m       0.7407        0.6065  0.3416\n",
      "     22        \u001b[36m0.5136\u001b[0m       0.7222        0.6162  0.3429\n",
      "     23        \u001b[36m0.4859\u001b[0m       0.7407        0.6266  0.3437\n",
      "     24        0.7064       0.5741        0.7408  0.3428\n",
      "     25        0.5719       0.7407        \u001b[35m0.5882\u001b[0m  0.3397\n",
      "     26        \u001b[36m0.4542\u001b[0m       0.7037        0.6810  0.3393\n",
      "     27        \u001b[36m0.4071\u001b[0m       0.7222        0.6520  0.3381\n",
      "     28        \u001b[36m0.3822\u001b[0m       0.7407        0.6725  0.3355\n",
      "     29        0.4525       \u001b[32m0.7593\u001b[0m        0.5971  0.3349\n",
      "     30        \u001b[36m0.3662\u001b[0m       0.7407        0.8416  0.3351\n",
      "     31        \u001b[36m0.3546\u001b[0m       0.7222        0.7763  0.3349\n",
      "     32        0.3874       0.7037        1.0696  0.3337\n",
      "     33        \u001b[36m0.3327\u001b[0m       0.7222        0.8428  0.3351\n",
      "     34        \u001b[36m0.3029\u001b[0m       0.7037        0.7437  0.3343\n",
      "     35        \u001b[36m0.2975\u001b[0m       0.7037        1.1192  0.3355\n",
      "     36        \u001b[36m0.2887\u001b[0m       0.7222        1.0875  0.3340\n",
      "     37        0.3034       0.7407        0.9164  0.3336\n",
      "     38        \u001b[36m0.2541\u001b[0m       0.6852        1.4224  0.3454\n",
      "     39        0.2737       0.7037        1.0718  0.3503\n",
      "     40        0.3836       0.7407        0.7070  0.3485\n",
      "     41        0.2973       0.7407        0.9426  0.3483\n",
      "     42        \u001b[36m0.2406\u001b[0m       0.7407        1.1362  0.3374\n",
      "     43        \u001b[36m0.2273\u001b[0m       0.7037        1.1658  0.3359\n",
      "     44        \u001b[36m0.2108\u001b[0m       0.7222        1.1462  0.3353\n",
      "     45        \u001b[36m0.2046\u001b[0m       0.7037        1.3172  0.3418\n",
      "     46        0.2398       0.6852        0.8801  0.3360\n",
      "     47        \u001b[36m0.2020\u001b[0m       0.7222        1.0988  0.3351\n",
      "     48        \u001b[36m0.1859\u001b[0m       0.7222        1.1368  0.3357\n",
      "     49        \u001b[36m0.1679\u001b[0m       0.7222        1.4815  0.3356\n",
      "     50        0.1972       0.5926        1.1233  0.3361\n",
      "Re-initializing module because the following parameters were re-set: .\n",
      "Re-initializing optimizer because the following parameters were re-set: .\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0567\u001b[0m       \u001b[32m0.5741\u001b[0m        \u001b[35m0.9391\u001b[0m  0.3331\n",
      "      2        \u001b[36m0.9891\u001b[0m       \u001b[32m0.5926\u001b[0m        0.9402  0.3335\n",
      "      3        \u001b[36m0.8815\u001b[0m       0.5926        \u001b[35m0.8145\u001b[0m  0.3348\n",
      "      4        \u001b[36m0.8320\u001b[0m       0.5741        \u001b[35m0.8129\u001b[0m  0.3357\n",
      "      5        1.0135       \u001b[32m0.6481\u001b[0m        \u001b[35m0.7989\u001b[0m  0.3331\n",
      "      6        \u001b[36m0.7758\u001b[0m       0.6481        \u001b[35m0.7511\u001b[0m  0.3330\n",
      "      7        \u001b[36m0.7313\u001b[0m       0.6296        \u001b[35m0.7361\u001b[0m  0.3332\n",
      "      8        \u001b[36m0.6773\u001b[0m       0.6481        \u001b[35m0.7054\u001b[0m  0.3352\n",
      "      9        0.6909       0.6481        \u001b[35m0.6732\u001b[0m  0.3336\n",
      "     10        \u001b[36m0.5676\u001b[0m       0.5926        0.9498  0.3329\n",
      "     11        0.6175       0.6111        0.8860  0.3330\n",
      "     12        0.5806       0.5926        0.9096  0.3471\n",
      "     13        \u001b[36m0.5280\u001b[0m       0.6296        1.0244  0.3424\n",
      "     14        0.5411       0.6296        0.7359  0.3400\n",
      "     15        0.5658       0.6111        0.9387  0.3447\n",
      "     16        \u001b[36m0.5007\u001b[0m       0.6481        0.7666  0.3397\n",
      "     17        \u001b[36m0.4478\u001b[0m       0.6296        0.8805  0.3408\n",
      "     18        0.4698       \u001b[32m0.6667\u001b[0m        0.8121  0.3402\n",
      "     19        \u001b[36m0.4417\u001b[0m       0.5926        1.0728  0.3401\n",
      "     20        \u001b[36m0.3914\u001b[0m       0.6296        0.9606  0.3394\n",
      "     21        \u001b[36m0.3581\u001b[0m       0.5926        0.9919  0.3397\n",
      "     22        \u001b[36m0.3388\u001b[0m       0.5926        1.7851  0.3395\n",
      "     23        0.4038       0.6667        0.9477  0.3359\n",
      "     24        \u001b[36m0.2661\u001b[0m       0.5926        1.5468  0.3341\n",
      "     25        0.2932       0.5556        1.6523  0.3344\n",
      "     26        0.2945       0.5926        1.2701  0.3339\n",
      "     27        0.3280       \u001b[32m0.7037\u001b[0m        0.7718  0.3340\n",
      "     28        \u001b[36m0.2464\u001b[0m       0.7037        0.8008  0.3336\n",
      "     29        \u001b[36m0.1956\u001b[0m       0.6296        1.4030  0.3336\n",
      "     30        0.2119       0.6296        1.4439  0.3339\n",
      "     31        \u001b[36m0.1756\u001b[0m       0.6296        1.5962  0.3328\n",
      "     32        0.1786       0.6481        1.1134  0.3333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     33        1.2248       0.3148        1.2386  0.3339\n",
      "     34        1.0211       \u001b[32m0.7222\u001b[0m        1.0592  0.3329\n",
      "     35        0.4108       0.5926        1.2880  0.3319\n",
      "     36        0.2500       0.7222        \u001b[35m0.6552\u001b[0m  0.3316\n",
      "     37        \u001b[36m0.1719\u001b[0m       0.6852        0.7814  0.3326\n",
      "     38        \u001b[36m0.1711\u001b[0m       \u001b[32m0.7593\u001b[0m        0.7429  0.3322\n",
      "     39        \u001b[36m0.1578\u001b[0m       0.7037        0.7868  0.3321\n",
      "     40        \u001b[36m0.1475\u001b[0m       0.7593        0.7996  0.3319\n",
      "     41        0.1547       0.7222        0.8231  0.3313\n",
      "     42        \u001b[36m0.1375\u001b[0m       0.7222        0.8564  0.3319\n",
      "     43        0.2299       0.3148        2.2362  0.3456\n",
      "     44        0.7863       0.5556        1.7682  0.3451\n",
      "     45        0.5312       0.5926        1.3714  0.3385\n",
      "     46        1.6518       0.3148        1.1517  0.3423\n",
      "     47        1.1205       0.3148        1.1475  0.3784\n",
      "     48        1.1198       0.3148        1.1441  0.3376\n",
      "     49        1.1192       0.3148        1.1409  0.3439\n",
      "     50        1.1186       0.3148        1.1381  0.3387\n",
      "Re-initializing module because the following parameters were re-set: .\n",
      "Re-initializing optimizer because the following parameters were re-set: .\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1053\u001b[0m       \u001b[32m0.5273\u001b[0m        \u001b[35m1.0550\u001b[0m  0.3337\n",
      "      2        \u001b[36m1.0410\u001b[0m       \u001b[32m0.6545\u001b[0m        \u001b[35m0.8360\u001b[0m  0.3407\n",
      "      3        \u001b[36m0.9746\u001b[0m       0.5818        0.8389  0.3410\n",
      "      4        \u001b[36m0.8672\u001b[0m       0.6364        \u001b[35m0.6984\u001b[0m  0.3668\n",
      "      5        0.8738       0.6545        0.7781  0.3548\n",
      "      6        \u001b[36m0.8027\u001b[0m       \u001b[32m0.6727\u001b[0m        0.7105  0.3621\n",
      "      7        \u001b[36m0.7437\u001b[0m       0.6364        \u001b[35m0.6698\u001b[0m  0.3525\n",
      "      8        \u001b[36m0.6877\u001b[0m       0.6000        1.0804  0.3653\n",
      "      9        \u001b[36m0.6514\u001b[0m       0.6545        0.8141  0.3718\n",
      "     10        \u001b[36m0.5906\u001b[0m       0.6364        0.8060  0.3395\n",
      "     11        \u001b[36m0.5300\u001b[0m       0.6545        0.7370  0.3385\n",
      "     12        0.5687       0.6545        0.7015  0.3353\n",
      "     13        \u001b[36m0.4476\u001b[0m       0.6545        1.0127  0.3341\n",
      "     14        0.5562       0.6727        0.8147  0.3339\n",
      "     15        \u001b[36m0.3822\u001b[0m       \u001b[32m0.7273\u001b[0m        1.0994  0.3500\n",
      "     16        0.4421       0.6364        0.8492  0.3551\n",
      "     17        0.5880       0.6364        0.7386  0.3763\n",
      "     18        0.5453       0.6545        0.7379  0.3690\n",
      "     19        0.5096       0.6727        0.8399  0.3545\n",
      "     20        0.4634       0.6545        0.8699  0.3788\n",
      "     21        0.4486       0.6545        0.9221  0.3790\n",
      "     22        0.4129       0.6545        0.9031  0.3670\n",
      "     23        0.4513       0.6727        0.9914  0.3660\n",
      "     24        \u001b[36m0.3791\u001b[0m       0.6727        0.8438  0.3485\n",
      "     25        0.3819       0.6545        0.9274  0.3671\n",
      "     26        0.5048       0.6909        0.8071  0.3691\n",
      "     27        0.4253       0.6545        1.3672  0.3396\n",
      "     28        0.3820       0.6545        1.1138  0.3658\n",
      "     29        \u001b[36m0.3719\u001b[0m       0.6182        1.4370  0.3526\n",
      "     30        \u001b[36m0.3475\u001b[0m       0.6182        1.0788  0.3567\n",
      "     31        \u001b[36m0.3117\u001b[0m       0.6545        1.7227  0.3592\n",
      "     32        0.3997       0.6182        1.8936  0.3637\n",
      "     33        0.4195       0.6000        1.5328  0.3657\n",
      "     34        0.3788       0.6545        0.8699  0.3598\n",
      "     35        0.3332       0.6727        0.9475  0.3658\n",
      "     36        0.3142       0.5636        0.9282  0.3376\n",
      "     37        0.4196       0.6545        0.7734  0.3730\n",
      "     38        \u001b[36m0.1983\u001b[0m       0.6182        1.6927  0.3636\n",
      "     39        0.2509       0.6545        1.2000  0.3336\n",
      "     40        0.2135       0.6909        0.8627  0.3344\n",
      "     41        0.2055       0.7091        0.9452  0.3419\n",
      "     42        0.2373       0.7273        0.7781  0.3378\n",
      "     43        0.3734       0.7273        0.8518  0.3415\n",
      "     44        0.2156       0.6545        0.9133  0.3353\n",
      "     45        \u001b[36m0.1487\u001b[0m       0.6182        1.0884  0.3338\n",
      "     46        \u001b[36m0.1476\u001b[0m       0.6545        0.9667  0.3371\n",
      "     47        \u001b[36m0.1329\u001b[0m       0.6364        1.5074  0.3371\n",
      "     48        0.1435       0.6727        1.0133  0.3363\n",
      "     49        \u001b[36m0.0714\u001b[0m       0.7273        1.0051  0.3345\n",
      "     50        0.0736       0.6727        1.1638  0.3341\n",
      "Re-initializing module because the following parameters were re-set: .\n",
      "Re-initializing optimizer because the following parameters were re-set: .\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1062\u001b[0m       \u001b[32m0.3333\u001b[0m        \u001b[35m1.0725\u001b[0m  0.3338\n",
      "      2        \u001b[36m1.0690\u001b[0m       \u001b[32m0.4074\u001b[0m        \u001b[35m0.9680\u001b[0m  0.3340\n",
      "      3        \u001b[36m1.0092\u001b[0m       \u001b[32m0.6481\u001b[0m        \u001b[35m0.8695\u001b[0m  0.3332\n",
      "      4        \u001b[36m0.9423\u001b[0m       \u001b[32m0.7037\u001b[0m        \u001b[35m0.7912\u001b[0m  0.3332\n",
      "      5        \u001b[36m0.8826\u001b[0m       0.7037        \u001b[35m0.7491\u001b[0m  0.3334\n",
      "      6        \u001b[36m0.8137\u001b[0m       \u001b[32m0.7407\u001b[0m        \u001b[35m0.7003\u001b[0m  0.3426\n",
      "      7        \u001b[36m0.7662\u001b[0m       0.7407        \u001b[35m0.6869\u001b[0m  0.3381\n",
      "      8        \u001b[36m0.7368\u001b[0m       0.7222        \u001b[35m0.6643\u001b[0m  0.3345\n",
      "      9        \u001b[36m0.6935\u001b[0m       \u001b[32m0.7593\u001b[0m        0.6753  0.3340\n",
      "     10        \u001b[36m0.6816\u001b[0m       0.7593        \u001b[35m0.6187\u001b[0m  0.3331\n",
      "     11        \u001b[36m0.6367\u001b[0m       \u001b[32m0.7778\u001b[0m        0.6398  0.3341\n",
      "     12        \u001b[36m0.6336\u001b[0m       0.7593        \u001b[35m0.6119\u001b[0m  0.3338\n",
      "     13        0.6477       0.7407        \u001b[35m0.6023\u001b[0m  0.3333\n",
      "     14        0.7010       0.7407        0.6750  0.3391\n",
      "     15        0.6633       0.7037        0.6652  0.3420\n",
      "     16        \u001b[36m0.5899\u001b[0m       0.5926        0.7609  0.3412\n",
      "     17        0.6009       0.6296        0.6811  0.3393\n",
      "     18        0.6158       0.6111        0.7410  0.3353\n",
      "     19        0.6110       0.7222        0.7269  0.3352\n",
      "     20        0.6646       0.6667        0.6823  0.3353\n",
      "     21        \u001b[36m0.5658\u001b[0m       0.6667        0.7028  0.3358\n",
      "     22        0.5787       0.5741        0.8151  0.3372\n",
      "     23        \u001b[36m0.5551\u001b[0m       0.5556        0.7913  0.3350\n",
      "     24        \u001b[36m0.5425\u001b[0m       0.5741        0.8365  0.3362\n",
      "     25        0.5506       0.5556        0.8597  0.3506\n",
      "     26        0.6745       0.6481        0.7331  0.3367\n",
      "     27        0.5506       0.6481        0.6948  0.3389\n",
      "     28        \u001b[36m0.4669\u001b[0m       0.5370        0.8796  0.3368\n",
      "     29        0.5238       0.6667        0.7240  0.3376\n",
      "     30        \u001b[36m0.4316\u001b[0m       0.6296        0.8049  0.3394\n",
      "     31        0.4981       0.6111        0.8484  0.3351\n",
      "     32        \u001b[36m0.4139\u001b[0m       0.6481        0.9050  0.3346\n",
      "     33        \u001b[36m0.3551\u001b[0m       0.5185        1.0298  0.3343\n",
      "     34        0.4175       0.6667        0.7502  0.3503\n",
      "     35        0.3678       0.5185        1.0862  0.3409\n",
      "     36        \u001b[36m0.3345\u001b[0m       0.6481        0.8717  0.3412\n",
      "     37        \u001b[36m0.2730\u001b[0m       0.6667        0.8355  0.3425\n",
      "     38        \u001b[36m0.2203\u001b[0m       0.6481        1.0104  0.3403\n",
      "     39        \u001b[36m0.1932\u001b[0m       0.6667        0.9750  0.3395\n",
      "     40        0.5365       0.6111        0.9645  0.3435\n",
      "     41        \u001b[36m0.1776\u001b[0m       0.6481        1.0601  0.3400\n",
      "     42        \u001b[36m0.1233\u001b[0m       0.6111        1.1896  0.3401\n",
      "     43        \u001b[36m0.0873\u001b[0m       0.6296        1.2682  0.3367\n",
      "     44        \u001b[36m0.0690\u001b[0m       0.6296        1.4078  0.3376\n",
      "     45        0.1247       0.5926        1.3068  0.3390\n",
      "     46        \u001b[36m0.0678\u001b[0m       0.6481        1.4316  0.3402\n",
      "     47        0.2189       0.5556        1.1818  0.3342\n",
      "     48        \u001b[36m0.0642\u001b[0m       0.6481        1.6051  0.3348\n",
      "     49        \u001b[36m0.0503\u001b[0m       0.6481        1.5870  0.3352\n",
      "     50        0.3454       0.6111        1.5390  0.3359\n",
      "Re-initializing module because the following parameters were re-set: .\n",
      "Re-initializing optimizer because the following parameters were re-set: .\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0943\u001b[0m       \u001b[32m0.4074\u001b[0m        \u001b[35m1.0865\u001b[0m  0.3338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2        1.1028       0.3519        1.0976  0.3353\n",
      "      3        1.1024       0.3519        1.0971  0.3451\n",
      "      4        \u001b[36m1.0889\u001b[0m       0.3148        2.6250  0.3381\n",
      "      5        1.1162       0.3519        1.0976  0.3384\n",
      "      6        1.1024       0.3519        1.0977  0.3377\n",
      "      7        1.1024       0.3519        1.0978  0.3339\n",
      "      8        1.1024       0.3519        1.0978  0.3336\n",
      "      9        1.1024       0.3519        1.0978  0.3340\n",
      "     10        1.1023       0.3519        1.0978  0.3359\n",
      "     11        1.1023       0.3519        1.0978  0.3328\n",
      "     12        1.1022       0.3519        1.0977  0.3356\n",
      "     13        1.1033       0.3333        1.0997  0.3360\n",
      "     14        1.1029       0.3519        1.0977  0.3339\n",
      "     15        1.1021       0.3519        1.0977  0.3356\n",
      "     16        1.1020       0.3519        1.0978  0.3354\n",
      "     17        1.1019       0.3519        1.0978  0.3360\n",
      "     18        1.1018       0.3519        1.0978  0.3349\n",
      "     19        1.1017       0.3519        1.0978  0.3358\n",
      "     20        1.1016       0.3519        1.0977  0.3360\n",
      "     21        1.1014       0.3519        1.0977  0.3356\n",
      "     22        1.1011       0.3519        1.0976  0.3356\n",
      "     23        1.1007       0.3519        1.0972  0.3318\n",
      "     24        1.0981       0.3704        \u001b[35m1.0784\u001b[0m  0.3340\n",
      "     25        \u001b[36m1.0117\u001b[0m       0.4074        1.3376  0.3361\n",
      "     26        1.1722       0.3519        1.0978  0.3409\n",
      "     27        1.1030       0.3519        1.0977  0.3331\n",
      "     28        1.1028       0.3519        1.0978  0.3402\n",
      "     29        1.1028       0.3519        1.0978  0.3403\n",
      "     30        1.1028       0.3519        1.0978  0.3446\n",
      "     31        1.1028       0.3519        1.0978  0.3549\n",
      "     32        1.1028       0.3519        1.0978  0.3405\n",
      "     33        1.1027       0.3519        1.0978  0.3400\n",
      "     34        1.1027       0.3519        1.0978  0.3387\n",
      "     35        1.1027       0.3519        1.0978  0.3396\n",
      "     36        1.1027       0.3519        1.0978  0.3397\n",
      "     37        1.1026       0.3519        1.0978  0.3384\n",
      "     38        1.1026       0.3519        1.0978  0.3384\n",
      "     39        1.1026       0.3519        1.0978  0.3388\n",
      "     40        1.1026       0.3519        1.0978  0.3387\n",
      "     41        1.1026       0.3519        1.0978  0.3389\n",
      "     42        1.1025       0.3519        1.0978  0.3390\n",
      "     43        1.1025       0.3519        1.0978  0.3392\n",
      "     44        1.1025       0.3519        1.0978  0.3387\n",
      "     45        1.1025       0.3519        1.0978  0.3389\n",
      "     46        1.1025       0.3519        1.0978  0.3388\n",
      "     47        1.1025       0.3519        1.0978  0.3389\n",
      "     48        1.1024       0.3519        1.0978  0.3388\n",
      "     49        1.1024       0.3519        1.0978  0.3389\n",
      "     50        1.1024       0.3519        1.0978  0.3387\n",
      "Re-initializing module because the following parameters were re-set: .\n",
      "Re-initializing optimizer because the following parameters were re-set: .\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1068\u001b[0m       \u001b[32m0.3636\u001b[0m        \u001b[35m1.0985\u001b[0m  0.3547\n",
      "      2        \u001b[36m1.1016\u001b[0m       \u001b[32m0.3818\u001b[0m        \u001b[35m1.0723\u001b[0m  0.3602\n",
      "      3        1.1027       0.3273        1.0944  0.3488\n",
      "      4        1.1404       0.3455        1.0987  0.3692\n",
      "      5        1.1051       0.3455        1.0988  0.3411\n",
      "      6        1.1051       0.3455        1.0988  0.3494\n",
      "      7        1.1051       0.3455        1.0988  0.3511\n",
      "      8        1.1050       0.3455        1.0988  0.3638\n",
      "      9        1.1050       0.3455        1.0988  0.3593\n",
      "     10        1.1050       0.3455        1.0988  0.3612\n",
      "     11        1.1050       0.3455        1.0988  0.3708\n",
      "     12        1.1049       0.3455        1.0988  0.3592\n",
      "     13        1.1049       0.3455        1.0988  0.3647\n",
      "     14        1.1049       0.3455        1.0988  0.3664\n",
      "     15        1.1049       0.3455        1.0988  0.3359\n",
      "     16        1.1048       0.3455        1.0988  0.3570\n",
      "     17        1.1048       0.3455        1.0988  0.3660\n",
      "     18        1.1048       0.3455        1.0987  0.3899\n",
      "     19        1.1047       0.3455        1.0987  0.3866\n",
      "     20        1.1046       0.3455        1.0987  0.3666\n",
      "     21        1.1036       0.3455        1.0948  0.3705\n",
      "     22        \u001b[36m1.0926\u001b[0m       \u001b[32m0.5273\u001b[0m        \u001b[35m1.0542\u001b[0m  0.3610\n",
      "     23        \u001b[36m1.0483\u001b[0m       \u001b[32m0.6000\u001b[0m        \u001b[35m0.8929\u001b[0m  0.3612\n",
      "     24        1.0829       0.3273        1.1020  0.3860\n",
      "     25        1.1064       0.3455        1.0991  0.3452\n",
      "     26        1.1055       0.3455        1.0988  0.3481\n",
      "     27        1.1053       0.3455        1.0986  0.3382\n",
      "     28        1.1244       0.3455        1.0992  0.3384\n",
      "     29        1.1056       0.3455        1.0989  0.3609\n",
      "     30        1.1053       0.3455        1.0988  0.3505\n",
      "     31        1.1052       0.3455        1.0988  0.3485\n",
      "     32        1.1052       0.3455        1.0988  0.3457\n",
      "     33        1.1051       0.3455        1.0988  0.3446\n",
      "     34        1.1051       0.3455        1.0988  0.3462\n",
      "     35        1.1051       0.3455        1.0988  0.3445\n",
      "     36        1.1050       0.3455        1.0988  0.3424\n",
      "     37        1.1050       0.3455        1.0988  0.3568\n",
      "     38        1.1050       0.3455        1.0988  0.3341\n",
      "     39        1.1050       0.3455        1.0988  0.3448\n",
      "     40        1.1049       0.3455        1.0988  0.3382\n",
      "     41        1.1049       0.3455        1.0988  0.3357\n",
      "     42        1.1049       0.3455        1.0988  0.3498\n",
      "     43        1.1049       0.3455        1.0988  0.3714\n",
      "     44        1.1049       0.3455        1.0988  0.3831\n",
      "     45        1.1048       0.3455        1.0988  0.3738\n",
      "     46        1.1048       0.3455        1.0988  0.3795\n",
      "     47        1.1048       0.3455        1.0988  0.3614\n",
      "     48        1.1048       0.3455        1.0988  0.3535\n",
      "     49        1.1048       0.3455        1.0988  0.3412\n",
      "     50        1.1048       0.3455        1.0988  0.3404\n",
      "Re-initializing module because the following parameters were re-set: .\n",
      "Re-initializing optimizer because the following parameters were re-set: .\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0735\u001b[0m       \u001b[32m0.5741\u001b[0m        \u001b[35m0.9306\u001b[0m  0.3406\n",
      "      2        1.0948       0.3333        1.0988  0.3360\n",
      "      3        1.1075       0.3333        1.0989  0.3355\n",
      "      4        1.1074       0.3333        1.0992  0.3346\n",
      "      5        1.1073       0.3333        1.0992  0.3381\n",
      "      6        1.1071       0.3333        1.0991  0.3347\n",
      "      7        1.1070       0.3333        1.0991  0.3350\n",
      "      8        1.1070       0.3333        1.0991  0.3349\n",
      "      9        1.1070       0.3333        1.0991  0.3346\n",
      "     10        1.1070       0.3333        1.0991  0.3348\n",
      "     11        1.1069       0.3333        1.0991  0.3345\n",
      "     12        1.1069       0.3333        1.0991  0.3343\n",
      "     13        1.1069       0.3333        1.0991  0.3330\n",
      "     14        1.1068       0.3333        1.0991  0.3385\n",
      "     15        1.1068       0.3333        1.0991  0.3354\n",
      "     16        1.1068       0.3333        1.0991  0.3352\n",
      "     17        1.1068       0.3333        1.0991  0.3358\n",
      "     18        1.1067       0.3333        1.0991  0.3381\n",
      "     19        1.1067       0.3333        1.0991  0.3382\n",
      "     20        1.1066       0.3333        1.0991  0.3363\n",
      "     21        1.1061       0.3333        1.0946  0.3367\n",
      "     22        \u001b[36m1.0588\u001b[0m       \u001b[32m0.6481\u001b[0m        \u001b[35m0.9022\u001b[0m  0.3384\n",
      "     23        1.1041       0.3333        1.0989  0.3334\n",
      "     24        1.1069       0.3333        1.0989  0.3353\n",
      "     25        1.1069       0.3333        1.0991  0.3335\n",
      "     26        1.1069       0.3333        1.0992  0.3334\n",
      "     27        1.1069       0.3333        1.0992  0.3470\n",
      "     28        1.1069       0.3333        1.0992  0.3487\n",
      "     29        1.1068       0.3333        1.0992  0.3489\n",
      "     30        1.1068       0.3333        1.0991  0.3486\n",
      "     31        1.1068       0.3333        1.0992  0.3460\n",
      "     32        1.1068       0.3333        1.0991  0.3419\n",
      "     33        1.1068       0.3333        1.0991  0.3400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     34        1.1067       0.3333        1.0991  0.3402\n",
      "     35        1.1067       0.3333        1.0991  0.3399\n",
      "     36        1.1067       0.3333        1.0991  0.3393\n",
      "     37        1.1067       0.3333        1.0991  0.3394\n",
      "     38        1.1067       0.3333        1.0991  0.3389\n",
      "     39        1.1067       0.3333        1.0991  0.3404\n",
      "     40        1.1067       0.3333        1.0991  0.3394\n",
      "     41        1.1067       0.3333        1.0991  0.3391\n",
      "     42        1.1066       0.3333        1.0991  0.3397\n",
      "     43        1.1066       0.3333        1.0991  0.3401\n",
      "     44        1.1066       0.3333        1.0991  0.3398\n",
      "     45        1.1066       0.3333        1.0991  0.3400\n",
      "     46        1.1066       0.3333        1.0991  0.3392\n",
      "     47        1.1066       0.3333        1.0991  0.3394\n",
      "     48        1.1066       0.3333        1.0991  0.3396\n",
      "     49        1.1066       0.3333        1.0991  0.3388\n",
      "     50        1.1066       0.3333        1.0991  0.3398\n",
      "Re-initializing module because the following parameters were re-set: .\n",
      "Re-initializing optimizer because the following parameters were re-set: .\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0964\u001b[0m       \u001b[32m0.3519\u001b[0m        \u001b[35m1.0960\u001b[0m  0.2763\n",
      "      2        \u001b[36m1.0923\u001b[0m       0.3519        \u001b[35m1.0932\u001b[0m  0.2763\n",
      "      3        \u001b[36m1.0873\u001b[0m       0.3519        \u001b[35m1.0895\u001b[0m  0.2724\n",
      "      4        \u001b[36m1.0799\u001b[0m       0.3519        \u001b[35m1.0830\u001b[0m  0.2839\n",
      "      5        \u001b[36m1.0695\u001b[0m       \u001b[32m0.3704\u001b[0m        \u001b[35m1.0747\u001b[0m  0.2894\n",
      "      6        \u001b[36m1.0538\u001b[0m       \u001b[32m0.5000\u001b[0m        \u001b[35m1.0637\u001b[0m  0.2802\n",
      "      7        \u001b[36m1.0302\u001b[0m       \u001b[32m0.5185\u001b[0m        \u001b[35m1.0426\u001b[0m  0.2747\n",
      "      8        \u001b[36m0.9949\u001b[0m       \u001b[32m0.5370\u001b[0m        \u001b[35m1.0152\u001b[0m  0.2734\n",
      "      9        \u001b[36m0.9436\u001b[0m       0.5000        \u001b[35m0.9726\u001b[0m  0.2735\n",
      "     10        \u001b[36m0.8831\u001b[0m       0.5185        \u001b[35m0.9286\u001b[0m  0.2742\n",
      "     11        \u001b[36m0.8251\u001b[0m       0.5000        \u001b[35m0.9050\u001b[0m  0.2734\n",
      "     12        \u001b[36m0.7751\u001b[0m       0.5185        \u001b[35m0.8872\u001b[0m  0.2737\n",
      "     13        \u001b[36m0.7344\u001b[0m       \u001b[32m0.5741\u001b[0m        \u001b[35m0.8750\u001b[0m  0.2729\n",
      "     14        \u001b[36m0.6984\u001b[0m       0.5741        \u001b[35m0.8659\u001b[0m  0.2714\n",
      "     15        \u001b[36m0.6737\u001b[0m       \u001b[32m0.6111\u001b[0m        \u001b[35m0.8569\u001b[0m  0.2728\n",
      "     16        \u001b[36m0.6515\u001b[0m       0.5926        0.8605  0.2928\n",
      "     17        \u001b[36m0.6318\u001b[0m       0.5926        \u001b[35m0.8515\u001b[0m  0.2762\n",
      "     18        \u001b[36m0.6151\u001b[0m       0.5926        \u001b[35m0.8461\u001b[0m  0.2788\n",
      "     19        \u001b[36m0.5989\u001b[0m       0.5926        \u001b[35m0.8353\u001b[0m  0.2734\n",
      "     20        \u001b[36m0.5839\u001b[0m       0.5926        \u001b[35m0.8242\u001b[0m  0.2737\n",
      "     21        \u001b[36m0.5690\u001b[0m       0.5741        0.8260  0.2721\n",
      "     22        \u001b[36m0.5526\u001b[0m       0.5741        \u001b[35m0.8158\u001b[0m  0.2741\n",
      "     23        \u001b[36m0.5367\u001b[0m       0.5741        \u001b[35m0.8080\u001b[0m  0.2702\n",
      "     24        \u001b[36m0.5230\u001b[0m       0.5926        \u001b[35m0.7973\u001b[0m  0.2710\n",
      "     25        \u001b[36m0.5086\u001b[0m       \u001b[32m0.6296\u001b[0m        0.8001  0.2708\n",
      "     26        \u001b[36m0.4956\u001b[0m       \u001b[32m0.6667\u001b[0m        0.7976  0.2704\n",
      "     27        \u001b[36m0.4824\u001b[0m       0.6667        \u001b[35m0.7916\u001b[0m  0.2716\n",
      "     28        \u001b[36m0.4701\u001b[0m       0.6667        \u001b[35m0.7841\u001b[0m  0.2726\n",
      "     29        \u001b[36m0.4594\u001b[0m       0.6667        \u001b[35m0.7702\u001b[0m  0.2778\n",
      "     30        \u001b[36m0.4486\u001b[0m       0.6667        0.7727  0.2803\n",
      "     31        \u001b[36m0.4371\u001b[0m       \u001b[32m0.6852\u001b[0m        \u001b[35m0.7626\u001b[0m  0.2816\n",
      "     32        \u001b[36m0.4264\u001b[0m       \u001b[32m0.7222\u001b[0m        \u001b[35m0.7548\u001b[0m  0.2768\n",
      "     33        \u001b[36m0.4142\u001b[0m       0.7222        \u001b[35m0.7543\u001b[0m  0.2789\n",
      "     34        \u001b[36m0.4028\u001b[0m       \u001b[32m0.7407\u001b[0m        \u001b[35m0.7515\u001b[0m  0.2797\n",
      "     35        \u001b[36m0.3892\u001b[0m       0.7407        \u001b[35m0.7458\u001b[0m  0.2797\n",
      "     36        \u001b[36m0.3790\u001b[0m       0.7407        \u001b[35m0.7350\u001b[0m  0.3024\n",
      "     37        \u001b[36m0.3699\u001b[0m       0.7037        \u001b[35m0.7255\u001b[0m  0.3048\n",
      "     38        \u001b[36m0.3597\u001b[0m       0.7037        \u001b[35m0.7173\u001b[0m  0.2832\n",
      "     39        \u001b[36m0.3485\u001b[0m       0.7037        \u001b[35m0.7061\u001b[0m  0.2736\n",
      "     40        \u001b[36m0.3387\u001b[0m       0.7222        \u001b[35m0.6969\u001b[0m  0.2954\n",
      "     41        \u001b[36m0.3288\u001b[0m       0.7222        \u001b[35m0.6898\u001b[0m  0.3225\n",
      "     42        \u001b[36m0.3183\u001b[0m       0.7222        \u001b[35m0.6796\u001b[0m  0.3136\n",
      "     43        \u001b[36m0.3090\u001b[0m       0.7407        \u001b[35m0.6726\u001b[0m  0.3043\n",
      "     44        \u001b[36m0.3002\u001b[0m       \u001b[32m0.7593\u001b[0m        \u001b[35m0.6667\u001b[0m  0.2834\n",
      "     45        \u001b[36m0.2910\u001b[0m       \u001b[32m0.7778\u001b[0m        \u001b[35m0.6611\u001b[0m  0.2758\n",
      "     46        \u001b[36m0.2837\u001b[0m       0.7593        \u001b[35m0.6592\u001b[0m  0.2774\n",
      "     47        \u001b[36m0.2746\u001b[0m       0.7593        \u001b[35m0.6574\u001b[0m  0.2779\n",
      "     48        \u001b[36m0.2665\u001b[0m       0.7593        \u001b[35m0.6558\u001b[0m  0.2738\n",
      "     49        \u001b[36m0.2575\u001b[0m       \u001b[32m0.7963\u001b[0m        \u001b[35m0.6558\u001b[0m  0.2738\n",
      "     50        \u001b[36m0.2507\u001b[0m       0.7963        0.6573  0.2736\n",
      "Re-initializing module because the following parameters were re-set: .\n",
      "Re-initializing optimizer because the following parameters were re-set: .\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1007\u001b[0m       \u001b[32m0.3273\u001b[0m        \u001b[35m1.0970\u001b[0m  0.2704\n",
      "      2        \u001b[36m1.0978\u001b[0m       0.3273        \u001b[35m1.0939\u001b[0m  0.2715\n",
      "      3        \u001b[36m1.0953\u001b[0m       0.3273        \u001b[35m1.0908\u001b[0m  0.2707\n",
      "      4        \u001b[36m1.0921\u001b[0m       0.3273        \u001b[35m1.0863\u001b[0m  0.2731\n",
      "      5        \u001b[36m1.0882\u001b[0m       0.3273        \u001b[35m1.0815\u001b[0m  0.2747\n",
      "      6        \u001b[36m1.0839\u001b[0m       0.3273        \u001b[35m1.0761\u001b[0m  0.2777\n",
      "      7        \u001b[36m1.0788\u001b[0m       0.3273        \u001b[35m1.0696\u001b[0m  0.2762\n",
      "      8        \u001b[36m1.0730\u001b[0m       0.3273        \u001b[35m1.0618\u001b[0m  0.2751\n",
      "      9        \u001b[36m1.0661\u001b[0m       \u001b[32m0.3455\u001b[0m        \u001b[35m1.0531\u001b[0m  0.2853\n",
      "     10        \u001b[36m1.0580\u001b[0m       \u001b[32m0.3818\u001b[0m        \u001b[35m1.0426\u001b[0m  0.2781\n",
      "     11        \u001b[36m1.0485\u001b[0m       \u001b[32m0.4909\u001b[0m        \u001b[35m1.0311\u001b[0m  0.2899\n",
      "     12        \u001b[36m1.0375\u001b[0m       \u001b[32m0.5273\u001b[0m        \u001b[35m1.0178\u001b[0m  0.3013\n",
      "     13        \u001b[36m1.0247\u001b[0m       \u001b[32m0.5455\u001b[0m        \u001b[35m1.0027\u001b[0m  0.3019\n",
      "     14        \u001b[36m1.0089\u001b[0m       \u001b[32m0.5636\u001b[0m        \u001b[35m0.9835\u001b[0m  0.3233\n",
      "     15        \u001b[36m0.9908\u001b[0m       \u001b[32m0.5818\u001b[0m        \u001b[35m0.9627\u001b[0m  0.3082\n",
      "     16        \u001b[36m0.9700\u001b[0m       \u001b[32m0.6182\u001b[0m        \u001b[35m0.9393\u001b[0m  0.2756\n",
      "     17        \u001b[36m0.9454\u001b[0m       0.6182        \u001b[35m0.9157\u001b[0m  0.2801\n",
      "     18        \u001b[36m0.9158\u001b[0m       0.6182        \u001b[35m0.8847\u001b[0m  0.2793\n",
      "     19        \u001b[36m0.8817\u001b[0m       0.6182        \u001b[35m0.8549\u001b[0m  0.2755\n",
      "     20        \u001b[36m0.8476\u001b[0m       0.6182        \u001b[35m0.8213\u001b[0m  0.2810\n",
      "     21        \u001b[36m0.8174\u001b[0m       \u001b[32m0.6545\u001b[0m        \u001b[35m0.7971\u001b[0m  0.2757\n",
      "     22        \u001b[36m0.7948\u001b[0m       0.6545        \u001b[35m0.7756\u001b[0m  0.2710\n",
      "     23        \u001b[36m0.7800\u001b[0m       \u001b[32m0.6909\u001b[0m        \u001b[35m0.7588\u001b[0m  0.2728\n",
      "     24        \u001b[36m0.7700\u001b[0m       0.6727        \u001b[35m0.7478\u001b[0m  0.2722\n",
      "     25        \u001b[36m0.7603\u001b[0m       0.6909        \u001b[35m0.7387\u001b[0m  0.2706\n",
      "     26        \u001b[36m0.7516\u001b[0m       0.6909        \u001b[35m0.7350\u001b[0m  0.2704\n",
      "     27        \u001b[36m0.7427\u001b[0m       0.6909        \u001b[35m0.7294\u001b[0m  0.2705\n",
      "     28        \u001b[36m0.7330\u001b[0m       0.6909        \u001b[35m0.7258\u001b[0m  0.2709\n",
      "     29        \u001b[36m0.7223\u001b[0m       0.6909        \u001b[35m0.7255\u001b[0m  0.2705\n",
      "     30        \u001b[36m0.7124\u001b[0m       \u001b[32m0.7091\u001b[0m        \u001b[35m0.7238\u001b[0m  0.2697\n",
      "     31        \u001b[36m0.7011\u001b[0m       0.7091        \u001b[35m0.7227\u001b[0m  0.2709\n",
      "     32        \u001b[36m0.6904\u001b[0m       \u001b[32m0.7273\u001b[0m        0.7237  0.2711\n",
      "     33        \u001b[36m0.6804\u001b[0m       0.7091        \u001b[35m0.7212\u001b[0m  0.2707\n",
      "     34        \u001b[36m0.6693\u001b[0m       \u001b[32m0.7455\u001b[0m        0.7221  0.2772\n",
      "     35        \u001b[36m0.6579\u001b[0m       0.7455        0.7218  0.2785\n",
      "     36        \u001b[36m0.6473\u001b[0m       \u001b[32m0.7636\u001b[0m        \u001b[35m0.7170\u001b[0m  0.2793\n",
      "     37        \u001b[36m0.6372\u001b[0m       0.7636        \u001b[35m0.7168\u001b[0m  0.2796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     38        \u001b[36m0.6239\u001b[0m       0.7636        0.7178  0.2726\n",
      "     39        \u001b[36m0.6086\u001b[0m       \u001b[32m0.8000\u001b[0m        0.7192  0.2716\n",
      "     40        \u001b[36m0.6023\u001b[0m       0.7455        0.7290  0.2709\n",
      "     41        \u001b[36m0.6022\u001b[0m       0.7636        0.7338  0.2711\n",
      "     42        \u001b[36m0.5913\u001b[0m       0.7273        0.7443  0.2709\n",
      "     43        \u001b[36m0.5731\u001b[0m       0.7273        0.7561  0.2700\n",
      "     44        \u001b[36m0.5729\u001b[0m       0.7273        0.7571  0.2695\n",
      "     45        \u001b[36m0.5591\u001b[0m       0.7273        0.7599  0.2698\n",
      "     46        \u001b[36m0.5507\u001b[0m       0.6909        0.7880  0.2696\n",
      "     47        \u001b[36m0.5468\u001b[0m       0.6909        0.7834  0.2695\n",
      "     48        \u001b[36m0.5324\u001b[0m       0.7091        0.7980  0.2694\n",
      "     49        \u001b[36m0.5282\u001b[0m       0.7091        0.8063  0.2697\n",
      "     50        \u001b[36m0.5140\u001b[0m       0.7273        0.8037  0.2695\n",
      "Re-initializing module because the following parameters were re-set: .\n",
      "Re-initializing optimizer because the following parameters were re-set: .\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0910\u001b[0m       \u001b[32m0.4815\u001b[0m        \u001b[35m1.0795\u001b[0m  0.2704\n",
      "      2        \u001b[36m1.0784\u001b[0m       0.3889        \u001b[35m1.0658\u001b[0m  0.2728\n",
      "      3        \u001b[36m1.0674\u001b[0m       0.3889        \u001b[35m1.0532\u001b[0m  0.2712\n",
      "      4        \u001b[36m1.0569\u001b[0m       0.3889        \u001b[35m1.0410\u001b[0m  0.2715\n",
      "      5        \u001b[36m1.0460\u001b[0m       0.3889        \u001b[35m1.0279\u001b[0m  0.2713\n",
      "      6        \u001b[36m1.0344\u001b[0m       0.3889        \u001b[35m1.0137\u001b[0m  0.2721\n",
      "      7        \u001b[36m1.0220\u001b[0m       0.4074        \u001b[35m0.9995\u001b[0m  0.2754\n",
      "      8        \u001b[36m1.0090\u001b[0m       0.4074        \u001b[35m0.9838\u001b[0m  0.2739\n",
      "      9        \u001b[36m0.9942\u001b[0m       0.4630        \u001b[35m0.9667\u001b[0m  0.2713\n",
      "     10        \u001b[36m0.9772\u001b[0m       \u001b[32m0.5370\u001b[0m        \u001b[35m0.9463\u001b[0m  0.2710\n",
      "     11        \u001b[36m0.9551\u001b[0m       0.5370        \u001b[35m0.9196\u001b[0m  0.2709\n",
      "     12        \u001b[36m0.9260\u001b[0m       \u001b[32m0.5556\u001b[0m        \u001b[35m0.8861\u001b[0m  0.2712\n",
      "     13        \u001b[36m0.8892\u001b[0m       \u001b[32m0.5741\u001b[0m        \u001b[35m0.8442\u001b[0m  0.2708\n",
      "     14        \u001b[36m0.8475\u001b[0m       \u001b[32m0.6296\u001b[0m        \u001b[35m0.8016\u001b[0m  0.2716\n",
      "     15        \u001b[36m0.8086\u001b[0m       0.6296        \u001b[35m0.7594\u001b[0m  0.2710\n",
      "     16        \u001b[36m0.7759\u001b[0m       \u001b[32m0.6481\u001b[0m        \u001b[35m0.7254\u001b[0m  0.2705\n",
      "     17        \u001b[36m0.7509\u001b[0m       \u001b[32m0.7037\u001b[0m        \u001b[35m0.6992\u001b[0m  0.2710\n",
      "     18        \u001b[36m0.7263\u001b[0m       \u001b[32m0.7222\u001b[0m        \u001b[35m0.6853\u001b[0m  0.2708\n",
      "     19        \u001b[36m0.7052\u001b[0m       \u001b[32m0.7407\u001b[0m        \u001b[35m0.6651\u001b[0m  0.2703\n",
      "     20        \u001b[36m0.6866\u001b[0m       \u001b[32m0.7593\u001b[0m        \u001b[35m0.6491\u001b[0m  0.2706\n",
      "     21        \u001b[36m0.6651\u001b[0m       0.7593        \u001b[35m0.6395\u001b[0m  0.2716\n",
      "     22        \u001b[36m0.6448\u001b[0m       0.7593        \u001b[35m0.6293\u001b[0m  0.2713\n",
      "     23        \u001b[36m0.6271\u001b[0m       0.7407        \u001b[35m0.6198\u001b[0m  0.2712\n",
      "     24        \u001b[36m0.6115\u001b[0m       0.7593        \u001b[35m0.6139\u001b[0m  0.2721\n",
      "     25        \u001b[36m0.5956\u001b[0m       \u001b[32m0.7778\u001b[0m        \u001b[35m0.6104\u001b[0m  0.2710\n",
      "     26        \u001b[36m0.5848\u001b[0m       \u001b[32m0.7963\u001b[0m        \u001b[35m0.6038\u001b[0m  0.2717\n",
      "     27        \u001b[36m0.5703\u001b[0m       0.7963        \u001b[35m0.6033\u001b[0m  0.2715\n",
      "     28        \u001b[36m0.5566\u001b[0m       0.7963        \u001b[35m0.5989\u001b[0m  0.2722\n",
      "     29        \u001b[36m0.5421\u001b[0m       0.7963        \u001b[35m0.5941\u001b[0m  0.2706\n",
      "     30        \u001b[36m0.5298\u001b[0m       0.7963        0.5971  0.2699\n",
      "     31        \u001b[36m0.5174\u001b[0m       0.7963        0.5950  0.2704\n",
      "     32        \u001b[36m0.5052\u001b[0m       0.7963        \u001b[35m0.5913\u001b[0m  0.2704\n",
      "     33        \u001b[36m0.4946\u001b[0m       0.7963        0.5932  0.2774\n",
      "     34        \u001b[36m0.4809\u001b[0m       0.7963        0.5929  0.2797\n",
      "     35        \u001b[36m0.4729\u001b[0m       \u001b[32m0.8148\u001b[0m        \u001b[35m0.5871\u001b[0m  0.2774\n",
      "     36        \u001b[36m0.4603\u001b[0m       0.7778        0.5915  0.2774\n",
      "     37        \u001b[36m0.4479\u001b[0m       0.7963        0.5898  0.2775\n",
      "     38        \u001b[36m0.4365\u001b[0m       0.8148        0.5877  0.2766\n",
      "     39        \u001b[36m0.4251\u001b[0m       0.8148        0.5886  0.2760\n",
      "     40        \u001b[36m0.4091\u001b[0m       0.7778        \u001b[35m0.5855\u001b[0m  0.2762\n",
      "     41        \u001b[36m0.3987\u001b[0m       0.7778        0.5974  0.2798\n",
      "     42        \u001b[36m0.3905\u001b[0m       0.7778        0.5924  0.2889\n",
      "     43        \u001b[36m0.3809\u001b[0m       0.7593        0.6045  0.2760\n",
      "     44        \u001b[36m0.3732\u001b[0m       0.7593        0.6062  0.2783\n",
      "     45        \u001b[36m0.3624\u001b[0m       0.7593        0.6142  0.2767\n",
      "     46        \u001b[36m0.3590\u001b[0m       0.7593        0.6081  0.2748\n",
      "     47        \u001b[36m0.3488\u001b[0m       0.7593        0.6163  0.2775\n",
      "     48        \u001b[36m0.3413\u001b[0m       0.7593        0.6158  0.2820\n",
      "     49        \u001b[36m0.3350\u001b[0m       0.7407        0.6248  0.2737\n",
      "     50        \u001b[36m0.3295\u001b[0m       0.7407        0.6271  0.3053\n",
      "Re-initializing module because the following parameters were re-set: .\n",
      "Re-initializing optimizer because the following parameters were re-set: .\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0898\u001b[0m       \u001b[32m0.3333\u001b[0m        \u001b[35m1.0882\u001b[0m  0.3037\n",
      "      2        \u001b[36m1.0581\u001b[0m       0.3333        \u001b[35m1.0593\u001b[0m  0.3113\n",
      "      3        \u001b[36m1.0081\u001b[0m       \u001b[32m0.5370\u001b[0m        \u001b[35m1.0080\u001b[0m  0.3088\n",
      "      4        \u001b[36m0.9344\u001b[0m       0.5185        \u001b[35m0.9445\u001b[0m  0.3032\n",
      "      5        \u001b[36m0.8392\u001b[0m       \u001b[32m0.5741\u001b[0m        \u001b[35m0.8828\u001b[0m  0.2781\n",
      "      6        \u001b[36m0.7482\u001b[0m       0.5000        0.9450  0.2752\n",
      "      7        \u001b[36m0.6934\u001b[0m       0.5370        0.9873  0.2706\n",
      "      8        0.6960       0.5370        0.9967  0.2708\n",
      "      9        \u001b[36m0.5949\u001b[0m       0.5741        1.0132  0.2712\n",
      "     10        0.6175       0.5556        0.9388  0.2694\n",
      "     11        \u001b[36m0.5326\u001b[0m       \u001b[32m0.6296\u001b[0m        \u001b[35m0.7660\u001b[0m  0.2701\n",
      "     12        0.5388       0.5926        0.8221  0.2703\n",
      "     13        \u001b[36m0.5019\u001b[0m       \u001b[32m0.6852\u001b[0m        \u001b[35m0.7450\u001b[0m  0.2764\n",
      "     14        0.5119       0.6667        0.8074  0.3175\n",
      "     15        \u001b[36m0.4847\u001b[0m       \u001b[32m0.7037\u001b[0m        \u001b[35m0.7123\u001b[0m  0.2728\n",
      "     16        \u001b[36m0.4775\u001b[0m       \u001b[32m0.7222\u001b[0m        \u001b[35m0.6921\u001b[0m  0.2770\n",
      "     17        \u001b[36m0.4616\u001b[0m       \u001b[32m0.7407\u001b[0m        \u001b[35m0.6841\u001b[0m  0.2711\n",
      "     18        \u001b[36m0.4270\u001b[0m       0.7037        0.7076  0.2708\n",
      "     19        \u001b[36m0.4153\u001b[0m       0.7222        \u001b[35m0.6721\u001b[0m  0.2774\n",
      "     20        \u001b[36m0.4093\u001b[0m       0.7222        \u001b[35m0.6682\u001b[0m  0.2707\n",
      "     21        \u001b[36m0.3895\u001b[0m       0.7222        0.6748  0.2715\n",
      "     22        \u001b[36m0.3721\u001b[0m       0.7037        0.6943  0.2705\n",
      "     23        \u001b[36m0.3482\u001b[0m       0.7222        0.6908  0.2737\n",
      "     24        0.3677       0.7407        0.7047  0.2767\n",
      "     25        \u001b[36m0.3097\u001b[0m       0.7407        0.7170  0.2739\n",
      "     26        \u001b[36m0.2934\u001b[0m       0.7407        0.7308  0.2747\n",
      "     27        \u001b[36m0.2859\u001b[0m       0.7407        0.7645  0.2723\n",
      "     28        0.2889       0.7407        0.7578  0.2707\n",
      "     29        0.2990       \u001b[32m0.7593\u001b[0m        0.6967  0.2831\n",
      "     30        \u001b[36m0.2813\u001b[0m       0.7593        0.7802  0.2780\n",
      "     31        \u001b[36m0.2430\u001b[0m       0.7593        0.7269  0.2784\n",
      "     32        \u001b[36m0.2200\u001b[0m       0.7593        0.8012  0.2794\n",
      "     33        \u001b[36m0.2003\u001b[0m       0.7593        0.7215  0.2801\n",
      "     34        0.2233       0.7593        0.6947  0.2784\n",
      "     35        \u001b[36m0.1893\u001b[0m       0.7593        0.7127  0.2845\n",
      "     36        \u001b[36m0.1803\u001b[0m       0.7593        0.7052  0.2814\n",
      "     37        \u001b[36m0.1546\u001b[0m       0.7593        0.7057  0.2757\n",
      "     38        \u001b[36m0.1396\u001b[0m       \u001b[32m0.7778\u001b[0m        0.7021  0.2736\n",
      "     39        \u001b[36m0.1339\u001b[0m       0.7778        0.7016  0.2729\n",
      "     40        0.1438       0.7593        0.7062  0.2718\n",
      "     41        0.1343       0.7407        0.7134  0.2785\n",
      "     42        0.1407       0.7778        0.7180  0.2748\n",
      "     43        \u001b[36m0.1119\u001b[0m       0.7778        0.7117  0.2769\n",
      "     44        0.1819       0.7778        0.6685  0.2855\n",
      "     45        0.1298       0.7593        0.7331  0.2852\n",
      "     46        \u001b[36m0.0929\u001b[0m       0.7593        0.7442  0.2728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     47        0.0979       0.7593        0.7330  0.2708\n",
      "     48        0.2628       0.7037        \u001b[35m0.6598\u001b[0m  0.2703\n",
      "     49        \u001b[36m0.0800\u001b[0m       0.7407        0.7206  0.2714\n",
      "     50        0.0840       0.7222        0.7020  0.2721\n",
      "Re-initializing module because the following parameters were re-set: .\n",
      "Re-initializing optimizer because the following parameters were re-set: .\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0947\u001b[0m       \u001b[32m0.3455\u001b[0m        \u001b[35m1.0851\u001b[0m  0.2752\n",
      "      2        \u001b[36m1.0815\u001b[0m       \u001b[32m0.3818\u001b[0m        \u001b[35m1.0667\u001b[0m  0.2715\n",
      "      3        \u001b[36m1.0564\u001b[0m       \u001b[32m0.6364\u001b[0m        \u001b[35m1.0195\u001b[0m  0.2741\n",
      "      4        \u001b[36m0.9952\u001b[0m       0.6182        \u001b[35m0.9238\u001b[0m  0.2752\n",
      "      5        \u001b[36m0.8911\u001b[0m       0.6182        \u001b[35m0.7941\u001b[0m  0.2802\n",
      "      6        \u001b[36m0.7880\u001b[0m       \u001b[32m0.7091\u001b[0m        \u001b[35m0.7044\u001b[0m  0.2761\n",
      "      7        \u001b[36m0.7290\u001b[0m       0.6182        0.7594  0.2780\n",
      "      8        \u001b[36m0.7047\u001b[0m       \u001b[32m0.7273\u001b[0m        \u001b[35m0.6184\u001b[0m  0.2721\n",
      "      9        \u001b[36m0.6861\u001b[0m       \u001b[32m0.7455\u001b[0m        \u001b[35m0.5929\u001b[0m  0.2703\n",
      "     10        \u001b[36m0.6209\u001b[0m       0.6909        0.6657  0.2713\n",
      "     11        \u001b[36m0.6004\u001b[0m       \u001b[32m0.7818\u001b[0m        \u001b[35m0.5494\u001b[0m  0.2714\n",
      "     12        \u001b[36m0.5703\u001b[0m       0.7636        \u001b[35m0.5403\u001b[0m  0.2708\n",
      "     13        \u001b[36m0.5389\u001b[0m       \u001b[32m0.8000\u001b[0m        0.5420  0.2701\n",
      "     14        \u001b[36m0.5191\u001b[0m       0.8000        0.5467  0.2703\n",
      "     15        \u001b[36m0.5009\u001b[0m       0.7818        0.5545  0.2700\n",
      "     16        \u001b[36m0.4930\u001b[0m       0.7636        0.6177  0.2702\n",
      "     17        \u001b[36m0.4707\u001b[0m       0.7818        0.5972  0.2785\n",
      "     18        \u001b[36m0.4397\u001b[0m       0.7818        0.5818  0.2771\n",
      "     19        \u001b[36m0.4177\u001b[0m       0.8000        0.5827  0.2780\n",
      "     20        \u001b[36m0.3952\u001b[0m       \u001b[32m0.8182\u001b[0m        0.5888  0.2772\n",
      "     21        \u001b[36m0.3807\u001b[0m       0.8000        0.5857  0.2769\n",
      "     22        \u001b[36m0.3577\u001b[0m       0.8000        0.5934  0.2767\n",
      "     23        \u001b[36m0.3506\u001b[0m       0.8000        0.5978  0.2772\n",
      "     24        \u001b[36m0.3401\u001b[0m       0.8000        0.6016  0.2774\n",
      "     25        \u001b[36m0.3158\u001b[0m       0.8000        0.6116  0.2778\n",
      "     26        \u001b[36m0.3075\u001b[0m       0.8000        0.6123  0.2774\n",
      "     27        \u001b[36m0.2924\u001b[0m       0.8000        0.6297  0.2769\n",
      "     28        \u001b[36m0.2840\u001b[0m       0.8000        0.6531  0.2774\n",
      "     29        \u001b[36m0.2743\u001b[0m       0.8000        0.6562  0.2769\n",
      "     30        \u001b[36m0.2662\u001b[0m       0.7818        0.6567  0.2772\n",
      "     31        \u001b[36m0.2567\u001b[0m       0.7818        0.6609  0.2769\n",
      "     32        \u001b[36m0.2483\u001b[0m       0.7818        0.6545  0.2784\n",
      "     33        \u001b[36m0.2408\u001b[0m       0.7818        0.6579  0.2782\n",
      "     34        \u001b[36m0.2336\u001b[0m       0.7636        0.6636  0.2773\n",
      "     35        \u001b[36m0.2265\u001b[0m       0.7273        0.8828  0.2776\n",
      "     36        0.3663       0.7818        0.6657  0.2782\n",
      "     37        \u001b[36m0.2132\u001b[0m       0.7818        0.6724  0.2773\n",
      "     38        \u001b[36m0.2082\u001b[0m       0.7636        0.6873  0.2779\n",
      "     39        \u001b[36m0.1991\u001b[0m       0.7455        0.6852  0.2779\n",
      "     40        \u001b[36m0.1968\u001b[0m       0.7636        0.8080  0.2788\n",
      "     41        \u001b[36m0.1903\u001b[0m       0.7636        0.6953  0.2748\n",
      "     42        0.1945       0.8000        0.7294  0.2710\n",
      "     43        \u001b[36m0.1710\u001b[0m       0.7636        0.7098  0.2705\n",
      "     44        0.2920       0.8000        0.6661  0.2698\n",
      "     45        \u001b[36m0.1690\u001b[0m       0.7636        0.8836  0.2720\n",
      "     46        \u001b[36m0.1645\u001b[0m       0.8000        0.7884  0.2733\n",
      "     47        \u001b[36m0.1487\u001b[0m       0.7455        0.7466  0.2730\n",
      "     48        \u001b[36m0.1410\u001b[0m       0.7455        0.7658  0.2739\n",
      "     49        \u001b[36m0.1303\u001b[0m       0.7636        0.7744  0.2727\n",
      "     50        \u001b[36m0.1274\u001b[0m       0.7455        0.7913  0.2717\n",
      "Re-initializing module because the following parameters were re-set: .\n",
      "Re-initializing optimizer because the following parameters were re-set: .\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1021\u001b[0m       \u001b[32m0.3333\u001b[0m        \u001b[35m1.1011\u001b[0m  0.2703\n",
      "      2        \u001b[36m1.1000\u001b[0m       0.3333        \u001b[35m1.0991\u001b[0m  0.2757\n",
      "      3        \u001b[36m1.0987\u001b[0m       0.3333        \u001b[35m1.0975\u001b[0m  0.2760\n",
      "      4        \u001b[36m1.0974\u001b[0m       0.3333        \u001b[35m1.0957\u001b[0m  0.2730\n",
      "      5        \u001b[36m1.0957\u001b[0m       \u001b[32m0.4444\u001b[0m        \u001b[35m1.0933\u001b[0m  0.2805\n",
      "      6        \u001b[36m1.0935\u001b[0m       \u001b[32m0.4815\u001b[0m        \u001b[35m1.0894\u001b[0m  0.2869\n",
      "      7        \u001b[36m1.0877\u001b[0m       \u001b[32m0.5000\u001b[0m        \u001b[35m1.0804\u001b[0m  0.2793\n",
      "      8        \u001b[36m1.0788\u001b[0m       \u001b[32m0.5185\u001b[0m        \u001b[35m1.0643\u001b[0m  0.2843\n",
      "      9        \u001b[36m1.0637\u001b[0m       \u001b[32m0.5556\u001b[0m        \u001b[35m1.0366\u001b[0m  0.2714\n",
      "     10        \u001b[36m1.0370\u001b[0m       \u001b[32m0.5926\u001b[0m        \u001b[35m0.9924\u001b[0m  0.2798\n",
      "     11        \u001b[36m0.9961\u001b[0m       \u001b[32m0.6296\u001b[0m        \u001b[35m0.9293\u001b[0m  0.2837\n",
      "     12        \u001b[36m0.9495\u001b[0m       0.6296        \u001b[35m0.8612\u001b[0m  0.2718\n",
      "     13        \u001b[36m0.9152\u001b[0m       0.6296        \u001b[35m0.8106\u001b[0m  0.3052\n",
      "     14        \u001b[36m0.9006\u001b[0m       0.6296        \u001b[35m0.7848\u001b[0m  0.2846\n",
      "     15        \u001b[36m0.8858\u001b[0m       0.6296        \u001b[35m0.7711\u001b[0m  0.2939\n",
      "     16        \u001b[36m0.8730\u001b[0m       0.6296        \u001b[35m0.7625\u001b[0m  0.2781\n",
      "     17        \u001b[36m0.8547\u001b[0m       0.6296        \u001b[35m0.7508\u001b[0m  0.2795\n",
      "     18        \u001b[36m0.8497\u001b[0m       0.6296        \u001b[35m0.7483\u001b[0m  0.2826\n",
      "     19        \u001b[36m0.8306\u001b[0m       \u001b[32m0.6481\u001b[0m        \u001b[35m0.7428\u001b[0m  0.2780\n",
      "     20        \u001b[36m0.8245\u001b[0m       0.6481        \u001b[35m0.7396\u001b[0m  0.2783\n",
      "     21        \u001b[36m0.8174\u001b[0m       0.6481        \u001b[35m0.7376\u001b[0m  0.2784\n",
      "     22        \u001b[36m0.8126\u001b[0m       \u001b[32m0.6667\u001b[0m        \u001b[35m0.7354\u001b[0m  0.2799\n",
      "     23        \u001b[36m0.7838\u001b[0m       0.6667        \u001b[35m0.7347\u001b[0m  0.2771\n",
      "     24        0.7887       0.6667        \u001b[35m0.7330\u001b[0m  0.2774\n",
      "     25        \u001b[36m0.7633\u001b[0m       0.6667        \u001b[35m0.7318\u001b[0m  0.2774\n",
      "     26        \u001b[36m0.7616\u001b[0m       0.6667        \u001b[35m0.7296\u001b[0m  0.2773\n",
      "     27        \u001b[36m0.7489\u001b[0m       0.6667        \u001b[35m0.7278\u001b[0m  0.2774\n",
      "     28        \u001b[36m0.7305\u001b[0m       0.6481        0.7359  0.2779\n",
      "     29        \u001b[36m0.7226\u001b[0m       0.6667        0.7314  0.2775\n",
      "     30        \u001b[36m0.7158\u001b[0m       0.6296        0.7340  0.2773\n",
      "     31        \u001b[36m0.7036\u001b[0m       0.6111        0.7384  0.2772\n",
      "     32        \u001b[36m0.6921\u001b[0m       0.6481        0.7344  0.2792\n",
      "     33        \u001b[36m0.6902\u001b[0m       0.6481        \u001b[35m0.7261\u001b[0m  0.2777\n",
      "     34        \u001b[36m0.6735\u001b[0m       0.6667        \u001b[35m0.7189\u001b[0m  0.2784\n",
      "     35        \u001b[36m0.6617\u001b[0m       0.6667        0.7264  0.2775\n",
      "     36        \u001b[36m0.6500\u001b[0m       \u001b[32m0.6852\u001b[0m        0.7228  0.2772\n",
      "     37        \u001b[36m0.6388\u001b[0m       0.6852        \u001b[35m0.7132\u001b[0m  0.2783\n",
      "     38        0.6416       0.6852        \u001b[35m0.7075\u001b[0m  0.2780\n",
      "     39        \u001b[36m0.6172\u001b[0m       0.6852        \u001b[35m0.6978\u001b[0m  0.2776\n",
      "     40        0.6387       \u001b[32m0.7037\u001b[0m        \u001b[35m0.6949\u001b[0m  0.2776\n",
      "     41        \u001b[36m0.6112\u001b[0m       \u001b[32m0.7407\u001b[0m        0.6951  0.2776\n",
      "     42        \u001b[36m0.5783\u001b[0m       0.7222        \u001b[35m0.6875\u001b[0m  0.2801\n",
      "     43        \u001b[36m0.5762\u001b[0m       0.7407        0.7122  0.2775\n",
      "     44        \u001b[36m0.5726\u001b[0m       0.7222        0.7109  0.2776\n",
      "     45        \u001b[36m0.5234\u001b[0m       0.7407        \u001b[35m0.6687\u001b[0m  0.2777\n",
      "     46        0.5244       0.7407        \u001b[35m0.6526\u001b[0m  0.2787\n",
      "     47        0.6351       0.5185        0.8521  0.2777\n",
      "     48        0.5649       \u001b[32m0.7593\u001b[0m        \u001b[35m0.6190\u001b[0m  0.2781\n",
      "     49        \u001b[36m0.5042\u001b[0m       \u001b[32m0.7778\u001b[0m        \u001b[35m0.5965\u001b[0m  0.2767\n",
      "     50        \u001b[36m0.4947\u001b[0m       0.7593        0.6292  0.2770\n",
      "Re-initializing module because the following parameters were re-set: .\n",
      "Re-initializing optimizer because the following parameters were re-set: .\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1043\u001b[0m       \u001b[32m0.3148\u001b[0m        \u001b[35m1.1029\u001b[0m  0.2780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2        \u001b[36m1.1002\u001b[0m       0.3148        \u001b[35m1.1008\u001b[0m  0.2779\n",
      "      3        \u001b[36m1.0966\u001b[0m       0.3148        \u001b[35m1.0970\u001b[0m  0.2769\n",
      "      4        \u001b[36m1.0844\u001b[0m       \u001b[32m0.4259\u001b[0m        \u001b[35m1.0871\u001b[0m  0.2769\n",
      "      5        \u001b[36m1.0451\u001b[0m       \u001b[32m0.5741\u001b[0m        \u001b[35m1.0355\u001b[0m  0.2780\n",
      "      6        \u001b[36m0.9597\u001b[0m       \u001b[32m0.6667\u001b[0m        \u001b[35m0.9460\u001b[0m  0.2769\n",
      "      7        \u001b[36m0.8792\u001b[0m       0.6111        \u001b[35m0.8665\u001b[0m  0.2770\n",
      "      8        \u001b[36m0.8313\u001b[0m       0.4444        1.0261  0.2769\n",
      "      9        0.8413       \u001b[32m0.7037\u001b[0m        \u001b[35m0.7834\u001b[0m  0.2778\n",
      "     10        \u001b[36m0.7540\u001b[0m       0.4444        1.1474  0.2779\n",
      "     11        \u001b[36m0.6901\u001b[0m       0.4074        1.4631  0.2740\n",
      "     12        0.7401       0.6667        0.7969  0.2702\n",
      "     13        0.7003       0.7037        \u001b[35m0.7474\u001b[0m  0.2698\n",
      "     14        \u001b[36m0.6360\u001b[0m       0.6852        0.7551  0.2697\n",
      "     15        \u001b[36m0.6181\u001b[0m       0.6481        0.8659  0.2702\n",
      "     16        \u001b[36m0.5870\u001b[0m       0.6111        0.9469  0.2694\n",
      "     17        \u001b[36m0.5131\u001b[0m       0.6481        \u001b[35m0.6953\u001b[0m  0.2698\n",
      "     18        0.6431       0.7037        \u001b[35m0.6764\u001b[0m  0.2702\n",
      "     19        0.5884       0.7037        \u001b[35m0.6347\u001b[0m  0.2699\n",
      "     20        \u001b[36m0.4824\u001b[0m       0.6852        0.6382  0.2700\n",
      "     21        \u001b[36m0.4747\u001b[0m       0.5556        1.2913  0.2701\n",
      "     22        \u001b[36m0.4533\u001b[0m       0.6852        \u001b[35m0.6134\u001b[0m  0.2703\n",
      "     23        \u001b[36m0.3967\u001b[0m       0.6852        \u001b[35m0.5800\u001b[0m  0.2700\n",
      "     24        \u001b[36m0.3905\u001b[0m       0.5556        1.7817  0.2697\n",
      "     25        \u001b[36m0.3695\u001b[0m       0.6481        0.7913  0.2700\n",
      "     26        \u001b[36m0.3629\u001b[0m       0.6667        0.7162  0.2703\n",
      "     27        0.4801       0.7037        0.8416  0.2695\n",
      "     28        \u001b[36m0.3599\u001b[0m       \u001b[32m0.7407\u001b[0m        0.6603  0.2687\n",
      "     29        \u001b[36m0.2493\u001b[0m       0.6481        0.9031  0.2731\n",
      "     30        \u001b[36m0.2459\u001b[0m       0.6111        1.0838  0.2709\n",
      "     31        0.3843       0.7037        0.6114  0.2705\n",
      "     32        0.4716       0.6852        0.6426  0.2704\n",
      "     33        \u001b[36m0.2380\u001b[0m       0.6111        0.9661  0.2703\n",
      "     34        \u001b[36m0.1926\u001b[0m       0.6667        1.0191  0.2704\n",
      "     35        0.3038       0.7037        0.6391  0.2699\n",
      "     36        0.2948       0.6481        0.9278  0.2703\n",
      "     37        0.5201       0.5000        1.2187  0.2709\n",
      "     38        0.2675       0.6296        1.0489  0.2700\n",
      "     39        0.3710       0.6852        0.6644  0.2703\n",
      "     40        0.2262       0.7037        0.8418  0.2731\n",
      "     41        0.3442       0.7407        0.5843  0.2735\n",
      "     42        \u001b[36m0.1736\u001b[0m       0.6111        1.0773  0.2706\n",
      "     43        \u001b[36m0.1235\u001b[0m       0.6481        0.9162  0.2687\n",
      "     44        \u001b[36m0.0977\u001b[0m       0.6852        0.9375  0.2800\n",
      "     45        0.2476       0.6852        0.7174  0.2757\n",
      "     46        0.1869       0.7037        0.7692  0.2717\n",
      "     47        0.4571       \u001b[32m0.7778\u001b[0m        \u001b[35m0.5681\u001b[0m  0.2720\n",
      "     48        0.1119       0.5926        1.1998  0.2718\n",
      "     49        0.2367       0.6852        0.7437  0.2720\n",
      "     50        0.2524       0.7407        0.6734  0.2711\n",
      "Re-initializing module because the following parameters were re-set: .\n",
      "Re-initializing optimizer because the following parameters were re-set: .\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0961\u001b[0m       \u001b[32m0.3818\u001b[0m        \u001b[35m1.0836\u001b[0m  0.2710\n",
      "      2        \u001b[36m1.0573\u001b[0m       \u001b[32m0.6182\u001b[0m        \u001b[35m1.0081\u001b[0m  0.2736\n",
      "      3        \u001b[36m0.9854\u001b[0m       \u001b[32m0.6545\u001b[0m        \u001b[35m0.8792\u001b[0m  0.2728\n",
      "      4        \u001b[36m0.9355\u001b[0m       \u001b[32m0.7273\u001b[0m        \u001b[35m0.7485\u001b[0m  0.2730\n",
      "      5        \u001b[36m0.9287\u001b[0m       0.7273        \u001b[35m0.7412\u001b[0m  0.2723\n",
      "      6        \u001b[36m0.8283\u001b[0m       0.5818        0.8687  0.2739\n",
      "      7        \u001b[36m0.7630\u001b[0m       \u001b[32m0.7636\u001b[0m        \u001b[35m0.7215\u001b[0m  0.2748\n",
      "      8        0.8534       0.5818        0.8364  0.2749\n",
      "      9        0.7635       0.6909        \u001b[35m0.6142\u001b[0m  0.2708\n",
      "     10        \u001b[36m0.7058\u001b[0m       \u001b[32m0.8000\u001b[0m        0.6321  0.2705\n",
      "     11        \u001b[36m0.6830\u001b[0m       0.6182        0.9593  0.2783\n",
      "     12        \u001b[36m0.6748\u001b[0m       0.6909        0.7065  0.2763\n",
      "     13        0.7215       0.7636        0.6763  0.2762\n",
      "     14        \u001b[36m0.6541\u001b[0m       0.7273        0.6629  0.2780\n",
      "     15        \u001b[36m0.6370\u001b[0m       0.7091        0.6857  0.2781\n",
      "     16        0.6883       0.6909        0.6782  0.2782\n",
      "     17        \u001b[36m0.5871\u001b[0m       0.7455        0.6658  0.2778\n",
      "     18        0.5913       0.6909        0.6766  0.2779\n",
      "     19        \u001b[36m0.5447\u001b[0m       0.7273        0.6901  0.2778\n",
      "     20        \u001b[36m0.5014\u001b[0m       0.6909        0.7422  0.2767\n",
      "     21        0.5214       0.7091        0.7148  0.2724\n",
      "     22        \u001b[36m0.4793\u001b[0m       0.6909        0.7588  0.2730\n",
      "     23        0.5936       0.6364        0.6960  0.2714\n",
      "     24        \u001b[36m0.4234\u001b[0m       0.6727        0.7954  0.2707\n",
      "     25        \u001b[36m0.3960\u001b[0m       0.7091        0.8397  0.2715\n",
      "     26        0.4464       0.7091        0.8180  0.2707\n",
      "     27        0.4359       0.6545        0.8066  0.2704\n",
      "     28        0.4310       0.6727        0.8507  0.2724\n",
      "     29        0.5015       0.5636        0.9635  0.2720\n",
      "     30        0.5532       0.4909        0.9442  0.2717\n",
      "     31        0.4710       0.6545        0.7985  0.2720\n",
      "     32        \u001b[36m0.3500\u001b[0m       0.6727        0.8863  0.2722\n",
      "     33        \u001b[36m0.3375\u001b[0m       0.6545        1.1157  0.2718\n",
      "     34        0.3406       0.7091        0.9108  0.2719\n",
      "     35        \u001b[36m0.3087\u001b[0m       0.6727        1.0074  0.2717\n",
      "     36        \u001b[36m0.2816\u001b[0m       0.6909        1.0614  0.2718\n",
      "     37        0.3242       0.6909        0.9974  0.2718\n",
      "     38        0.3865       0.6909        1.0775  0.2725\n",
      "     39        0.2941       0.6182        1.0457  0.2856\n",
      "     40        0.3197       0.6727        1.0488  0.2936\n",
      "     41        \u001b[36m0.2467\u001b[0m       0.6364        1.4039  0.2831\n",
      "     42        0.6032       0.6909        0.8886  0.2927\n",
      "     43        0.2588       0.6727        1.1506  0.2955\n",
      "     44        \u001b[36m0.2258\u001b[0m       0.6545        1.2834  0.2927\n",
      "     45        0.2800       0.6545        1.1372  0.2918\n",
      "     46        0.3222       0.6909        0.9552  0.2747\n",
      "     47        0.2388       0.4727        1.9419  0.2750\n",
      "     48        0.3465       0.6364        1.1952  0.2699\n",
      "     49        \u001b[36m0.2095\u001b[0m       0.6000        1.3760  0.2694\n",
      "     50        0.3212       0.6909        1.3608  0.2695\n",
      "Re-initializing module because the following parameters were re-set: .\n",
      "Re-initializing optimizer because the following parameters were re-set: .\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1019\u001b[0m       \u001b[32m0.3333\u001b[0m        \u001b[35m1.0554\u001b[0m  0.2714\n",
      "      2        \u001b[36m1.0531\u001b[0m       \u001b[32m0.5926\u001b[0m        \u001b[35m0.9639\u001b[0m  0.2712\n",
      "      3        \u001b[36m0.9978\u001b[0m       0.5926        \u001b[35m0.8855\u001b[0m  0.2702\n",
      "      4        \u001b[36m0.9601\u001b[0m       0.5926        \u001b[35m0.8362\u001b[0m  0.2702\n",
      "      5        \u001b[36m0.9245\u001b[0m       \u001b[32m0.6296\u001b[0m        \u001b[35m0.7970\u001b[0m  0.2703\n",
      "      6        \u001b[36m0.8904\u001b[0m       \u001b[32m0.6852\u001b[0m        \u001b[35m0.7632\u001b[0m  0.2828\n",
      "      7        \u001b[36m0.8515\u001b[0m       \u001b[32m0.7222\u001b[0m        \u001b[35m0.7355\u001b[0m  0.3350\n",
      "      8        \u001b[36m0.8200\u001b[0m       0.6852        \u001b[35m0.7095\u001b[0m  0.3098\n",
      "      9        \u001b[36m0.7905\u001b[0m       0.6667        \u001b[35m0.6947\u001b[0m  0.2866\n",
      "     10        \u001b[36m0.7614\u001b[0m       0.6852        \u001b[35m0.6772\u001b[0m  0.3327\n",
      "     11        \u001b[36m0.7260\u001b[0m       \u001b[32m0.7778\u001b[0m        \u001b[35m0.6723\u001b[0m  0.3079\n",
      "     12        \u001b[36m0.6981\u001b[0m       0.6667        0.6935  0.2783\n",
      "     13        \u001b[36m0.6728\u001b[0m       0.5556        0.8190  0.2720\n",
      "     14        \u001b[36m0.6534\u001b[0m       0.5556        0.8241  0.2712\n",
      "     15        \u001b[36m0.6441\u001b[0m       0.5185        0.7966  0.2773\n",
      "     16        \u001b[36m0.6182\u001b[0m       0.5556        0.8564  0.2723\n",
      "     17        \u001b[36m0.6172\u001b[0m       0.7222        \u001b[35m0.6489\u001b[0m  0.2716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18        \u001b[36m0.5660\u001b[0m       0.6852        0.7165  0.2708\n",
      "     19        \u001b[36m0.5351\u001b[0m       0.7222        0.7233  0.2696\n",
      "     20        0.5558       0.7037        0.6962  0.2696\n",
      "     21        0.5372       0.5926        0.8651  0.2699\n",
      "     22        0.5468       0.7407        0.6739  0.2697\n",
      "     23        \u001b[36m0.5116\u001b[0m       0.6296        0.7763  0.2694\n",
      "     24        0.6026       0.7037        0.6562  0.2696\n",
      "     25        \u001b[36m0.4740\u001b[0m       0.7407        0.6871  0.2700\n",
      "     26        \u001b[36m0.4510\u001b[0m       0.7037        0.7448  0.2698\n",
      "     27        0.4661       0.7593        0.7058  0.2699\n",
      "     28        0.5484       0.7222        0.6597  0.2698\n",
      "     29        \u001b[36m0.4412\u001b[0m       0.5185        0.8377  0.2698\n",
      "     30        0.5000       0.7222        0.7482  0.2693\n",
      "     31        0.5155       0.6852        0.8033  0.2694\n",
      "     32        0.5442       0.7407        0.7656  0.2696\n",
      "     33        0.4678       0.6111        0.8854  0.2695\n",
      "     34        \u001b[36m0.4233\u001b[0m       0.7778        0.8404  0.2702\n",
      "     35        \u001b[36m0.3742\u001b[0m       0.7222        0.7831  0.2699\n",
      "     36        \u001b[36m0.3460\u001b[0m       0.5000        1.2927  0.2704\n",
      "     37        0.5136       0.6667        0.8910  0.2700\n",
      "     38        0.3981       0.7037        0.7584  0.2696\n",
      "     39        0.6565       0.7407        0.7142  0.2698\n",
      "     40        0.3694       0.5556        1.7130  0.2695\n",
      "     41        1.5720       0.6667        0.7008  0.2694\n",
      "     42        0.6465       0.5926        0.8819  0.2696\n",
      "     43        0.5776       0.4815        0.9519  0.2695\n",
      "     44        0.5153       0.5556        0.8607  0.2696\n",
      "     45        0.4943       0.5741        1.4359  0.2696\n",
      "     46        0.7181       0.6296        0.7855  0.2693\n",
      "     47        0.4514       0.5741        1.0031  0.2693\n",
      "     48        0.4951       0.6111        0.9539  0.2697\n",
      "     49        0.4197       0.4444        1.1021  0.2699\n",
      "     50        0.4099       0.6667        0.8122  0.2696\n",
      "Re-initializing module because the following parameters were re-set: .\n",
      "Re-initializing optimizer because the following parameters were re-set: .\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0753\u001b[0m       \u001b[32m0.3333\u001b[0m        \u001b[35m1.0915\u001b[0m  0.2691\n",
      "      2        \u001b[36m1.0380\u001b[0m       \u001b[32m0.5370\u001b[0m        \u001b[35m0.9930\u001b[0m  0.2697\n",
      "      3        \u001b[36m0.9594\u001b[0m       \u001b[32m0.6667\u001b[0m        \u001b[35m0.9000\u001b[0m  0.2690\n",
      "      4        \u001b[36m0.9112\u001b[0m       \u001b[32m0.6852\u001b[0m        \u001b[35m0.8368\u001b[0m  0.2686\n",
      "      5        \u001b[36m0.8227\u001b[0m       0.6852        \u001b[35m0.7885\u001b[0m  0.2686\n",
      "      6        \u001b[36m0.7628\u001b[0m       \u001b[32m0.7037\u001b[0m        \u001b[35m0.7139\u001b[0m  0.2686\n",
      "      7        0.7704       0.5556        0.7898  0.2684\n",
      "      8        \u001b[36m0.6545\u001b[0m       0.6111        0.8340  0.2733\n",
      "      9        0.7714       \u001b[32m0.7778\u001b[0m        \u001b[35m0.6989\u001b[0m  0.2755\n",
      "     10        \u001b[36m0.6494\u001b[0m       0.7778        \u001b[35m0.6244\u001b[0m  0.2760\n",
      "     11        \u001b[36m0.6392\u001b[0m       0.7222        0.6696  0.2765\n",
      "     12        0.7362       0.5741        0.7390  0.2770\n",
      "     13        \u001b[36m0.6355\u001b[0m       0.6296        0.9602  0.2769\n",
      "     14        0.7637       0.7037        0.7258  0.2761\n",
      "     15        \u001b[36m0.6195\u001b[0m       0.6111        0.8995  0.2767\n",
      "     16        \u001b[36m0.5758\u001b[0m       0.7407        \u001b[35m0.6226\u001b[0m  0.2757\n",
      "     17        \u001b[36m0.4691\u001b[0m       0.6852        0.8454  0.2769\n",
      "     18        0.7195       0.5926        0.8770  0.2761\n",
      "     19        \u001b[36m0.4639\u001b[0m       0.6111        1.2858  0.2782\n",
      "     20        0.7814       0.6111        0.7737  0.2752\n",
      "     21        0.6292       0.7407        0.7471  0.2760\n",
      "     22        0.5543       0.7222        0.6437  0.2763\n",
      "     23        \u001b[36m0.4470\u001b[0m       0.7593        0.6412  0.2764\n",
      "     24        \u001b[36m0.4015\u001b[0m       0.6852        0.6795  0.2752\n",
      "     25        0.4169       0.7593        0.7076  0.2763\n",
      "     26        \u001b[36m0.3203\u001b[0m       0.6111        1.1030  0.2761\n",
      "     27        0.5940       0.7778        0.6241  0.2761\n",
      "     28        0.3331       0.7037        0.8235  0.2763\n",
      "     29        0.4683       0.7222        0.7130  0.2775\n",
      "     30        0.3986       0.7222        0.6679  0.2761\n",
      "     31        0.3838       0.6481        0.6792  0.2762\n",
      "     32        0.4389       0.7222        0.7293  0.2754\n",
      "     33        \u001b[36m0.2381\u001b[0m       0.7593        0.7357  0.2766\n",
      "     34        \u001b[36m0.1855\u001b[0m       0.7407        0.7388  0.2762\n",
      "     35        0.2474       0.6111        1.3039  0.2766\n",
      "     36        0.3251       0.6481        0.9247  0.2765\n",
      "     37        0.4192       0.6111        0.9245  0.2759\n",
      "     38        0.1873       0.7222        0.7734  0.2766\n",
      "     39        0.2026       0.7222        0.7476  0.2763\n",
      "     40        \u001b[36m0.0854\u001b[0m       0.7037        0.8356  0.2762\n",
      "     41        0.1341       0.7407        0.8964  0.2759\n",
      "     42        0.0985       0.7037        0.9615  0.2765\n",
      "     43        \u001b[36m0.0774\u001b[0m       0.6481        1.3089  0.2769\n",
      "     44        0.3675       0.7037        0.9953  0.2758\n",
      "     45        0.2298       0.6667        1.1194  0.2765\n",
      "     46        \u001b[36m0.0668\u001b[0m       0.6481        1.4643  0.2765\n",
      "     47        \u001b[36m0.0412\u001b[0m       0.6667        1.0083  0.2774\n",
      "     48        0.1115       0.6481        1.4019  0.2768\n",
      "     49        \u001b[36m0.0247\u001b[0m       0.6852        0.9599  0.2762\n",
      "     50        \u001b[36m0.0174\u001b[0m       0.6667        1.0146  0.2760\n",
      "Re-initializing module because the following parameters were re-set: .\n",
      "Re-initializing optimizer because the following parameters were re-set: .\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0903\u001b[0m       \u001b[32m0.6000\u001b[0m        \u001b[35m1.0050\u001b[0m  0.2769\n",
      "      2        \u001b[36m0.9774\u001b[0m       \u001b[32m0.6545\u001b[0m        \u001b[35m0.8294\u001b[0m  0.2765\n",
      "      3        1.1019       0.3273        1.1187  0.2774\n",
      "      4        1.1105       0.3273        1.1055  0.2764\n",
      "      5        1.1042       0.3273        1.1022  0.2766\n",
      "      6        1.1027       0.3273        1.1013  0.2765\n",
      "      7        1.1022       0.3273        1.1009  0.2764\n",
      "      8        1.1020       0.3273        1.1008  0.2768\n",
      "      9        1.1020       0.3273        1.1007  0.2764\n",
      "     10        1.1019       0.3273        1.1007  0.2768\n",
      "     11        1.1019       0.3273        1.1007  0.2767\n",
      "     12        1.1017       0.3273        1.1004  0.2766\n",
      "     13        1.1012       0.3273        1.0984  0.2775\n",
      "     14        1.0943       0.3273        1.0955  0.2770\n",
      "     15        1.1019       0.3273        1.2520  0.2767\n",
      "     16        1.1157       0.3273        1.0999  0.2766\n",
      "     17        1.1015       0.3273        1.1003  0.2770\n",
      "     18        1.1017       0.3273        1.1005  0.2779\n",
      "     19        1.1018       0.3273        1.1006  0.2779\n",
      "     20        1.1018       0.3273        1.1007  0.2771\n",
      "     21        1.1019       0.3273        1.1007  0.2767\n",
      "     22        1.1019       0.3273        1.1007  0.2764\n",
      "     23        1.1019       0.3273        1.1007  0.2769\n",
      "     24        1.1019       0.3273        1.1007  0.2768\n",
      "     25        1.1018       0.3273        1.1007  0.2765\n",
      "     26        1.1018       0.3273        1.1007  0.2763\n",
      "     27        1.1018       0.3273        1.1007  0.2772\n",
      "     28        1.1018       0.3273        1.1007  0.2771\n",
      "     29        1.1018       0.3273        1.1007  0.2764\n",
      "     30        1.1018       0.3273        1.1007  0.2764\n",
      "     31        1.1018       0.3273        1.1007  0.2762\n",
      "     32        1.1018       0.3273        1.1006  0.2768\n",
      "     33        1.1018       0.3273        1.1006  0.2779\n",
      "     34        1.1018       0.3273        1.1006  0.2768\n",
      "     35        1.1017       0.3273        1.1005  0.2767\n",
      "     36        1.1016       0.3273        1.1003  0.2767\n",
      "     37        1.1013       0.3273        1.1003  0.2771\n",
      "     38        1.0993       0.3273        1.0983  0.2775\n",
      "     39        1.0512       0.3636        1.0674  0.2769\n",
      "     40        \u001b[36m0.9523\u001b[0m       0.6000        \u001b[35m0.8056\u001b[0m  0.2764\n",
      "     41        1.1179       0.3273        1.1240  0.2773\n",
      "     42        1.1128       0.3273        1.1069  0.2775\n",
      "     43        1.1048       0.3273        1.1027  0.2764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     44        1.1028       0.3273        1.1014  0.2764\n",
      "     45        1.1022       0.3273        1.1009  0.2769\n",
      "     46        1.1020       0.3273        1.1008  0.2703\n",
      "     47        1.1019       0.3273        1.1007  0.2697\n",
      "     48        1.1018       0.3273        1.1007  0.2696\n",
      "     49        1.1018       0.3273        1.1006  0.2698\n",
      "     50        1.1018       0.3273        1.1006  0.2695\n",
      "Re-initializing module because the following parameters were re-set: .\n",
      "Re-initializing optimizer because the following parameters were re-set: .\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1033\u001b[0m       \u001b[32m0.3333\u001b[0m        \u001b[35m1.0704\u001b[0m  0.2688\n",
      "      2        \u001b[36m1.0716\u001b[0m       0.3333        1.1018  0.2702\n",
      "      3        \u001b[36m1.0460\u001b[0m       \u001b[32m0.4630\u001b[0m        \u001b[35m1.0409\u001b[0m  0.2740\n",
      "      4        \u001b[36m0.9770\u001b[0m       \u001b[32m0.6296\u001b[0m        \u001b[35m0.9266\u001b[0m  0.2733\n",
      "      5        \u001b[36m0.9279\u001b[0m       0.5741        \u001b[35m0.8999\u001b[0m  0.2759\n",
      "      6        \u001b[36m0.8937\u001b[0m       0.3889        1.0419  0.2781\n",
      "      7        \u001b[36m0.8206\u001b[0m       \u001b[32m0.6667\u001b[0m        \u001b[35m0.7269\u001b[0m  0.2751\n",
      "      8        \u001b[36m0.7906\u001b[0m       \u001b[32m0.7222\u001b[0m        \u001b[35m0.7028\u001b[0m  0.2903\n",
      "      9        \u001b[36m0.7800\u001b[0m       0.5926        0.7717  0.3034\n",
      "     10        \u001b[36m0.7581\u001b[0m       0.6481        \u001b[35m0.6732\u001b[0m  0.2927\n",
      "     11        \u001b[36m0.7340\u001b[0m       0.7222        \u001b[35m0.6504\u001b[0m  0.2724\n",
      "     12        \u001b[36m0.7035\u001b[0m       0.5741        0.7403  0.3180\n",
      "     13        0.7174       0.5741        0.7057  0.2702\n",
      "     14        \u001b[36m0.6953\u001b[0m       0.6667        0.7167  0.2872\n",
      "     15        0.7606       0.5000        1.3233  0.2926\n",
      "     16        1.3242       0.3333        1.1316  0.2893\n",
      "     17        1.1244       0.3333        1.1023  0.2707\n",
      "     18        1.1095       0.3333        1.0992  0.2699\n",
      "     19        1.1074       0.3333        1.0992  0.2934\n",
      "     20        1.1071       0.3333        1.0993  0.3086\n",
      "     21        1.1069       0.3333        1.0993  0.2951\n",
      "     22        1.1066       0.3333        1.0994  0.2869\n",
      "     23        1.1061       0.3333        1.0994  0.2963\n",
      "     24        1.1052       0.3333        1.0995  0.2976\n",
      "     25        1.0773       0.6296        0.9657  0.2794\n",
      "     26        1.1414       0.3333        1.0991  0.2713\n",
      "     27        1.1054       0.3333        1.0987  0.2831\n",
      "     28        1.0991       0.3519        1.0916  0.2834\n",
      "     29        1.0995       0.6296        1.0110  0.3052\n",
      "     30        1.0862       0.3333        1.1104  0.2783\n",
      "     31        1.1066       0.3333        1.1111  0.2750\n",
      "     32        1.1057       0.3333        1.1084  0.2830\n",
      "     33        1.1054       0.3333        1.1122  0.2805\n",
      "     34        1.1056       0.3333        1.1120  0.2768\n",
      "     35        1.1056       0.3333        1.1106  0.2980\n",
      "     36        1.1054       0.3333        1.1063  0.2974\n",
      "     37        1.1059       0.3333        1.1111  0.2727\n",
      "     38        1.0816       0.5741        1.0739  0.2694\n",
      "     39        1.0630       0.4815        0.9979  0.2698\n",
      "     40        1.0293       0.5185        0.9982  0.2711\n",
      "     41        0.9915       0.5926        0.8185  0.2707\n",
      "     42        1.0525       0.3704        1.0882  0.2696\n",
      "     43        1.0102       0.3333        1.2466  0.2692\n",
      "     44        1.1095       0.3333        1.1586  0.2724\n",
      "     45        1.1034       0.3333        1.1660  0.2717\n",
      "     46        1.1035       0.3333        1.1300  0.2697\n",
      "     47        1.1019       0.3333        1.1198  0.2705\n",
      "     48        1.1013       0.3333        1.1126  0.2710\n",
      "     49        1.1010       0.3704        1.1101  0.2708\n",
      "     50        1.1008       0.3889        1.1029  0.2701\n",
      "0.78 {'batch_size': 10, 'lr': 0.005}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadas/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "params = {\n",
    "    'lr': [0.005, 0.01, 0.05, 0.1],\n",
    "    'batch_size': [5, 10],\n",
    "}\n",
    "gs = GridSearchCV(net, params, refit=False, cv=3, scoring='accuracy')\n",
    "\n",
    "gs.fit(train_images.numpy(), train_data.numpy())\n",
    "print(gs.best_score_, gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1026\u001b[0m       \u001b[32m0.5432\u001b[0m        \u001b[35m1.0994\u001b[0m  0.4464\n",
      "      2        \u001b[36m1.0983\u001b[0m       0.5432        \u001b[35m1.0953\u001b[0m  0.4035\n",
      "      3        \u001b[36m1.0945\u001b[0m       \u001b[32m0.5556\u001b[0m        \u001b[35m1.0899\u001b[0m  0.4034\n",
      "      4        \u001b[36m1.0893\u001b[0m       \u001b[32m0.5679\u001b[0m        \u001b[35m1.0826\u001b[0m  0.4009\n",
      "      5        \u001b[36m1.0826\u001b[0m       0.5679        \u001b[35m1.0737\u001b[0m  0.4009\n",
      "      6        \u001b[36m1.0750\u001b[0m       \u001b[32m0.7037\u001b[0m        \u001b[35m1.0637\u001b[0m  0.4003\n",
      "      7        \u001b[36m1.0663\u001b[0m       0.6914        \u001b[35m1.0520\u001b[0m  0.4004\n",
      "      8        \u001b[36m1.0556\u001b[0m       0.5926        \u001b[35m1.0370\u001b[0m  0.4003\n",
      "      9        \u001b[36m1.0424\u001b[0m       0.5926        \u001b[35m1.0181\u001b[0m  0.4002\n",
      "     10        \u001b[36m1.0259\u001b[0m       0.6296        \u001b[35m0.9941\u001b[0m  0.4013\n",
      "     11        \u001b[36m1.0051\u001b[0m       0.7037        \u001b[35m0.9636\u001b[0m  0.3991\n",
      "     12        \u001b[36m0.9792\u001b[0m       0.6914        \u001b[35m0.9248\u001b[0m  0.4039\n",
      "     13        \u001b[36m0.9482\u001b[0m       \u001b[32m0.7531\u001b[0m        \u001b[35m0.8798\u001b[0m  0.4029\n",
      "     14        \u001b[36m0.9157\u001b[0m       \u001b[32m0.7654\u001b[0m        \u001b[35m0.8334\u001b[0m  0.4013\n",
      "     15        \u001b[36m0.8860\u001b[0m       0.7407        \u001b[35m0.7905\u001b[0m  0.3992\n",
      "     16        \u001b[36m0.8629\u001b[0m       0.7284        \u001b[35m0.7551\u001b[0m  0.3990\n",
      "     17        \u001b[36m0.8461\u001b[0m       0.7284        \u001b[35m0.7286\u001b[0m  0.3979\n",
      "     18        \u001b[36m0.8356\u001b[0m       0.7160        \u001b[35m0.7095\u001b[0m  0.3992\n",
      "     19        \u001b[36m0.8283\u001b[0m       0.7284        \u001b[35m0.6958\u001b[0m  0.4008\n",
      "     20        \u001b[36m0.8226\u001b[0m       0.7284        \u001b[35m0.6857\u001b[0m  0.4064\n",
      "     21        \u001b[36m0.8167\u001b[0m       0.7284        \u001b[35m0.6779\u001b[0m  0.4004\n",
      "     22        \u001b[36m0.8111\u001b[0m       0.7407        \u001b[35m0.6719\u001b[0m  0.4012\n",
      "     23        \u001b[36m0.8063\u001b[0m       0.7407        \u001b[35m0.6657\u001b[0m  0.3995\n",
      "     24        \u001b[36m0.8004\u001b[0m       0.7407        \u001b[35m0.6602\u001b[0m  0.4010\n",
      "     25        \u001b[36m0.7952\u001b[0m       0.7531        \u001b[35m0.6556\u001b[0m  0.4004\n",
      "     26        \u001b[36m0.7901\u001b[0m       0.7531        \u001b[35m0.6512\u001b[0m  0.4004\n",
      "     27        \u001b[36m0.7843\u001b[0m       0.7531        \u001b[35m0.6470\u001b[0m  0.4000\n",
      "     28        \u001b[36m0.7778\u001b[0m       0.7531        \u001b[35m0.6425\u001b[0m  0.4000\n",
      "     29        \u001b[36m0.7715\u001b[0m       0.7407        \u001b[35m0.6381\u001b[0m  0.3995\n",
      "     30        \u001b[36m0.7655\u001b[0m       0.7284        \u001b[35m0.6336\u001b[0m  0.4022\n",
      "     31        \u001b[36m0.7592\u001b[0m       0.7284        \u001b[35m0.6294\u001b[0m  0.3999\n",
      "     32        \u001b[36m0.7534\u001b[0m       0.7407        \u001b[35m0.6259\u001b[0m  0.3997\n",
      "     33        \u001b[36m0.7469\u001b[0m       0.7531        \u001b[35m0.6218\u001b[0m  0.3993\n",
      "     34        \u001b[36m0.7412\u001b[0m       0.7531        \u001b[35m0.6182\u001b[0m  0.3993\n",
      "     35        \u001b[36m0.7358\u001b[0m       0.7654        \u001b[35m0.6146\u001b[0m  0.3992\n",
      "     36        \u001b[36m0.7303\u001b[0m       \u001b[32m0.7778\u001b[0m        \u001b[35m0.6116\u001b[0m  0.4004\n",
      "     37        \u001b[36m0.7260\u001b[0m       0.7778        \u001b[35m0.6092\u001b[0m  0.4000\n",
      "     38        \u001b[36m0.7212\u001b[0m       0.7778        \u001b[35m0.6067\u001b[0m  0.3992\n",
      "     39        \u001b[36m0.7171\u001b[0m       \u001b[32m0.7901\u001b[0m        \u001b[35m0.6046\u001b[0m  0.3996\n",
      "     40        \u001b[36m0.7122\u001b[0m       \u001b[32m0.8025\u001b[0m        \u001b[35m0.6027\u001b[0m  0.3987\n",
      "     41        \u001b[36m0.7079\u001b[0m       0.8025        \u001b[35m0.6005\u001b[0m  0.3997\n",
      "     42        \u001b[36m0.7037\u001b[0m       0.8025        \u001b[35m0.5989\u001b[0m  0.4082\n",
      "     43        \u001b[36m0.6996\u001b[0m       0.8025        \u001b[35m0.5971\u001b[0m  0.4083\n",
      "     44        \u001b[36m0.6953\u001b[0m       0.8025        \u001b[35m0.5965\u001b[0m  0.4097\n",
      "     45        \u001b[36m0.6914\u001b[0m       0.8025        \u001b[35m0.5953\u001b[0m  0.4091\n",
      "     46        \u001b[36m0.6870\u001b[0m       0.7901        \u001b[35m0.5941\u001b[0m  0.4081\n",
      "     47        \u001b[36m0.6828\u001b[0m       0.7901        \u001b[35m0.5927\u001b[0m  0.4069\n",
      "     48        \u001b[36m0.6782\u001b[0m       0.7901        \u001b[35m0.5914\u001b[0m  0.4074\n",
      "     49        \u001b[36m0.6735\u001b[0m       0.8025        \u001b[35m0.5901\u001b[0m  0.4080\n",
      "     50        \u001b[36m0.6689\u001b[0m       0.8025        \u001b[35m0.5885\u001b[0m  0.4009\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "  module_=Net(\n",
       "    (conv1): Conv2d(1, 3, kernel_size=(6, 6), stride=(1, 1))\n",
       "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (fc1): Linear(in_features=46875, out_features=100, bias=True)\n",
       "    (fc2): Linear(in_features=100, out_features=30, bias=True)\n",
       "    (fc3): Linear(in_features=30, out_features=3, bias=True)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = NeuralNetClassifier(\n",
    "    Net,\n",
    "    max_epochs=50,\n",
    "    lr=0.005,\n",
    "    batch_size = 10,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "net.fit(train_images, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = net.predict(validation_images)\n",
    "np.mean(predicted == validation_data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = net.predict(test_images)\n",
    "np.mean(predicted == test_data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos son nuestros parámetros de red, usaremos como y la función de costo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(log_interval, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, model, device, test_loader, best_accuracy, best_epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    if best_accuracy <= correct:\n",
    "        best_accuracy = correct\n",
    "        best_epoch = epoch\n",
    "        print('This is the best test accuracy to date.')\n",
    "    print('The corresonding test accuracy : {}/{}'.format(correct, \n",
    "        len(test_loader.dataset)))\n",
    "    \n",
    "    return best_accuracy, best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, device, validation_loader):\n",
    "    model.eval()\n",
    "    validation_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in validation_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            validation_loss += F.nll_loss(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    validation_loss /= len(validation_loader.dataset)\n",
    "    print('The corresonding validation accuracy : {}/{}'.format(correct, \n",
    "        len(validation_loader.dataset)))\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, params_net, epochs, schedule = False):\n",
    "    optimizer = optim.SGD(model.parameters(), **params_net)\n",
    "    best_accuracy = 0\n",
    "    best_epoch = 0\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                        'max', verbose = True, patience = 20) \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(50, model, device, train_dl, optimizer, epoch)\n",
    "        best_accuracy, best_epoch = test(epoch, model, device, \n",
    "                                         test_dl, best_accuracy, best_epoch)\n",
    "        correct = validation(model,  device, validation_dl)\n",
    "        \n",
    "        if schedule:\n",
    "            scheduler.step(correct)\n",
    "        \n",
    "        print('\\n')\n",
    "        \n",
    "    print(\"Finished training network.\")\n",
    "    print(\"Corresponding test accuracy of {}/{} obtained in epoch {}\".format(best_accuracy, \n",
    "                                                            len(test_dl.dataset), best_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/400 (0%)]\tLoss: -0.341280\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 42/100\n",
      "The corresonding validation accuracy : 25/100\n",
      "\n",
      "\n",
      "Train Epoch: 2 [0/400 (0%)]\tLoss: -0.333387\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 42/100\n",
      "The corresonding validation accuracy : 25/100\n",
      "\n",
      "\n",
      "Train Epoch: 3 [0/400 (0%)]\tLoss: -0.328278\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 42/100\n",
      "The corresonding validation accuracy : 25/100\n",
      "\n",
      "\n",
      "Train Epoch: 4 [0/400 (0%)]\tLoss: -0.337588\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 42/100\n",
      "The corresonding validation accuracy : 25/100\n",
      "\n",
      "\n",
      "Train Epoch: 5 [0/400 (0%)]\tLoss: -0.322546\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 42/100\n",
      "The corresonding validation accuracy : 25/100\n",
      "\n",
      "\n",
      "Train Epoch: 6 [0/400 (0%)]\tLoss: -0.336310\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 42/100\n",
      "The corresonding validation accuracy : 25/100\n",
      "\n",
      "\n",
      "Train Epoch: 7 [0/400 (0%)]\tLoss: -0.319240\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 42/100\n",
      "The corresonding validation accuracy : 25/100\n",
      "\n",
      "\n",
      "Train Epoch: 8 [0/400 (0%)]\tLoss: -0.345466\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 43/100\n",
      "The corresonding validation accuracy : 26/100\n",
      "\n",
      "\n",
      "Train Epoch: 9 [0/400 (0%)]\tLoss: -0.341230\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 47/100\n",
      "The corresonding validation accuracy : 37/100\n",
      "\n",
      "\n",
      "Train Epoch: 10 [0/400 (0%)]\tLoss: -0.325398\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 53/100\n",
      "The corresonding validation accuracy : 44/100\n",
      "\n",
      "\n",
      "Train Epoch: 11 [0/400 (0%)]\tLoss: -0.342404\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 53/100\n",
      "The corresonding validation accuracy : 44/100\n",
      "\n",
      "\n",
      "Train Epoch: 12 [0/400 (0%)]\tLoss: -0.326928\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 60/100\n",
      "The corresonding validation accuracy : 51/100\n",
      "\n",
      "\n",
      "Train Epoch: 13 [0/400 (0%)]\tLoss: -0.334470\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 62/100\n",
      "The corresonding validation accuracy : 53/100\n",
      "\n",
      "\n",
      "Train Epoch: 14 [0/400 (0%)]\tLoss: -0.362686\n",
      "The corresonding test accuracy : 50/100\n",
      "The corresonding validation accuracy : 41/100\n",
      "\n",
      "\n",
      "Train Epoch: 15 [0/400 (0%)]\tLoss: -0.481183\n",
      "The corresonding test accuracy : 50/100\n",
      "The corresonding validation accuracy : 41/100\n",
      "\n",
      "\n",
      "Train Epoch: 16 [0/400 (0%)]\tLoss: -0.349467\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 62/100\n",
      "The corresonding validation accuracy : 54/100\n",
      "\n",
      "\n",
      "Train Epoch: 17 [0/400 (0%)]\tLoss: -0.385887\n",
      "The corresonding test accuracy : 61/100\n",
      "The corresonding validation accuracy : 53/100\n",
      "\n",
      "\n",
      "Train Epoch: 18 [0/400 (0%)]\tLoss: -0.430662\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 64/100\n",
      "The corresonding validation accuracy : 55/100\n",
      "\n",
      "\n",
      "Train Epoch: 19 [0/400 (0%)]\tLoss: -0.421848\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 66/100\n",
      "The corresonding validation accuracy : 53/100\n",
      "\n",
      "\n",
      "Train Epoch: 20 [0/400 (0%)]\tLoss: -0.550255\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 67/100\n",
      "The corresonding validation accuracy : 53/100\n",
      "\n",
      "\n",
      "Train Epoch: 21 [0/400 (0%)]\tLoss: -0.483104\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 69/100\n",
      "The corresonding validation accuracy : 53/100\n",
      "\n",
      "\n",
      "Train Epoch: 22 [0/400 (0%)]\tLoss: -0.574426\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 70/100\n",
      "The corresonding validation accuracy : 56/100\n",
      "\n",
      "\n",
      "Train Epoch: 23 [0/400 (0%)]\tLoss: -0.572233\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 72/100\n",
      "The corresonding validation accuracy : 57/100\n",
      "\n",
      "\n",
      "Train Epoch: 24 [0/400 (0%)]\tLoss: -0.529622\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 73/100\n",
      "The corresonding validation accuracy : 58/100\n",
      "\n",
      "\n",
      "Train Epoch: 25 [0/400 (0%)]\tLoss: -0.581499\n",
      "The corresonding test accuracy : 72/100\n",
      "The corresonding validation accuracy : 63/100\n",
      "\n",
      "\n",
      "Train Epoch: 26 [0/400 (0%)]\tLoss: -0.526077\n",
      "The corresonding test accuracy : 72/100\n",
      "The corresonding validation accuracy : 65/100\n",
      "\n",
      "\n",
      "Train Epoch: 27 [0/400 (0%)]\tLoss: -0.486334\n",
      "The corresonding test accuracy : 64/100\n",
      "The corresonding validation accuracy : 71/100\n",
      "\n",
      "\n",
      "Train Epoch: 28 [0/400 (0%)]\tLoss: -0.717975\n",
      "The corresonding test accuracy : 72/100\n",
      "The corresonding validation accuracy : 67/100\n",
      "\n",
      "\n",
      "Train Epoch: 29 [0/400 (0%)]\tLoss: -0.780940\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 73/100\n",
      "The corresonding validation accuracy : 69/100\n",
      "\n",
      "\n",
      "Train Epoch: 30 [0/400 (0%)]\tLoss: -0.629167\n",
      "The corresonding test accuracy : 72/100\n",
      "The corresonding validation accuracy : 71/100\n",
      "\n",
      "\n",
      "Train Epoch: 31 [0/400 (0%)]\tLoss: -0.674429\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 74/100\n",
      "The corresonding validation accuracy : 67/100\n",
      "\n",
      "\n",
      "Train Epoch: 32 [0/400 (0%)]\tLoss: -0.660198\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 76/100\n",
      "The corresonding validation accuracy : 70/100\n",
      "\n",
      "\n",
      "Train Epoch: 33 [0/400 (0%)]\tLoss: -0.683246\n",
      "The corresonding test accuracy : 73/100\n",
      "The corresonding validation accuracy : 73/100\n",
      "\n",
      "\n",
      "Train Epoch: 34 [0/400 (0%)]\tLoss: -0.820861\n",
      "The corresonding test accuracy : 73/100\n",
      "The corresonding validation accuracy : 72/100\n",
      "\n",
      "\n",
      "Train Epoch: 35 [0/400 (0%)]\tLoss: -0.935097\n",
      "The corresonding test accuracy : 72/100\n",
      "The corresonding validation accuracy : 75/100\n",
      "\n",
      "\n",
      "Train Epoch: 36 [0/400 (0%)]\tLoss: -0.579027\n",
      "The corresonding test accuracy : 75/100\n",
      "The corresonding validation accuracy : 70/100\n",
      "\n",
      "\n",
      "Train Epoch: 37 [0/400 (0%)]\tLoss: -0.924424\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 76/100\n",
      "The corresonding validation accuracy : 72/100\n",
      "\n",
      "\n",
      "Train Epoch: 38 [0/400 (0%)]\tLoss: -0.687997\n",
      "The corresonding test accuracy : 75/100\n",
      "The corresonding validation accuracy : 70/100\n",
      "\n",
      "\n",
      "Train Epoch: 39 [0/400 (0%)]\tLoss: -0.856400\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 79/100\n",
      "The corresonding validation accuracy : 70/100\n",
      "\n",
      "\n",
      "Train Epoch: 40 [0/400 (0%)]\tLoss: -0.837513\n",
      "The corresonding test accuracy : 76/100\n",
      "The corresonding validation accuracy : 73/100\n",
      "\n",
      "\n",
      "Train Epoch: 41 [0/400 (0%)]\tLoss: -0.901800\n",
      "The corresonding test accuracy : 74/100\n",
      "The corresonding validation accuracy : 74/100\n",
      "\n",
      "\n",
      "Train Epoch: 42 [0/400 (0%)]\tLoss: -0.795763\n",
      "The corresonding test accuracy : 75/100\n",
      "The corresonding validation accuracy : 72/100\n",
      "\n",
      "\n",
      "Train Epoch: 43 [0/400 (0%)]\tLoss: -0.762591\n",
      "The corresonding test accuracy : 74/100\n",
      "The corresonding validation accuracy : 74/100\n",
      "\n",
      "\n",
      "Train Epoch: 44 [0/400 (0%)]\tLoss: -0.775501\n",
      "The corresonding test accuracy : 71/100\n",
      "The corresonding validation accuracy : 76/100\n",
      "\n",
      "\n",
      "Train Epoch: 45 [0/400 (0%)]\tLoss: -0.598643\n",
      "The corresonding test accuracy : 75/100\n",
      "The corresonding validation accuracy : 69/100\n",
      "\n",
      "\n",
      "Train Epoch: 46 [0/400 (0%)]\tLoss: -0.933089\n",
      "The corresonding test accuracy : 75/100\n",
      "The corresonding validation accuracy : 71/100\n",
      "\n",
      "\n",
      "Train Epoch: 47 [0/400 (0%)]\tLoss: -0.684992\n",
      "The corresonding test accuracy : 76/100\n",
      "The corresonding validation accuracy : 71/100\n",
      "\n",
      "\n",
      "Train Epoch: 48 [0/400 (0%)]\tLoss: -0.818150\n",
      "The corresonding test accuracy : 72/100\n",
      "The corresonding validation accuracy : 79/100\n",
      "\n",
      "\n",
      "Train Epoch: 49 [0/400 (0%)]\tLoss: -0.916282\n",
      "The corresonding test accuracy : 76/100\n",
      "The corresonding validation accuracy : 72/100\n",
      "\n",
      "\n",
      "Train Epoch: 50 [0/400 (0%)]\tLoss: -0.922887\n",
      "The corresonding test accuracy : 72/100\n",
      "The corresonding validation accuracy : 74/100\n",
      "\n",
      "\n",
      "Finished training network.\n",
      "Corresponding test accuracy of 79/100 obtained in epoch 39\n"
     ]
    }
   ],
   "source": [
    "params_net = {'lr':0.005}\n",
    "fit(Net().to(device), params_net, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La mejor precisión fue del 80% sobre el conjunto de prueba. Ahora usaremos un algoritmo de actualización de la razón de aprendizaje para mejorar nuestra red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params_net = {'lr':0.01, 'momentum':0.1}\n",
    "# fit(Net().to(device), params_net, schedule= schudle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usemos una red convolutiva más..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_new(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_new, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 3, 6)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(3, 6, 6)\n",
    "        self.fc1 = nn.Linear(6 * 60 * 60, 100)\n",
    "        self.fc2 = nn.Linear(100, 30)\n",
    "        self.fc3 = nn.Linear(30, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 255, 255)\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 6 * 60* 60)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.softmax(self.fc3(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/400 (0%)]\tLoss: -0.309058\n",
      "Train Epoch: 1 [250/400 (62%)]\tLoss: -0.333694\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 42/100\n",
      "The corresonding validation accuracy : 25/100\n",
      "\n",
      "\n",
      "Train Epoch: 2 [0/400 (0%)]\tLoss: -0.334055\n",
      "Train Epoch: 2 [250/400 (62%)]\tLoss: -0.355570\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 42/100\n",
      "The corresonding validation accuracy : 25/100\n",
      "\n",
      "\n",
      "Train Epoch: 3 [0/400 (0%)]\tLoss: -0.355692\n",
      "Train Epoch: 3 [250/400 (62%)]\tLoss: -0.317056\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 42/100\n",
      "The corresonding validation accuracy : 25/100\n",
      "\n",
      "\n",
      "Train Epoch: 4 [0/400 (0%)]\tLoss: -0.325824\n",
      "Train Epoch: 4 [250/400 (62%)]\tLoss: -0.349789\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 42/100\n",
      "The corresonding validation accuracy : 25/100\n",
      "\n",
      "\n",
      "Train Epoch: 5 [0/400 (0%)]\tLoss: -0.314930\n",
      "Train Epoch: 5 [250/400 (62%)]\tLoss: -0.335289\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 42/100\n",
      "The corresonding validation accuracy : 25/100\n",
      "\n",
      "\n",
      "Train Epoch: 6 [0/400 (0%)]\tLoss: -0.341155\n",
      "Train Epoch: 6 [250/400 (62%)]\tLoss: -0.353955\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 42/100\n",
      "The corresonding validation accuracy : 25/100\n",
      "\n",
      "\n",
      "Train Epoch: 7 [0/400 (0%)]\tLoss: -0.307535\n",
      "Train Epoch: 7 [250/400 (62%)]\tLoss: -0.327488\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 42/100\n",
      "The corresonding validation accuracy : 25/100\n",
      "\n",
      "\n",
      "Train Epoch: 8 [0/400 (0%)]\tLoss: -0.327618\n",
      "Train Epoch: 8 [250/400 (62%)]\tLoss: -0.341406\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 42/100\n",
      "The corresonding validation accuracy : 25/100\n",
      "\n",
      "\n",
      "Train Epoch: 9 [0/400 (0%)]\tLoss: -0.323969\n",
      "Train Epoch: 9 [250/400 (62%)]\tLoss: -0.336965\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 42/100\n",
      "The corresonding validation accuracy : 25/100\n",
      "\n",
      "\n",
      "Train Epoch: 10 [0/400 (0%)]\tLoss: -0.350006\n",
      "Train Epoch: 10 [250/400 (62%)]\tLoss: -0.329384\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 42/100\n",
      "The corresonding validation accuracy : 25/100\n",
      "\n",
      "\n",
      "Train Epoch: 11 [0/400 (0%)]\tLoss: -0.345672\n",
      "Train Epoch: 11 [250/400 (62%)]\tLoss: -0.352241\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 54/100\n",
      "The corresonding validation accuracy : 45/100\n",
      "\n",
      "\n",
      "Train Epoch: 12 [0/400 (0%)]\tLoss: -0.325326\n",
      "Train Epoch: 12 [250/400 (62%)]\tLoss: -0.363576\n",
      "The corresonding test accuracy : 51/100\n",
      "The corresonding validation accuracy : 44/100\n",
      "\n",
      "\n",
      "Train Epoch: 13 [0/400 (0%)]\tLoss: -0.331597\n",
      "Train Epoch: 13 [250/400 (62%)]\tLoss: -0.576754\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 68/100\n",
      "The corresonding validation accuracy : 53/100\n",
      "\n",
      "\n",
      "Train Epoch: 14 [0/400 (0%)]\tLoss: -0.456230\n",
      "Train Epoch: 14 [250/400 (62%)]\tLoss: -0.624629\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 70/100\n",
      "The corresonding validation accuracy : 71/100\n",
      "\n",
      "\n",
      "Train Epoch: 15 [0/400 (0%)]\tLoss: -0.798637\n",
      "Train Epoch: 15 [250/400 (62%)]\tLoss: -0.316823\n",
      "The corresonding test accuracy : 65/100\n",
      "The corresonding validation accuracy : 60/100\n",
      "\n",
      "\n",
      "Train Epoch: 16 [0/400 (0%)]\tLoss: -0.589816\n",
      "Train Epoch: 16 [250/400 (62%)]\tLoss: -0.866682\n",
      "The corresonding test accuracy : 67/100\n",
      "The corresonding validation accuracy : 68/100\n",
      "\n",
      "\n",
      "Train Epoch: 17 [0/400 (0%)]\tLoss: -0.812075\n",
      "Train Epoch: 17 [250/400 (62%)]\tLoss: -0.558836\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 77/100\n",
      "The corresonding validation accuracy : 70/100\n",
      "\n",
      "\n",
      "Train Epoch: 18 [0/400 (0%)]\tLoss: -0.803268\n",
      "Train Epoch: 18 [250/400 (62%)]\tLoss: -0.731759\n",
      "The corresonding test accuracy : 76/100\n",
      "The corresonding validation accuracy : 75/100\n",
      "\n",
      "\n",
      "Train Epoch: 19 [0/400 (0%)]\tLoss: -0.665002\n",
      "Train Epoch: 19 [250/400 (62%)]\tLoss: -0.945002\n",
      "The corresonding test accuracy : 76/100\n",
      "The corresonding validation accuracy : 72/100\n",
      "\n",
      "\n",
      "Train Epoch: 20 [0/400 (0%)]\tLoss: -0.688938\n",
      "Train Epoch: 20 [250/400 (62%)]\tLoss: -0.682003\n",
      "The corresonding test accuracy : 72/100\n",
      "The corresonding validation accuracy : 77/100\n",
      "\n",
      "\n",
      "Train Epoch: 21 [0/400 (0%)]\tLoss: -0.999888\n",
      "Train Epoch: 21 [250/400 (62%)]\tLoss: -0.756920\n",
      "The corresonding test accuracy : 70/100\n",
      "The corresonding validation accuracy : 84/100\n",
      "\n",
      "\n",
      "Train Epoch: 22 [0/400 (0%)]\tLoss: -0.984432\n",
      "Train Epoch: 22 [250/400 (62%)]\tLoss: -0.633234\n",
      "The corresonding test accuracy : 66/100\n",
      "The corresonding validation accuracy : 77/100\n",
      "\n",
      "\n",
      "Train Epoch: 23 [0/400 (0%)]\tLoss: -0.797977\n",
      "Train Epoch: 23 [250/400 (62%)]\tLoss: -0.796924\n",
      "The corresonding test accuracy : 69/100\n",
      "The corresonding validation accuracy : 79/100\n",
      "\n",
      "\n",
      "Train Epoch: 24 [0/400 (0%)]\tLoss: -0.971216\n",
      "Train Epoch: 24 [250/400 (62%)]\tLoss: -0.970999\n",
      "The corresonding test accuracy : 70/100\n",
      "The corresonding validation accuracy : 77/100\n",
      "\n",
      "\n",
      "Train Epoch: 25 [0/400 (0%)]\tLoss: -0.799389\n",
      "Train Epoch: 25 [250/400 (62%)]\tLoss: -0.400043\n",
      "The corresonding test accuracy : 70/100\n",
      "The corresonding validation accuracy : 78/100\n",
      "\n",
      "\n",
      "Train Epoch: 26 [0/400 (0%)]\tLoss: -0.562229\n",
      "Train Epoch: 26 [250/400 (62%)]\tLoss: -0.831538\n",
      "The corresonding test accuracy : 70/100\n",
      "The corresonding validation accuracy : 81/100\n",
      "\n",
      "\n",
      "Train Epoch: 27 [0/400 (0%)]\tLoss: -0.999981\n",
      "Train Epoch: 27 [250/400 (62%)]\tLoss: -0.606820\n",
      "The corresonding test accuracy : 71/100\n",
      "The corresonding validation accuracy : 70/100\n",
      "\n",
      "\n",
      "Train Epoch: 28 [0/400 (0%)]\tLoss: -0.985305\n",
      "Train Epoch: 28 [250/400 (62%)]\tLoss: -0.999805\n",
      "The corresonding test accuracy : 75/100\n",
      "The corresonding validation accuracy : 77/100\n",
      "\n",
      "\n",
      "Train Epoch: 29 [0/400 (0%)]\tLoss: -0.400026\n",
      "Train Epoch: 29 [250/400 (62%)]\tLoss: -0.947552\n",
      "The corresonding test accuracy : 70/100\n",
      "The corresonding validation accuracy : 80/100\n",
      "\n",
      "\n",
      "Train Epoch: 30 [0/400 (0%)]\tLoss: -0.987103\n",
      "Train Epoch: 30 [250/400 (62%)]\tLoss: -0.697512\n",
      "The corresonding test accuracy : 71/100\n",
      "The corresonding validation accuracy : 74/100\n",
      "\n",
      "\n",
      "Train Epoch: 31 [0/400 (0%)]\tLoss: -0.653720\n",
      "Train Epoch: 31 [250/400 (62%)]\tLoss: -0.823568\n",
      "The corresonding test accuracy : 74/100\n",
      "The corresonding validation accuracy : 73/100\n",
      "\n",
      "\n",
      "Train Epoch: 32 [0/400 (0%)]\tLoss: -0.913286\n",
      "Train Epoch: 32 [250/400 (62%)]\tLoss: -0.716373\n",
      "The corresonding test accuracy : 65/100\n",
      "The corresonding validation accuracy : 67/100\n",
      "\n",
      "\n",
      "Train Epoch: 33 [0/400 (0%)]\tLoss: -0.831241\n",
      "Train Epoch: 33 [250/400 (62%)]\tLoss: -0.677824\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 80/100\n",
      "The corresonding validation accuracy : 73/100\n",
      "\n",
      "\n",
      "Train Epoch: 34 [0/400 (0%)]\tLoss: -0.798485\n",
      "Train Epoch: 34 [250/400 (62%)]\tLoss: -0.793739\n",
      "The corresonding test accuracy : 73/100\n",
      "The corresonding validation accuracy : 79/100\n",
      "\n",
      "\n",
      "Train Epoch: 35 [0/400 (0%)]\tLoss: -0.600006\n",
      "Train Epoch: 35 [250/400 (62%)]\tLoss: -0.583148\n",
      "The corresonding test accuracy : 67/100\n",
      "The corresonding validation accuracy : 82/100\n",
      "\n",
      "\n",
      "Train Epoch: 36 [0/400 (0%)]\tLoss: -0.788093\n",
      "Train Epoch: 36 [250/400 (62%)]\tLoss: -0.895894\n",
      "The corresonding test accuracy : 70/100\n",
      "The corresonding validation accuracy : 71/100\n",
      "\n",
      "\n",
      "Train Epoch: 37 [0/400 (0%)]\tLoss: -0.600000\n",
      "Train Epoch: 37 [250/400 (62%)]\tLoss: -0.600490\n",
      "The corresonding test accuracy : 76/100\n",
      "The corresonding validation accuracy : 74/100\n",
      "\n",
      "\n",
      "Train Epoch: 38 [0/400 (0%)]\tLoss: -0.800003\n",
      "Train Epoch: 38 [250/400 (62%)]\tLoss: -0.886249\n",
      "The corresonding test accuracy : 61/100\n",
      "The corresonding validation accuracy : 84/100\n",
      "\n",
      "\n",
      "Train Epoch: 39 [0/400 (0%)]\tLoss: -0.743728\n",
      "Train Epoch: 39 [250/400 (62%)]\tLoss: -1.000000\n",
      "The corresonding test accuracy : 73/100\n",
      "The corresonding validation accuracy : 74/100\n",
      "\n",
      "\n",
      "Train Epoch: 40 [0/400 (0%)]\tLoss: -0.815832\n",
      "Train Epoch: 40 [250/400 (62%)]\tLoss: -0.999993\n",
      "The corresonding test accuracy : 70/100\n",
      "The corresonding validation accuracy : 69/100\n",
      "\n",
      "\n",
      "Train Epoch: 41 [0/400 (0%)]\tLoss: -0.400095\n",
      "Train Epoch: 41 [250/400 (62%)]\tLoss: -0.600000\n",
      "The corresonding test accuracy : 72/100\n",
      "The corresonding validation accuracy : 72/100\n",
      "\n",
      "\n",
      "Train Epoch: 42 [0/400 (0%)]\tLoss: -0.999990\n",
      "Train Epoch: 42 [250/400 (62%)]\tLoss: -0.526676\n",
      "The corresonding test accuracy : 75/100\n",
      "The corresonding validation accuracy : 75/100\n",
      "\n",
      "\n",
      "Train Epoch: 43 [0/400 (0%)]\tLoss: -0.799809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 43 [250/400 (62%)]\tLoss: -0.999072\n",
      "The corresonding test accuracy : 41/100\n",
      "The corresonding validation accuracy : 44/100\n",
      "\n",
      "\n",
      "Train Epoch: 44 [0/400 (0%)]\tLoss: -0.600006\n",
      "Train Epoch: 44 [250/400 (62%)]\tLoss: -0.600000\n",
      "The corresonding test accuracy : 68/100\n",
      "The corresonding validation accuracy : 82/100\n",
      "\n",
      "\n",
      "Train Epoch: 45 [0/400 (0%)]\tLoss: -0.799999\n",
      "Train Epoch: 45 [250/400 (62%)]\tLoss: -1.000000\n",
      "The corresonding test accuracy : 73/100\n",
      "The corresonding validation accuracy : 72/100\n",
      "\n",
      "\n",
      "Train Epoch: 46 [0/400 (0%)]\tLoss: -0.786904\n",
      "Train Epoch: 46 [250/400 (62%)]\tLoss: -0.799997\n",
      "The corresonding test accuracy : 75/100\n",
      "The corresonding validation accuracy : 76/100\n",
      "\n",
      "\n",
      "Train Epoch: 47 [0/400 (0%)]\tLoss: -1.000000\n",
      "Train Epoch: 47 [250/400 (62%)]\tLoss: -0.600000\n",
      "The corresonding test accuracy : 73/100\n",
      "The corresonding validation accuracy : 74/100\n",
      "\n",
      "\n",
      "Train Epoch: 48 [0/400 (0%)]\tLoss: -0.998434\n",
      "Train Epoch: 48 [250/400 (62%)]\tLoss: -0.800000\n",
      "The corresonding test accuracy : 73/100\n",
      "The corresonding validation accuracy : 79/100\n",
      "\n",
      "\n",
      "Train Epoch: 49 [0/400 (0%)]\tLoss: -0.999904\n",
      "Train Epoch: 49 [250/400 (62%)]\tLoss: -0.999962\n",
      "The corresonding test accuracy : 44/100\n",
      "The corresonding validation accuracy : 62/100\n",
      "\n",
      "\n",
      "Train Epoch: 50 [0/400 (0%)]\tLoss: -0.600000\n",
      "Train Epoch: 50 [250/400 (62%)]\tLoss: -0.400262\n",
      "The corresonding test accuracy : 42/100\n",
      "The corresonding validation accuracy : 61/100\n",
      "\n",
      "\n",
      "Finished training network.\n",
      "Corresponding test accuracy of 80/100 obtained in epoch 33\n"
     ]
    }
   ],
   "source": [
    "params_net = {'lr':0.01, 'momentum':0.1}\n",
    "fit(Net_new().to(device), params_net, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No cambia realmente, ahora con el learning rate schudele..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# params_net = {'lr':0.01, 'momentum':0.1}\n",
    "# fit(Net_new().to(device), params_net, 100, schedule = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La cosa no va muy bien, pero probemos a aumentar el conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcImageRGB(row):\n",
    "    archive = zipfile.ZipFile('source.zip', 'r')\n",
    "    img_data = archive.read(os.path.join('source',row+'.png'))\n",
    "    bytes_io = io.BytesIO(img_data)\n",
    "    return Image.open(bytes_io).convert('L')\n",
    "\n",
    "class CustomDatasetFromImages(Dataset):\n",
    "    def __init__(self, data, transform_me=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path (string): path to csv file\n",
    "            img_path (string): path to the folder where images are\n",
    "            transform: pytorch transforms for transforms and tensor conversion\n",
    "        \"\"\"\n",
    "        self.transform = transform_me\n",
    "        self.data_info = data\n",
    "        self.image_arr = np.asarray(self.data_info.PhotoName)\n",
    "        self.label_arr = np.asarray(self.data_info.NType)\n",
    "        self.data_len = len(self.data_info.index)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        single_image_name = self.image_arr[index]\n",
    "        img_as_img = ProcImageRGB(single_image_name)\n",
    "    \n",
    "        if self.transform is not None:\n",
    "            img_as_tensor = self.transform(img_as_img)\n",
    "\n",
    "        single_image_label = self.label_arr[index]\n",
    "\n",
    "        return (img_as_tensor, single_image_label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "params = {'batch_size': 10,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 1, \n",
    "          'pin_memory': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Helper function to show a batch\n",
    "def show_landmarks_batch(sample_batched):\n",
    "    \"\"\"Show image with landmarks for a batch of samples.\"\"\"\n",
    "    images_batch, landmarks_batch = \\\n",
    "            sample_batched[0], sample_batched[1]\n",
    "    batch_size = len(images_batch)\n",
    "    im_size = images_batch.size(2)\n",
    "\n",
    "    grid = utils.make_grid(images_batch)\n",
    "    plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "\n",
    "    plt.title('Batch from dataloader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_me1 = transforms.Compose([\n",
    "    transforms.Resize(255),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "transform_me2 = transforms.Compose([\n",
    "    transforms.Resize(255),\n",
    "    transforms.RandomAffine(0, translate=(20/255, 20/255)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "transform_me3 = transforms.Compose([\n",
    "    transforms.Resize(255),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = (CustomDatasetFromImages(data.iloc[:400], transform_me1)\n",
    "                + CustomDatasetFromImages(data.iloc[:400], transform_me2)\n",
    "                + CustomDatasetFromImages(data.iloc[:400], transform_me3))\n",
    "validation_dataset = (CustomDatasetFromImages(data.iloc[400:500], transform_me1))\n",
    "#                  + CustomDatasetFromImages(data.iloc[400:500], transform_me2)\n",
    "#                  + CustomDatasetFromImages(data.iloc[400:500], transform_me3))\n",
    "test_dataset = (CustomDatasetFromImages(data.iloc[500:], transform_me1))\n",
    "#                  + CustomDatasetFromImages(data.iloc[500:], transform_me2)\n",
    "#                  + CustomDatasetFromImages(data.iloc[500:], transform_me3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_dataset, **params)\n",
    "validation_dl = DataLoader(validation_dataset, **params)\n",
    "test_dl = DataLoader(test_dataset, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAACDCAYAAACDStD1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzsvVmsZFl2HbZORNyY53l4EW/OrMzKrC6TtCkCaoFAS2hDAi0Chn5kAW3YMCAYNAxSlk2YptXdpmDBHzYBflgGbIOmacqmbEGwJEhiG222iGabA8wqVeXwXr58U8zzjTniRty4/oi3dt5IdjWzpG53ViEOkMjMF/FiuPecfdZea+19lGVZ2I3d2I3d2I3P73D8sD/AbuzGbuzGbvxgxy7Q78Zu7MZufM7HLtDvxm7sxm58zscu0O/GbuzGbnzOxy7Q78Zu7MZufM7HLtDvxm7sxm58zscu0O/GD3Uopa6VUn/2Uzz/l5RSHaVU4wf5ud50KKUspdTJD+i1P9W1+RSv+1Wl1K9/v193N97esQv0u/HHxl2AmSmlxkqpvlLqHymlim/4uwd3wc/1A/hcRQB/DcBDy7Ky3+/X/0GOH+R12Y3d+JPGLtDvxieNn7IsKwggB6AJ4Fd+yJ8HAPYBdC3Lan23B3dB9Ac71GbsYsZncOxu2m58z2FZ1hzA/w7gIX+mlPoLSqk/UkoNlVJlpdRXbb/yz+7+1u8ygp+4+51/Tyn1TCk1Uko9VUr9iO133ldK/XOl1EAp9b8ppbyvf447CuMbAPJ3r/urNpT87yqlbgF88+65/4ZS6olSSldK/bZS6oHtda6VUn/97v0mSqn/QSmVUUr947vP9n8ppWKfdD3ufreulKoppf6d1x77VNdFKXWslPqmUqp7R0f9L0qp6Ce8r0cp9ct371u7+7fn7rGYUuofKqXadxnYP1RK7dl+91Ap9a277/cNAMnXXvtPKaV+9+56faiU+knbY7+tlPqbSqlvA5gCOPqka7Mbb/GwLGv3Z/dn6w+AawB/9u7ffgD/E4Bfsz3+kwAeYwMU3sMG8f/03WMHACwALtvz/xKAKoB/FYACcAJg3/Zevw8gDyAO4BmAv/oJn+snAVRs/+d7/RqAAAAfgHsAJgD+HAANwH8M4AKA2/Z+/w+ADIACgBaA/xfAvwLAg81m8Tc+4f3/9bvv+uju/X7j7v1P/gWvy8nd5/QASGGzGfzyJ9yHr9997vTdc38XwH9x91gCwL95d69CAP4ugL9ve53vAPiv797nzwAYAfj1u8cKALoA/vzd5/5zd/9P3T3+2wBuAbwLwAVA+2HPz92ff4E1/cP+ALs/b9+fuwAzBqADWAGoAXj8PZ7/ywD+m7t/f7eA9k8B/Iff473+iu3//xWAv/0Jz/2kQH9k+9kvAvhN2/8d2GwyP2l7v3/L9vj/AeC/tf3/P7AHydfe/38E8Lds/79nD/Sf9rp8l+f/NIA/eu3aMNC/BPDnbY99GcD1J7zO+wD6d/8u3d3DgO3x37AF+v8EwP/82u//UwBfufv3bwP4+g97Tu7+/Mv92VE3u/FJ46cty4pigwJ/BsC3lFJZAFBK/bhS6v++owoGAP4qXqMDXhtFbALVJw27g2YKIPgpP2vZ9u88gBv+x7Ks9d3jBdtzmrZ/z77L/z/p/fOvvdeN/cFPe12UUmml1P+qlKoqpYYAfv17PH/re939O3/3On6l1H+nlLq5e51/BiCqlHLePadvWdbkEz73PoC/dEfb6EopHcCfxkab4bB/5934DI5doN+N7zksyzIty/p7AExsAgCwQYT/J4CiZVkRAH8bG0oG2KDW10cZwPEP8mPa/l3DJngB2AiI2Gw01e/D+9TvXouj9Nrjn/a6/Jd3P3/PsqwwgL9ie/7rY+t73b137e7ffw3AfQA/fvc6f+bu5+ruM8eUUoFP+NxlbBB91PYnYFnW37I9Z9fi9jM+doF+N77nuHNa/EUAMWz4c2DDA/csy5orpf41AH/Z9ittAGtsi3b/PYD/SCn1o3evd6KUsget7+f4TQB/QSn1JaWUhk0QXGDDaX8/XvvfVko9VEr5AfyN1x7/tNclhDuKTClVAPDXv8d7/x0A/5lSKqWUSgL4z7HJAPg6s7vXids/l2VZNwD+EMDXlFJupdSfBvBTttf9dQA/pZT6slLKqZTyKqV+0i7m7sZnf+wC/W580vgHSqkxgCGAv4kNZ/vk7rF/H8DXlVIjbALOb/KXLMua3j3/23dUwJ+yLOvv3v3sN7ARAv8+NsLr931YlnWGDTL+FQAdbILaT1mWZXwfXvsfY8O7fxMbgfebrz3lU10XAF8D8CMABgD+EYC/9z3e/pewCdj/HMBH2AjIv3T32C9jI0R3sBFs/8lrv/uXAfw4gB42m8Cv2T5XGcBfBPCfYrMZlbHZcHax4XM0lGXtsrLd2I3d2I3P89jt2ruxG7uxG5/zsQv0u7Ebu7Ebn/OxC/S7sRu7sRuf87EL9LuxG7uxG5/zsQv0u7Ebu7Ebn/PxVnT7U0pZ2WwW2WwWDsdm7wkEAhiPx1gsFojFYhiPx/B6vdA0DaZpYr1ew+l0YjweIxjcFDI6HA44HA54vV4sl0t5rdFoBI/HI787n8/h9XqhlIJSCuv1GqZpwu12YzqdYjqdwuPxAAA8Hg9msxm8Xi++/e1v/3Au0Kcc4XAYoVAIkUgET58+BQA8evQIs9kMlmVhOBzC5XIhHA6j1+shHo9jNpuhXC7D7/djuVwiEonA4/Gg3W4jFAphNBpBKQVN07BYLHB8fIxut4t2u41sNotIJIJer4fBYIBYLAaHw4FerwcAWK/X2N/fx3w+x3A4xHQ6RTQaRSAQgMvlwnq9BgC0220YhoGTkxPc3NwglUqh1WrBNE0opVAoFKDrOizLwnK5hNPpRCaTwWKxQLVaRTQaRTKZxHA4xGKxQDgcRjabhaZpuL29xXq9xnQ6RTAYlPdyOBzY39+H0+lEpVJBMpnEYDDAYrGAx+NBt9v9vtyTXC4Hp9MJh8MBwzAwmUyQTCYxGo0wmUwQj8dRrVbhcDiwXq9xdHSEy8tLxONxeL1eBINB3N7ewuFwIJfLoVKpYLFYyOsfHBygXq/D7/fD7XYjHo/DNE10u12sViv4fD6s12skk0nU63X0+33k83m43W7UajUYhoFSqYTb21scHBxguVyi2WwiEomg2+2iUChgOp1itVohl8uh3W7D4XBgOBzC4XAgFAphuVzC4/FgOp3C6XQCAOLxOBaLBVarlXyez8I4Pj7GbDZDNBqFrutwOByYz+eYzWbw+/0wTRMOhwPhcBgejwcOhwO6rmMymcA0TWiahnw+D4fDgfF4LPHM7/djNpshFAqh3W5jsVhgPB4jHA4jnU5jPB7DMAzMZjM4HA74fD5Uq59c62dZ1icV2G2NtyLQA0Amk4HT6YSmaVitVjKJh8Mh3G43IpGIBIT1eg232w1gE9Qsy4LD4ZDfdTgccLvdmM1mcLvdiEajmE6nME0TAKBpGjweD9brtVzoxWIBv98Pj8eDQCCA9XqN1WoFwzCwWq3Q6XTg8XhwdLSpd3n58iUMw4DH40EoFMJiscB8PkexWESlUoFhGIjH41itVgAAp9OJ6XSKdDqNVqsFr9cLt9uNyWSC5XKJUCiEZDKJdrsNn8+HTqeDRCKByWSCbDYLXdcRiUQwnU6xXq9Rr9eRTqexXC4xHA7x+PFjfPDBBwCAUqmEZ8+ewe/3I5VKYTqdYj6fo9/vQ9M07O/vQ9d1KKXg8Xjg8/mgaRqSyaT8bDqdot1uI5lMYjKZIBAIwOv1IhKJ4OrqCs+fP0cmk5EFXqvVkMvl4PV6MRqNUCqVZGL3ej2YponBYIDRaIRisYjRaAS/3y/fw+12y/WYTqcwDAM+nw+z2QylUglOpxP9fh/RaBSDwQClUgnlchn9fh/D4RDFYhHdbhfNZhPBYBBKKQSDQZydnSGdTiMQCODs7AzHx8fwer0YDAYAgL29PbRaLYRCIbjdbvT7fcxmM/h8Prhcn355vPPOO5hMJiiXyzg+Pkan08FgMJB7CkCusWVZCAQCSCQSODs7QyAQgGVZshkxEOu6jlarBafTiVgshtvbW0Qika1AP51O4Xa7MRqNsFqtMJvNEAgE0O12USwWsV6v4fP58PTpU1lrqVQKlUoFHo8He3t7sCwL+/v7mEwmApqUUgKc4vE4yuUyms0mnE4n5vM5EokEdF1Hp9PB3t4elFJYLpdQSqHX62E2m8HpdKJQKKDdbn/q6/nDGlxn1WoVSimsViu43W4sl0sBlpqmSUxqtVpIJBJwOp3w+/2wLAubomxgPB4jEolA0zQMh0NMJhOZby6XC4ZhIBQKCVgJBoNwuVxYLBZYr9d48OABXrx4gUKhgNVqhV6vh1gsJpvpm4y3wkevlLK+8IUvyBewLAuxWAzL5RKr1QqTyQSxWAymacI0TbmYHo8HhmFguVxC13UkEgm43W5p5GNH7PP5HG63G+v1WlDV9fU1jo+PsVwusV6vEQqFYBgGDMPAer3GcDiEx+OB1+vFer1GuVyWCd9oNBCPxzEcDuVmB4PBrZ1/OBxKAB0MBphOp8jlcmi1WjJ5AODBgweyeVWrVQwGA+RyOUHARFqhUAiDwUAmndfrxXw+h9/vh8vlkkASi8XgcrmwWq3g8Xig6zpcLhcSiYR8j/39fSyXSywWC7jdboRCIbkf7XZbkCQRWDgchsvlQqu1aQXPoKSUQr/fl8/v9XoxnU6haZqgbq/XC6fTidFohFwuh06nA8uysFqtEIlEMBqNEIlE0O/3sbe3h06ng36/j2KxiOFwiNFoBMuykE6n0Ww2sbe3B13XkUqlcH19jVAoJAjW7XZLlscskHOcC6/b7cpi4mbPDf78/BzZbBb1eh0+n082hO82Tk5O0O12EY/H8fLlS8lszs7OcHJyAofDgevrawEEi8UCJycn6Pf7mM/nSKfTkvUMBgOkUin5nufn5/I9R6MRUqkUvF4vqtUqjo6O0Ov1tgJnIBBAOp2Gy+XC9fU1Dg8PJUsDNkEpGAxivV7DsiwcHBxgOp3i4uJC1lMqlUK5XMbBwQE6nQ6i0SjK5TJCoRAsyxIkHwqFUC6XoWkajo6OcH5+jng8jn6/L5tzNBrFbDbDYrHA0dERbm9vUSwWcXZ29i8fMP5/GPv7+3C73XA4HJhOpwAgG95kMkEkEoFlWXC73Wg2m3C5XDBNE8FgEMFgEKvVCsPhUDJrXdcRCoUwn88BbOaifUO1LEvWKucwMz6n0wmfz4flcolut4tEIoFOpwOlFHRd/2wherfbjWAwiFAoJAHa6XTC5XJJgF6tVtA0DU6nUygYTdMAbFLjxWIBl8uF8XgMn88HAELxeDwe+X2i/sPDQ7hcLliWJcHa4XDA6XTC7XZjPp9juVxKsOh2u3A4HFgul5jP54LGiYAdDgfS6TQWiwXK5TKKxSLa7bagLaUUut0unE4notGobD4vX75EOBzGarWCruuStqdSKYTDYVxdXQmaHg6HiMfj6PV6CIVCQm9MJq96VsXjm6LTRqMBt9uN1WoFv98vmUs6nYbf70e/30cqlUKv15MNo1arSbCdz+cwTROr1QrNZhNKKSQSCfR6Pcku+v0+4vG4vH4gEBBECUAmNidno9GQAO7z+XB5eSm0j6ZpOD8/R7FYlMC/XC4FrY/HY2QyGZTLZeTzeZTLZfh8PgyHQwBAMpnEy5cvEY1GMRwOBVWuVivJPCaTCQzDQCKR2KJ4uMBisRjW67Wk2K+Pg4MDuZ7dbhfD4RDJZBInJyeYTqeSQXS7Xei6jnA4DL/fD13XAWxoRJfLJfeeFEAymZS5dnV1hUAggOVyCZ/Ph8lkgn6/L3Rjq9XCdDpFOByW7z6ZTIQ6Ozk5EaSeSCRgWRbG4zH6/T4A4OjoCE+fPsVqtRIEuVgsoJSCz+eD0+kUOpPrgdnYYrGQ7Nnn8+HJkyeS5ZJSNQxDAJDf7xdQdH5+DpfLBafTiWw2i/F4LOtlMBjIOorFYjLPuelzUzFNE81mU2iRm5sbuN1u5PN5aJqGm5sboXi5SSYSCcRiMbTbbXi9XiwWCwFcXq9XAA3vEbChbJmd1Ot1hMNheL1eOBwOBAIByZhIwREQ+Hw+9Pt9FAoFhMNhzGYzdDodBINBGIYh1xiAUJD9fl/mHWnVTCYDwzDgdDrR6XSQz+flPjDr29t78y4Vb40Yy4nb6XRkksxmM6zXa7hcLmiaJouV6BLYXFjDMAQ1KqWE+2VAN01TKArysl6vFx6PBx6PB263Gz6fD6vVSugaTdMQjUYFdZPfj8VimEwmWxnAeDyW1zg5OUGz2YRpmiiXy0in0wA2N9XpdOL4+Bh+v182kUQiIdoEaZperycbVigUQjabxWQyQavVQjqdlrRvPp9DKYXpdIrlcinXcjKZYDgcCqpl1mIPKERdg8EApmkKN3///n3M53M4nU5Z1KVSCYVCAaFQCI1GA/l8XlB8MpnEfD7Hixcv4PP58PLlS0wmE4xGIxweHkrQ5eYbjUbx/PlzTCYTPH36FOv1GsFgUBZ4JBJBp9NBr9eDYRhwuVwYjUbCUxPRJhIJaJoGTdNQKm16dLVaLZycnCCZTMrGms/nsVqtEAgEEIlEEAwGBc32+30JHEyF0+k0otEovF4v7t27B2CTIZ2cnMDr9QoVcXNzIxtbp9PBxcWFbLbtdhsul0tQdjL5qiFlu93Ger0WSo3ZB6ktr9crlN98Pke9XgewoTbj8fiWpmTfiDRNQywWw97eHi4uLmCaJgzDkMw4Ho/j3XffxcnJCWazGVarFWKxGKLRKEajkQRxl8uFyWSCyWQiACmRSEhAIue+Xq8xmUwQjUbh8XhQrVbl57lcDplMRrI8h8MBl8uFaDSKo6MjAQl+vx/VahWTyQTj8RjtdhvBYBCapsHhcKDT6SAQCCCbzUIphdlshouLC8kUDMOQ+3V7e4tyuSzvw+9YKpVk/nF9JxIJXF5ewufziWbl9/u34pGu6zAMA41GA4FAAA6HA7PZDL1eD8PhEIlEAqZpymPJZBLxeFzonclkgsViAZ/PJ9Tker2WjNc0TSSTSdGZxuOxgKlMJiOfn+uY9zgQCCAQCMhaeNPh/OpXv/rGT/5Bja997WtfPTo6wmg0QigUkhu9XC6xXC7hcrkkxfH7/RL8iVbJ15OzYtB2uVzw+XyCnIhWNE0TYZEbATcWpZSITNzBORGIEskxcxNIJpMSMJ4+fSpIEYBMMIfDgdFoBF3XEYvFoGmaiL/ktYPBIDqdDnw+H+bzOcLhMBaLhWxi1Bw40X0+n9Aa5NIBIBqNysa0Wq0kyMfjcXg8HhHaCoWCbKaxWAyz2UwEveVyiXQ6DY/Hg3q9Lp+ZlA0pKqWUBJThcIhcLgfLshCNRiVLmkwmcs2r1SoSiQTG4zH29/fh8XjQbDaF09d1XTK7aDSKXq8nQuz19TWOjo7gdDoljc3n8wA2CCwajcp9oLZDSkopJZSUw+HAarVCMBiEZVmYTCayYWqaJvSUruuiu+i6LmJkNBqVVNowDBweHmI0GkkmFIvFRBBvNpsibi8WCxE3V6uVID7SX263G8PhELquo1QqYbFYYDqdCnIkhcNsI5VKYTQaAQAikcjWa6bTafh8PlxfXwuI6Ha76HQ68Pv9iEajIvz2+30Eg0HJZLvdLrxer2RdFFcZyIGNDtTr9ZBOp7Fer4UGLBaLsCxLBMVIJALTNNHv90XHsiwLo9EI4/EYuVxO1pxlWfB6vajVaqKrhEIheL1euFwuDIdDHB8fYzAYIJvNwrIs9Ho9+P1+oeUYAOPxuACFXC6HZrOJg4MDiRUUWQeDgWhjXM/ARrshd86Nn5kFn0+6kGuYP2O2Ho/HcXV1BcMw5FoRkJEyZBanlEIkEkEgEEC1WhXzA0HnYrEQurLZbGIymWBvbw8/+7M/+7U3ibFvDaI3TRPhcBi12qbzKkVUOhXsgZgpFYMlhdXXUX0gEBD+lQHS7/djtVoJKqLApZQSBZ3vrZSSALxcLkWsJIWglJI0fDgcSvpFofXx48eCLIPBIDKZjCC1SCSC5XIpwZ8InpQCERuFV4q+uq6j3++Lo8jn8yEYDG6hu36/L9oFEZHb7d6iMRKJBJ48eYJwOIxSqYSzszPU63Ukk0lxUfR6PclU+Pksy0K/38d0OkUqlZLsZjweIxaLifDIAMsJaZomIpGI0DbL5RLlchlOp1N4etJaRNrdbhe5XE448C984QtCtTUaDRwdHeHrX/86SqWSBPhqtYqHDx+K8DUYDIRnJSokpeDz+RAOh3F4eIhsNovDw0OEw2Fx7/AaU8NJJpOwLAsXFxciPrtcLjQaDViWhZcvX8Lj8SASiUh2UygUMBqN0Gg0JMWPRCK4d+8eHA6HBM9Hjx4BgKDo9XotFBzRYCqVQrvdxmw2w3w+F+6Yv0fNwZ7JJpNJHBwcCF0CbIIcnT+GYYj2Va/XMZ1OxRWyv78vlASzIjpKSCuNRiNZs4lEQgAU9aL5fI5oNIrlcolUKoVEIgGv14t3330X+/v7iEQiso4KhQJisc0pjqR8VqsVWq0WNE0TqpauoJubG4xGI8k0vF6vaCvMTghISE9xrVFvozify9nb70O+Z6/XE4OA3+8XCoUZE2ONZVkoFouYTCYYDAayXpRSspkR9FG3IqvANUUQms/ncf/+fYl1sVhMAFC325X1Rdr6TcZbI8b+xE/8hHxwctoM2vybIis5c04QBufJZCLcGh+zT3xy8XQMMACQ5pnP53C5XKJ4K6UkRfR4PBgMBoLQuIuTI3U6nSKkkd7o9XpCGYVCIbRaLUSjUUwmExEjPR4P7t27B13XcXZ2BofDgYODA9ze3kqQGI/HcDgcCAaDklIzsCYSCTx79gxutxuGsWnQeHBwgEAgIM4VUjEOhwPValXQZTKZxHg8RrfbRalUQrPZFCdAOBzeuj6dTgfxeFwCPPl9XdclM2IQ1TQNlmWhUqnANM2t1DUWi6FarUrg5CK+mwcAAMMwhFIg9cJBIZVukHA4DLfbjXq9Ltxxo9GAz+eD1+sV61qj0dhaGNlsFk6nE9VqFfl8HvP5HI1GQwTHvb09LJdLfPzxxwAgTiVmK7VaDX6/X7SJcDgMp9OJcrmMk5MTXFxcwOv1ytxLJBKo1+t4/PgxAODs7AyapgkltVqtYFkWarUa4vG4bJR0YzA7o4BLwZiInnx9JpOR+WinWwhu1uu12GqPj4/x7NkzOJ1OEWpLpRKm0ymazaZkdl6vF71eTzLi4XAIy7KEt242m4JyqU8UCgWhXrrdLgzDQCaTgWma6HQ6eP/990XrarVaKBQK8hhdQpVKRQRQp9O5BR5isRgMw8BgMEA4HEa5XMbR0ZHYRePxuATbeDyOVqslm9t6vcbe3h7G47FscswsORKJhLhoJpMJZrMZYrGYfCcGaFogeZ0qlYpkXaZpSiwhI8C45vF40O/3xexBUDWdTgXgXF5eipOJll/STRTi39Re+dYg+vF4jMFggF6vt8U9UUClCMILQwulz+cTnzU9xEz1iEQZ/Mnrm6Yp6dJ6vZaNgenrfD6XSW1ZlghIT58+FV6QThePx4N0Oo18Pi/INJfLiX1wPp+j0+lA13UsFgssFguxm52enmKxWOD6+hrz+Vx4cOoI9JAz1SU6pQjcarVE3PR6X52nzQ1ttVpJIL+9vUW328XJyckWco9EIigUCnC5XILiaeHie4VCIQm2XIB0CPh8Pui6jmaziVQqhXq9LhRaoVBAMpkUpE+qhGIotQ3SKqQj0um0pM30JVcqFbTbbfG804UymUzw+7//+7J47JTcer1GJBJBvV4XJ8PBwQH29vYEDVL3iUQick0YdCgkU8AngqYllB7pVCqFWq2G6XQKn8+HZrOJQCAguhJ/J5vN4vr6Wq5bMpkUgb/VamG5XAqinUwmcLlc4nFn5rVarRCPxyX74SDHTHqEnDevM+/ffD5Hr9eTjWtvb080i0AggMFggBcvXoidmeiTG9p4PAYAcZ2Q7mGtCjNhctmkETVNkzly//59Qev05U+nU9RqNSwWC0wmE5yfn0MphXg8jslkgkwmIxt1JpMRJ5XD4dgyFTBbI3VkmuYWBVwqleS7ud1uLBYLzGYzEao5QqGQ6EPT6RSDwUBs0IZhCBfv8/lkzpmmicPDQxFbSS/TmpxOp7fiGWlUbqK87/F4HO12G5lMRuydwWAQ7777rtB2tES/6XhrOPp8Pi/oN5vNbqFr7ugM1kT8DMakSij88G8GaSI9TdNE2OUktaN9Ikpa4RaLhbzGer0Wbmw2myEcDovAyIIiv98Pr9eLVquF+XyOd955B61WC8FgUIoqiGyJDim6cEIS7fNzdjodRCIR8ebafflELKR6yJ8y3aYYy0nFwi962+k0CQQC4hsOBoPY29sTV0k4HAYA8fKTvmLwq1QqyOVy8Pv9oqPQ+UA0VavV4HK50O12xTWl67r47huNhmw0pNz4+elbjsfjKBQKon2Qa+XGT4Gen61WqyGfz8MwDHEP+Xw+tNtt8d3zs9C/bEeIRL7VahXpdFo2Ur/fL/QLg+pwOBQKgnzzaDSSBe50OpFIJCRox2IxdLtdmKaJVColxVzUlebzOQzDgN/vF51mMBgIECJVMBgMRMhLJBJSCOb3+2XO0Q5JDSKZTCIcDuPi4kKyHsMwYFkWOp0OcrmciO7L5RLRaFTACYvhYrEYarUaSqUS6vW6bM6LxUIyXr/fD03T0G63MZ/PcXx8LBoEzQPVahW5XA6GYeDm5gbvvvuuzGMiZrrBqLNVKhWZHzRwBAIBQcOcP6TuNE1DtVrF3t4eVqsVRqMRwuEw+v0+AoGAzO/5fC7rB9hsJrx2zDA1TYPf7xcKi2yDYRhb1mpSpsywCTwnk4lkSVzDjE2j0Qg+n082VBa5sY7Cnl0sl0tkMhl4vV783M/93Btx9G8NdfPee+9JWmK3UJKmYRrMCj/SNev1GoFAQII0LxyRMFH56wsJgPCTrIQlnUMah+kukdN3vvMd2RDIOZMvzWQyaDabSKfT4loANo6NQCAgqXKz2cR4PMa7776L+XyOi4sLnJ6eSqpMdMHsYr1SaiFlAAAgAElEQVRei6dZ13Vx5XAyR6NRNBoNhMNhVCoVAJsAWSgUJDXm9WKATSaTYuOs1WoiMFIA5kSlCOdwONBoNHB8fCyOAdIAwCtHERf4dDpFr9dDMpkUB5G98pM0Ae83fdyLxWILjVP8pujV6/VE56BNjYsegFj7mFKz0vf29halUkl0HS74SCSCUCi0ZdFjhscg9eTJ5qyVfD4vqM4wDBSLRfj9fim2mk6nwkuTTopEIiiXy1IUVa1WRUsJh8PiWul2u0LpORwOZDIZqbRk6s8iPupKpGUIShKJhAQWFpn1+31ks1k8e/YM4XBYPg/dO9FoVNweBEekSEjBZTIZsXNalgWfz4darSZFhxRCw+EwRqORfEZgYyWl9bZer8s6JDXDucEgqWkaBoMB4vG4CMl83Ww2i06ng2w2i+l0iuFwiHA4jPl8LplfIpGQz8eqZoIdblK857PZDLVaDUopsXqy5gAA0uk04vE46vW6ZBBkD4AN0BgMBjJ3Oa/G47GI9dwE3G63CKqMM5z3LFzk3F2tVuj3+6LH0cqZzWbh9Xrx8ccfw+PxiBOvWq1+tqgbr9crBQcUkrgrktfixWHQ4o7KwgVOPqI8ohqfzwefz7eV2jPlpJjJoLZcLgVR8w/1ATobisWiaAJ0ZZDno2gLQKgmulW63S6WyyXy+Tyur6/hdDrx4MEDABvnDgUWTpxwOCzWKrs4ROeRruu4vLyEaZpbHuBAICCVlKQmGEDpP8/lcqhWqzAMA9lsVjIPu4fc4/HANE20Wi1BXtxAeV3r9bpsoL1eTwIfH3O73ULpkG4gugQgjiDymbquo1arYTKZIJVKCfrl/+0WM4rnFLVoceMmOZvNhA6qVCqwLEscPeTa2QKhVCohFAqJ751uDWAT5OnMMQxD2jmQ07Xf59PTU/E+39zcCEK7vb1FKpUSsTaRSCCXy0l7CGCT6p+engriozA/Go1gGAZGo5FkgHaHGbDRLubzObLZLE5OTnB5eQnLsmCapmziLpdLqC8Ke+FwGLFYDJlMBoVCAel0WgIUAKnkZMY6mUzg8/m2ah0Gg4Hw8DRKdLtdWZ/lclk0BjtFQc3NMAxUq1XJXinMJxIJDAYDmesUs4fDIQqFAqLRqLQSYFCezWZYLpdCfdBYcHJyIki+VquhVqvB4XAgGo2KxdM+mO0XCgVB6iykJGrn9aB9lCCG8Ylrib5+e7sPtlghSGBRFGtEuEnaK2CfPHkimeSLFy9En3mT8dYEejtvTmRBVMHHGajtFa6BQOC7Ujj0moZCIRHNGJy4cfBmUjnn47ywSil0Oh0pHCH9c3l5Ca/Xi9PTUxGCKBD1+33U6/Wt8nWiCQp25PVnsxlGoxGWyyUGgwEajYZQABS9rq+vAWwWfaFQwPHxsVi3yN0y8HKw2rXf7wuXadc2WMBDvWEymQhCZSDmdSCK9vl8MpEZBCiWkhpgfQLFa1ZqXlxcYLlcSgk3q2TdbjfC4TAGgwHq9bpkZ7ynFL1JnfB+sbiOojHbBfh8PrlOmqbh5OQEiUQCAMQeykBJm6ZSSjYIUmSpVEp86wAEqbOuodFoCDVFBwopxpubG5imKZsJXUeZTEYqR8ntUyhnRknPd6/Xg9frxcuXL+H3+/Hw4UPJoNiDqNPpSJDg/NA0Tbz5h4eH8j5ut1uC0nA4FBPAeDzGixcvkEqlxAFGVHx0dCSiN6kXBjXqF6xw9vl86PV6wjUzQNHqmEgkpNDM7XajUqkgGAwinU5jMBjA7/fjvffew/7+/lamRqBH9xwzV7p9hsOh6DbUeizLQjweRzabRbfbxWw2k3sTj8clptAWyk2P9S4clmWJw40Fg7Q1Mz6RtlJKoVarSdwhQmeQZybDGgG7nsLPMhwOYRiGFEGxiLBer2MwGIhITctoNBpFNBp94/j61lTGkhKhQ4T0AAVU8oTABvlw4RMR2qvW7GkoEQYRI61yRGd8HgM+JwaLq5LJpDx/NpuhUChIFV0oFNqaJM1mE++//z4++OADUc7JIzIbyWQyuL6+lp0/mUxK2kchi1w+sxneXPbbAbAlHjHgsV1BLpcToa1cLguSWa1W2Nvbw97enqTiREuJRAKLxUJQPXWD/f19sTSyIpTODlJC9P6z8IoUGCkJVpyyxQWfR6cJBUsWwwWDQZyfn+Po6EgCPpFeKBQSBEXahgiUApppmuLzZp8YUid2X7a9lQY5fmZlzGaAV3UJdF6RZ6XlkJXQbKPAVhGk6tjyIp1O4/LycivzXK/XyOfzsCwLfr8fkUhECqVIsT1//lw25E6nIxkjS/GpQ9BVRVqTwb9cLgunS182M69cLid00Gw2E16aIjiwyWgowtKmzEIv3mPaA0ejkVBvrHwNh8O4ublBsVjcuhcsYhuPxzIfE4kEWq2WFOWdnJyIVkY6kvUISimxnLJQi5WpRM/xeFwAi9/vRyKRQCAQQLlclrqJcDiMFy9ebMUjTdPEVMHiv36/j3Q6LRZOBvtIJCI1Dexpw3hGVw7nN6k2AGJ2sH+fdrst6J9znKYJZuJ079Hx9ybjrRFj2RuE3mUGa+BVWwLSMRS9SNcQAdqRPBFFMBgU4Y6bBqkZlhgz+HNyMOW1V9GysRmtWlT7iYzs7ggWV7DVQb1ex/7+Pvb39zGdTpHNZuH3+6Ukm7QJqQ97E6pWqwW3243Dw0NxE9jpLPKpnDAc5KHtHQfZG4ZZD+kZ9lihRZJZAF0ydAoQEXMjdrlcqNfrKBQKMqnZZI0aBvlbZk0MHpzAvI8U6lwuF66urnD//n1cXV1J5sbPxPtLNEbRmlkeqTkiK9pFTdMUNw2RVTAYlEpUInkKvyx4+sY3viHBg20qgsGgWBn9fj8uLy8l62ABDoV1akJsKre/v49erydNvkhHUQCu1WpYLpfStI1FNLquIxqNIpPJiBOKts7JZIIf+ZEfQaVSgcvlEsHUMAxMp1MBJzQT2HsD0W/u8XhkTfj9flxfXyOdTou9loiU1AKzL9J00Wh0S2gkzUdESyBA0ZjBkBqT1+tFKpUS6oUedAY9u22alkNgk2Wx6R31Nc5lBlvakWlVpieepgpmz3Yb79HREZrNplgj6cZjLQ/7asViMaxWKzE5cE5S6Ob/yTawcJAZGp9jZzDowyeoAV71KqI5oVarIRqNvrEY+1YheiJXADLhAEgQ5uO8oNwAyLfzgsbjcSmgUEpJtSHFovV6veWf54Qaj8fweDyyU9JuRo5uvV5jMBggFApJm1e7AAds0nwihVKpJH0pPvroI+kjw0B9eHgotkqq+YvFAqPRSDz477//PnRdx4cffgjDMMTGeXNzAwAS5EinABBdIZvNolKpiJDY7Xal4yRbOLAUnBsW0SvvARt7MVja20iw2Im9TWhlJMdIxM0CnNvbW1mIk8kEjx8/loXQbDbFCkgHx/7+vmQWRN9KKeHDuWHTscT3Y/BiCwu/3y/ZEAMPrxc1B2ZRpDmCwSBSqRSATRpNuoh9Y05PT/Hs2TOpA4jFYnj58qVssKQzmPWZpilN8t555x30ej1pMkcayDAMARE3Nzc4ODgQAZc0B23E9XodoVBIsilaNo+OjvDBBx/Ic7gphUIh0XkGgwEuLi5weHgoDQEpUgKbHknUbVhVSqosk8lIhsyNpFKp4ODgAOVyGaenp5LdsOspkf58PsdoNJIWFSyKKxQK8jxSTNxMNO1Vh8jRaCRtqfnYo0ePpGcNM0+fzyccP4uj8vm8GB1CoRCGw6G0gzBNU/r/2NcQ6VwyA8wO2F2XwJTaHS2bXF8A5DsxhlEDBCD6DbUK4FVWS52BFK/b7cbt7a1UNbMC/U3HW8PR80KxIIhtDMgpA9hy47xeTEV0Gw6HkUqlcHh4iMePH+PRo0c4PT2VAGnnk7mzMk1ngOOkI2q1F5lQVKS18cGDB+KAYcrMZkV/9Ed/hMvLS+kXQ1qKXClTS3b8Yyk8Gzxls1nc3t5KupfJZKDr+lYTL/qwiXAAiN3NNE2x39E2yfdg5kKxh61UKTZxEySfyw2Voi4REwMunRzUJtisjQLWzc0NdF0X8ZyItN/vi74xn8/l+5L3bDQacDgcuLy8FNROXzQAyfRoTWMdBa24iUQC4XAYhUJBWjZQgGcZOoviyN+73W4Ui0V86UtfAgBxuDx//lx8zZVKBZFIBHt7e6KX5PN56ddEgW0wGGxVTrfbbbHWlkolPHr0SAIDi/C40VxcXOD4+BhHR0eYTCZwu91otVqoVCrY29sT4EL3zGw2w4cffghN01CpVHB4eAiPx4NKpSIUZ7FYxGAwkN+n2wmAFJWxdz6pCWagwMaGyGIfFtrl83m4XC4cHR3JWQ8Uufm5Op2OCOO8HpPJBIVCAZZlyf3t9/uIRCJSI8H7Sosq6VtmFMwCSY1y5HI5yRJZzESXE3+XVawssrIPbri9Xg+NRkOaxtnBItcl6TAWwNEcYqfy7L21WBVLcZqxh3GPwDOdTgulVy6XoZSSOEJzwJuOtwbRk16h2EeXC2kV9rhmes60kKm7HWHGYjFpSAVA+pswhdY0TW4OuUR6zcl12m16DH7FYlHSaQqJ5E/p1TZNEw8fPpR+KQcHB6jVasJ3a5qGRCIhC4xVcUR4PDjD6XTi+fPnUjzBMmpSH+xpsre3h0qlItcPgHwWpr38Dp1ORyZUp9NBJpORNJqbHwApWuF9oQ0SeOWSYUsK+s9pHaObwuHYNHpiEEkmkyKStlot2XzosmEfHvqtySu3Wi14PB48ePBgy65HBw2rBUnXMfOjSElrLpEV+46kUimZM3YPNdN32i75GovFAg8fPsTTp0+F8gKAXq8n/DZFbTa/oysnEomI40nXdXg8HkGHRLDMdJiN0UV1c3MjlBg5dwr4zDK4WTGD4dzgJkpuPBwO4/r6Wign8vGsXyDQov7EFg3j8VgCOKtsubGPRiOZa+12WxrQsTdQIBBArVbDgwcPUK1Wt7rKejweqaG4f/8+njx5glKpJFk03XCTyQTpdFrWLXUeOt/Y2ZWbKxv+3d7eIpFISCM2thUnECEde3l5KQVkHHSS8V7S9sr+PQRB9l5awCuameYNj8cjFCGdUwQczA7YbnwymUgbbmprZBhOTk5QLpclPtF+/KbjrQn05IEByKTiJAoGg1LlSmXd7ptn8QLHd6sYowDEnZUXjAiPNi47b0zUOxgMRNRitRupIE6CdruNo6MjdLtdXF9fC9JlR0I7T0muDoC8Rz6flxJwcvkMZul0GpVKRYRECrGsOAVedfEEXtFI3W5X2g4Mh0OxXbIgiguZqTl/j8KzvUc9MyYGRaautNQxA+LPWE1LZMiN4sWLF8hms2KHY8UgFwAXMWkTIkDqL3a+nn17fD6f9Adxu924uLgQJA1AwAA3Pfs8AiC0Extb0V3BMnNd1/HOO+9s9ebnY/v7+4LuCQhSqZS8VyqVQqPRgFKbhm6np6dQSkmLaV3XJUtkqfvx8bGk+vbgU6/XcXp6KlWnzJZYkcsCNRa/kb6r1WqiTzAbq9fr4tG39+1nMJ/NZtIor1KpSIM7IlsWCdIG3Wg0pNHfYrGQYjVd13FwcICLiwuppWCbBPLgpK6i0eiWO41AzE7B0V7IQEo7Nh0sNBKs12scHx9vgTilFM7OzhCJRBCPxyXgUsS3D7b//m6tDMbjMTqdDlKplFCLdnegXRvipm7vR0Uhn7GGm4NhGNJoMJFIiP01mUxKew62YCA1+KbjraFumA7Zy86ZknNxkioAILuaHWFSXLKLKhzMCJghMMAyS7BXzTKQEbWSm2XwJkedSCRwenoqwZaFTrQXttttcX4opeTEJQqt5IdZNUcPvt0GqJTCs2fPMJlMhG7Y39+XI/I8Hs9WwAc2Ab5cLkuKSM7U7XYjl8vJpCPtQfGRmQDpMlJkRPqszLS3ciZVxd4z3AhZ8GPvp06XCEvLWZTVaDTQarUQCATw/PlzSfPZb4hIiAuCfLxdkH748CG+9KUv4Ytf/KLUOdjnDAMHq4PZOZCeaKLkfr+PXq+HcrmM8/NzABAbJFsb0Bq6t7cnB2uwMIdV3IPBAIZhSFdKzh0KgMViEePxWBA0g1GpVMLV1RWur69lwwA2GUapVJLe7qwVIEdMtxjnq8/nQywWkzXENtfc/NmymQ4ozkUiTK6F6XSKYrEoXnzaNdlXnxWuLN2nJ5/0xf3790Wc5EZECuX8/FyshXR8sYMpAHHbkOojHccCPHt7Ac5vagnM4ggEuPE9ePBAzh+oVqvIZrNYrVZ/7PQrZpKMP3RjEVWTJaBAyvVEQwMttxSq79+/j/39fQFr9sJAXnc7VUN2gHZe1qI4HA48ePBAsvA3HW9NoCcipROGHB65Yvu5k0TxdmTHXZJI7PLyUl6bVYs8eAN4VX7MC20XNiiWcsIAkEpHIlcepEEBjtRJtVqVcuxsNgufz4dUKiXBnYGGHnYWSZEH5+I/OjrC3t4eHA6HOHnOzs4ESbHPOPuAsCc7ANEMdF0XAVPTNAlq8/l8K4gSTXADY7UhnSv2QjX6j7kRMnVlYOOmSVcKe7bbC5wAyMbscDgQi8UEuf/oj/4oWq2WvE4wGBSagYuJ6Izzhoufg/ws5waDPM8tIOoieucmzA2Mh2yQ/lmv1zKfWMzG57EIjrULnAMMUuzYGQwGpSvme++9J3OE/Vn4WjwukZZEgpbRaITb21u5ptR3aKW1Fzjd3t6KQ8beSiSdTqNQKEjfIc4VZhzsn8TOqc1mUypTdV2X+01EHQwGUSqVkEqlRD8g/cB19uTJE6nwJa+slBJLIdcf5xB5bQCyNhgUCSDoRLL3viKPz7nKe8o+++PxGI1GQ6qITdOUXlNcS68PHmnJjdvtdqNUKklMoPDOAjvGEK5nOsMikQi+/OUv4/Hjx/Jc6gkM6KR/nE6nUFAs5ioWiwCAYrG4dRbwpxFj3xrqptfryUEYdDHwgFyKKXbUAUDcFkyr7H3PLcuSI77sSI1pLTk2AFv8LDcUoljgVWEDK/fIcYfDYeE+ubB5Pi0D2/7+vqRZRCaDwQDRaFT6wFDopZOBpwld3x2Tx+BLrpd8O10NPJWJo9/v4+DgQA6Q5jm29EyzcyWFJCI7ZksUSjVNEzRkp9KIqOjDB16dn0lEaZqmBBRSYtRVWBzFoKVpmvSWJ5fqdrslYNAjTS6aDbXsFB193gAkG2IQt1tlV6uVeOE5v2gtJPdOJ4a9pQAAOYeAFYmLxQKmaUpdBHuixGIxXFxcyBxdLBbIZrNIJpNSZVsul1GpVDAYDCQA5vN5tFot6bOSTqflAGk6q3iyEKtBKeDy4HQe4k2bJ5E3AQYLjlgLQnQai8XE2ksHFrMnt3tz2PhHH30k2hIPv6HgTu2EWhIL/excNJu/+f1+oSgmk4l0zDw6OhLQRdGXAZ60IrMVIndy2wRn3LBpLuAmQ3rINDe98XO5nDSRu7q6+mN0r92eSZcYN3m2O7fX29ipQOCVY42BnJ+dhV3cDABI0RzbQnCDYxsR6hBsGEgAYp/zf9J4awK9nSOkj50d4qj429V1CmRUrxns2TCJPSNIzfAxCl/k8Wh/5Hvx9XmDaJmaz+cyIclLdjodOeybk4wl5MlkEplMBo8fP8bNzY1w7mxURTGIVYbs/sc0n4IM+3Ywa+l0OvB6vdLLhMHTTt04nU5cX1+LbZLH3QGblJRtBThxaenMZrOC+HmYCp1CbBAFQDICohBuzKQb2OUvEokI3QNsuG2e5sQDOIhaGNiBDUpnV04WG00mEzx69EiyEHvfkdVqhcvLS/zu7/4ukskknj59Ks20SEcxQ6PGQHspXUxcgPa+R/Z+RTw/1ePxCCK001YURW9ubuB0OqVKmuiN99UwDLx48QJPnz4V9429MC4Wi0mffp4byn4nrVZLnk+zAI+BHA6Hwr97vV4BGzymjjx0u92W75nP54V6YUBhzx02OKNGNhwOsbe3J553ajHkoRl0CIISiYS0KAA2hgh7vYFlWahWqzg4OBDLNKkZACJSszp5tVoJ3ZRIJJDP50XobTQa0HVdMjXGDBoveL+5aZDWTCaTqNVqUuRkp2/I+/f7fWQyGTnLulwuI5vNCtKvVCrY39/fooNJFa7XawFuv/VbvyUxiK48e80ONRkK0IFAQBxCZB8IhF0ulxQQvul4awqmisWi7Gb80rzxREX2ikWmc7xIAOQmE/WT52WAp1eZ/2cTIb4OizwACE3AXdo0TXz88ceymxqGgUgkguPjY0Gh3JmPj48l/axWq1u6AEu66XCYzWaCNulWoTOE/+fBCbPZDIeHh2i1WuIHXq/XKBQKUmAEbJw+LMIhjz6fz9HtdpHNZiVYsb3rdDoVXtXutbZvMBRiKW5RSGLgZAUwAPEGM2Cy2nWxWEglI1+DmzB5dx5eQb81rxdBAIuuyM3aF1ij0cDl5aWcd0sbJrUfuoP43qQnWIxEMZF99efzuQj4bElLHp5cNqsy7cfskaKjo4edN1kUR5spj5xke18aAIjwWbVbLpfljFIGo1KphJcvXyIUColPnTZgUm5sU2y3ydo3aBZf0Z5Jao3UpM/nk6I1Tdu0GSbgId1nr9CMRCJbhXsOx6YZXiaTkaDEtg3crNhDiZmVZVm4vb1FMplEMBgUmlLTNKFsisUivvKVr+Dhw4e4urqSzY7FfqSeeI9ptOj1eiL0ct6w3TWDOgerv6PRKJrNpsSCbDaLcrksXVxjsZhU7XMDJF1IyoyurH6/L0VbvE52Q4NdLLdveqy0Zi2EZW0OKimVSviZn/mZz9YJUw6HQ+gA2uCYggEQX+p0OpUUnjQLL6y9FzeDOflFXmAiPADC05JjZVpG8ZceXVbqkc5hSj+ZTPB7v/d76Ha70leD9EO/38dHH30kiJYiGgM3P6u9IMjj8WyVuzNNOzg4kEItLnS/3y88sq7r4gjhtWJQtGsQR0dH8v/Ly0vpgGcYBm5vb+WAcyJMZjUU5ZgF2YuNWGG7XC63giZ/DkCOU1utVri+vpaUmtbNTCYjXuz1eo3RaIT79++jXC5LFae9NS2dD1zQvAatVgu1Wk36oLBLIAEAgz4zGFI0hUJBepkwg6Ftkvc7k8mg1+uJvZAe9ePjY6nRoAea2WYul8P+/j5yuRxWq5X4sG9vb3F2dga3241CoSA1I0zPycMyezk5OZEOqCcnJ3L/U6mUaDP2E6E45zh3iax574jCacslRRKNRsWCW6vVUKlUxLLqdDrl+zQaDWlHfH19Da/XK758esW5kWWzWTmW065bsN2BPRO4vLyE0+nEwcGBZOJ0b3FuULM7OzvDs2fPoOu6aCuMAbzPPIiIXD5PgKPQrJTC06dPRZOxD/akIbgcjUaYTCa4vLyU+dRsNkVr4nse3J3mxaIytpiu1WpoNpvSAZXrlHqLXSNzOBxSdKlpmliWSffQ1fSZPDOW9i2mlZwMtCoRWRMFstcE7UlcGBTpKAyyrSx3e8MwZNHbUTxfl4MIzl40RS80kaFSrxpisfR8Pp9L+9ODgwPh8BkAqtWqBDkigNcLOliowRtKt4fdL22nUej1pXf/0aNHcmoR0QYnIm1pLHMPh8NbFX92aoMOCzazos0ReGULtV9nisYOx6Y9BEv8uVEy9aaAxXSbQhn7iHCjIzpmlbS9XJzIkwifQZ3/pmPJMIytoh1aGEmn2PuZEymt12vx6tdqNank5PmnDJL0gvP4RGoCvAYMVEyzW62WWPbG4zFOTk5gWZZQENSp2OiNFBYDHedhKpXC2dmZ9FZvt9vSSZUVnXQ3ARBLHu8vj7VsNBoIBoPo9/tIJBK4uLhAOp0WjYhuLE3TcH19LQYJzlUA8l1ZbcrNn719LGtzGhIDFz34bAjG+8kMhhkgKVS7TZFjtVqhWq3i/Pwcuq5LVkUNyW5jtCxLzlXm/Kf2xb7u7Kdk96WzfxW/597eHrrdLkKhELLZrIjRnHN2Zxo/L4OzaZqSKTJom6Ypx1sSXPBe8bUJuqjPse5B13XRIn7hF37hs9UCgTfbLpwArwIZaQR2tCPqZCrqcLw60oscGcUyih9Ez+TIGAjswivTJgY+++vV63WxFXq9Xnzxi1/EN7/5zS2LGwM6m3gtFpsTbPgaPPSY6IcTiQHI7o11OBzS6pYNnoi+7eKRx+ORI+8AyCbICkAiSbfbLe1ks9ksjo6O0Gg0xEfNQhDqFQymDLD2zQV4JWLbXSi0jlH0m8/nwjWyKpeZFS2qFKen0ykSiYQsdgqmdBeNx2PpgPk6xUYnB4VjcrOkbJiNtFot6RrKit5arSb0AovhXC6X0HQUheksYq9yBhH7HKQ9lhsk5wURvc/nk/u3Xq9xfX0tPDTnAW1/5IMByMbHnjksPjs7O8Pp6Sk6nY7w8eyBD0AOCdd1XaqwudEBr84SoK8dgGxypmmi3W5v8eZutxvHx8db7jLy6fzOPNiFAIwuLiJqVtuyHkXTNCm8I0XIA17s1BOpsNVqtWXBpK5E+oP6GX+HNl6KmexdxADM9WofzPI5l3gecCKRkLoCALKpsRiLIInZCwElNx07DW13DjIOsdsu5x8dWXSnMRv4tBz9WxXo7QeC8JAN9n1g8RD/TecMgK3Ujn/sPTKcTqc0GePzeaGBV35d3ng7N0aPLIOZ3Sf+h3/4h+I5Zyc/ov5utytVinSVsPiDN9S++3O3HgwGws3HYjFJGblR0ItMoRPYLNZCoSCp3OPHj3FxcSEcOQA5uMDj8eD29lY2VWZO5KfZAG06nYqgyI3JLpQTuXFysyV0v9+XI/kajYYUd/D+0OEyHo+RzWalz002m5XydwphFEk9Ho8cc0hKiwuMmyz966vV6o+dA8z0ms4YOqd4EDY3BOoIzAK4mLkBUiy1N1Cjz5rUH6t+qZswyPOAl8VigUKhIF0/79+/j8vLS+lkys9jWRYeP36Mfr8vB8qEw+GtzXCxWODevXsol8tYrTadSS3LwsHBgbQPuG1l3YwAACAASURBVL6+liDGecvrl0qltux7/E5EmUS9rOgOhUJS3MW+PBRv7euRwjo3RRb9XF5e4t133xUahzSfXcSkHsJMivGABXXz+VwqtVnYR4cKuWy7CEu07fV6hYbi/aYmeH19vUV9Aq+ONnU6nXIkJjMZtrKgKM/NyV6FTv6daJ31PqPRSPQNahnMAg3DkLYYL1++lPbne3t7UiRHcNvr9bYs1X9ifH1bqJtUKiW7nVKb028ogNitlZZlyYRnsRF5ZDoR7JQCLzqALWumfcMgqubvUtgiCiZSKJfLWwIOkQQ7BQKQowXp3OENjsfjqFQqIl6ZpimppsOxOTOU5fvs+UHuns+hQGv3JLP4hcIuAHzrW9/CBx98IL3JmS7yb4/Hg1arheFwKDQCkR+PoiNds1qtxO3BjY/C6+3trWRDRMGv00R0DSwWC1xcXMDv90uFZC6Xw+3tLYrFojiPqJNQXNM0Tdol8ABlOxdOVxEpO7vLisidG+KLFy+QTqeFF33x4gVKpdJWmszgFgwGxRdOFwQtnkRSdjsskSGpLV4LBh02IePmzxPFNE0TIZrPV0rh8PAQT58+lU2WwunR0ZGcP2xZlnw3nnZlGIacrERunKdL8XASbsQ0CpC+YZ8lHjdomibi8TgajQY6nQ663S4ePXqEcrksFlV2meR14kadzWZFVCfCpuuJbRCYAbMpXigUkoZ0dCnRLGHvSsmMnu2S2YyQn4m9iAigqLcxU+FJVaRV2GjQfviI/chMbkQApGVDKpWSecn5zgyQtKEdfNpjm73A6nUa+PDwEFdXV9IA0OFwiDjd7XalODKTyeD29ha/+Iu/+NkSY4lY2fmPBQcUBe0FE7VaTQIXgyGFWaa35LMpdHDh25tXcWOgQs7B4gv6WWl3czo3J0IxO6BTgfwwOUwKs3RBkELipAAgC93n8+Hy8nJLkOGhFUz7aRejFZQuA/av58/sw76JAJAWzzyGbbXanJ5DkU0phcvLS2mxGw6H5YQoLnyHwyFBlYdj0D+tlJI+3TzOjUGZn+Pg4EDoKr/fj+fPn2M0GuHFixfS94WpvmmauH//vlQL0ytud74AkPR2OByKCM/7wM2DKIh6DO2op6en0l6Cc+3jjz+WboF2yypPekomk0J/8HhHj8cjmwA3Np5CxRTcnpJ3Oh3JKGmL5MKnqEs02Ww2pYsngzmFZAYNtsTN5/Myp3q9ngT3crm8dXasrutSaEfhvFQqIZlMolgsSqdRBvHBYIBcLod0Oo3vfOc7W+41riO2J8hkMlstQg4ODmQt8pxluokymYy0nyBwAiBV4syU7Bk41zKNCNwoWF3MTZxtvXnYCNs/85QztgShdvJ68RG1Na5vVjYzK2AvHX4uUoPAK/qXcYhz1E4zEdBQn1NKodls4sMPP0Sj0ZDXJB03Go1wfHwMYOOqs4OsNxlvzZmxP/ZjPyZKOIs7iCLZB4dBmakY0Tz5aKZ0vNDAK/GWQd3j8Wwhf/vitIuNRExE9pqm4fz8fKsfNSc8hSh7rxkKfUyTdV2XghzLspBKpaQ8nhsVsxciAvsB4vl8Xjr8HR4eolKpCC/K4iNSN0+ePMGv/uqv4oMPPhA3A/uqsCCI/DELYsgrk1etVCpCS+i6jnw+DwDSiIy9UMrlMpbLJYrFoqD54XCIRCIhhS8ARAzn5sG+5PP5XE7PITXDgqzRaCTI2jAMOXaRtBGRPc8oYDMt3ncGBbsvnI4o9tS5uLiQAzjs7RoMw8DBwQH+4A/+AABw//596XFycHCAXq+HwWAg/XZSqZQ0DWN7Aop1H3/8MQ4ODuTglEajIRpOLpeDx+PB5eWlWDyZOZmmKT2J2ECOdtqXL19K3xdmFMw+SWssFgupvrY3NJvP5+LuYdsOaiQEWW63G8+fPxcB3TAMKcHPZrNiEOD6uHfvngCWYrEoDc7YhoFa13q9FsqjXC7j4cOHsqlSmDZNUz4H8CoTZ0Wq3UZtp81I37DFBhFwMBhEo9GQHjI3NzcCGhaLhZwiZy9A4vxdr9fCjdMCGg6HcXV1JW3K2TqZa5tGC3sMYp0E3UE+n09cbpZliSXZLgwzPtEuW6vVZB3Tc//ixYs36oPw1lA3dr7TbjuiPxXYiIB2yyAXBLkxTm4WQDGI83d5s5gGMqCSHmEpOF0hdD6Qn3z27JnwzQyWnJQsqKAIycO6WfnJG5zL5QQNU/jj5BiPx6hUKnLzKVju7+/j/PxcfOMUiIioKH4SgfKwE1on+XlJp+i6LqiWbox2uy1CM90u5Gt5gAjRKV0+9OXncrmtAx2IUmlTZKrKbI1UGHlyNu+io4P+7Fqthnq9jocPH8rCv729RSwWk9/j5sJsB4B8FqbGmqbJqUJsqdDv9yWw+f1+fOUrX8Hv/M7vYDAYSAZiL+XniVjj8RixWEzmCjc4iua0eiaTSbRaLZTLZViWhb29PREiDcOQs1rJeTN7TKfT4mtn9SkzQrpp+BkpbGva5hBvFg1S5GZXRF7v1WolXvhcLid00XA4RDqdFtql3+/L5kLHUbPZlM2emZvD4ZDCnlarJToBaza4odspERYUsvkb+Xdmavw+XPvsOcP1SjDF+0LahEVftFVTCyLICgQC8t3v3bu3VWRJ6s4e6OmyIg3b6XTkeRcXF3JIOYESLaU0f/BvAlPgVcMze/0P/+amAkB6RM3nc/n+7ChKJoFa1M///M9/tqgbHjRAOxELkLiAaakjp85AuF6vpaCBAYaPsbCFC5fcIBG93XrocDiEv+Up8UT/TqcT5+fnW93qTNPEycmJoCieIG9PI9lkiT7mYDCI58+fSyrKEvj1ei0T6d69eyKK0T738uVLcUzE43ERw4BNUPd6vdJSFwA+/PBDmWyXl5ey+GkfY4EXeUOWsI9GI1xdXWGx2PQLonvHsjbtJEg5sMET7xeDpmVZuL6+lnNauVjtPbfZzY+BIBwOwzAM6ZeyXG7O9jw/Pxf65qOPPhIqhoGEGxa5fPvJRKx56PV64qtvNBrY29uDaZpSwcvPpus6fuVXfkXuK9Exi2b4XcjpNhoNQWiVSgWNRgP7+/tbFbbtdlvOdS0Wi/j444+lp/i9e/fQ6XRwdXWFWCwmnDmpR1oPGWBJESaTSfR6PRFnI5GI3E9WTQOvRFtuEul0GsvlUlxgrJ+4vLxEJBKRnvDValX4/W63Kx5u1nJwvpJLZzaj1OZIP/LIw+FQjkckOAMgugEb3HGjoL2U9Krb7Uaj0cDV1ZVoWnResdiK2o29UKzX60lFNAMwadrpdIr79+/j/v37ePbsGUzTlLMPCBrtg8wC7+Pp6Smurq7Q7XZF46IrjMDIXp1smqYALYr+1A3tmy/B6et2bwLfw8ND0Q15pi5NAvbusn/SeGsQPfli7tSsBAQgghuLmeieYWsEollaL2mL4kXljmpHffw5uTNyw7FYbKsLIYMyi1o4QdkvhPbFXC4nJcqcANyFy+WyfC/2CWGqt1gs0Gq1pEKVR8MRcXCz4sHNXFx0PBCxs/gLgKBlpnysdCWCPDo6QqVSkQIuXlciclq37AIo++Kz2x/tZ9fX11K8c3NzI/SaaZq4ublBMBgUMS6TyaBWq6Hb7SKdTsPv9+Ply5fSgI2l6DwAm2jn6OhIuEwGZ7ZBACBIj4HWfqQdETAbyzHYkCN1uVzSW4apvNvtln70dGYRdbINx3g8xvHxMXy+zYHkvN8UEJllsnjr5OQEpmlK6s3gTecVN9xOpyMbHrPZg4MDCVoUqklVlkolAT9HR0dyChUDMv3p7IBIeyIrkGkYGAwGWCw2fYSYVbKwyDAMlMtlPHr0SNYlr5PLtTlOklkV+9tToCXFyv4wuq5LIzYGadoTR6MRcrmczEW2HLG755hB22lT9sWhoSGZTKLdbsvzZ7OZtGDgYfMUh2nbpUOLI5fLoVqtypGSBJDMcOhY4zrmz6i9kULihshaDdYT2C3KzB5ZAU5ah1oE+0Xd3t5isVhI++JgMPjGlbFvTaDPZrPCf7P5D7lYokGW2NudHbT7AZCLyudQSWdfGXJi/H0iHvKaDBwM8Ey9GLwvLi5QKpXg9/ulYZpSCicnJ7i6ukKxWJT+HRRnuSCIhJnKsjAlEonIcXQs/mCmkU6nUavVxI5J5MPvQJGGdAuvwzvvvLNVNUfOttVqSX+PbDYrHT35XYliZrMZMpmMFA5Np1NMp1Mp3GBXTFpeWe1HJwP73jMg93o9qXyNRCIYDoeCpFlUlMlkJDOjX52bKJFrs9mUe0ZPNzMbh2NzClUulxNUeHV1JZtkMBiU06FYdOZyuXBzc4Nutyt9dXjABXUPFqEBr2y5DJicH0RtrFa0Bzmm2zzGkeCC84AojgH4+PhYaJl8Pi/XsFqtioebIioPtY7H4yLGk1rz+/3SadQwNl07eTYxf2ZvvTAej6XYii0x6LgqlUrQdV3mMOkIZmipVAqVSkWQaLvdFtGVqJudF3nmcjgcFq5+vd4cUJ/JZERgpe2X5ov/r70ryW0su7KHn+Rn3/eN2CoU4cyAAcOe2BvwajzzAgzkBrwBb8QDe+KpB850pEIdKUr87NvPvq+BcG58RbhQWagql0J4B0gkkBEpsb3vvntPQzIBbz207W40Gs/ygYFPynWv14uHhwcUCgX5//iY2Hnzedjt9mcjJtYb3j6s4jQ+zs1mI1bSZNeQymrV55AYYV3Y8rPLzycVu/w5nCbw+cdiMeRyOWle2cz81MzYF1PoycYIBALSwVtFTgBEXcbZG2d9/LLR651Fk86Ln8/z2RFxHmiz2WT5AUA4xPwyA09fIHpVeDweyWy02tkGAgHhz3Nezi87F7i8+jGBh90MOwbK/bmQodrWyvLhl4gCkO12K6HTAIQZwq7zw4cPaLVaKJfLsggEIFFsuq4LHY6dGccEq9VKFof05+aykh0gPb8/fvwoC1aGnXy+uOt0OqIwBSCzTjIxqPzja0q2EzvLcDgs1FSn8yltiR4l4XBY2CUUItEN1efzwTAM0TTw9eYijZGKpmliNBrJ37N2eYyF0zRNbkikwtLYiwX84uJCGBb83M1mM7kVcdzAbpERcdalK4U65KnzszAej+WGyaU0Rz/ValVuEt1uVz6v2+1WclO5d+KBTPEYbxTRaFRyVln4OZ+m75FpmvL68JDn94pNCf3lNe0pwvDu7g6TyUS8a+i8ySU1qZf8/nKXRfovZ+I8/NkAdTodmKYpexsu2yk04k2Zj5XkDf6zXC7lNk7wxsP3aLd7SvgKBAIiFmPDaGVJ6bouN1m+XuFwGADkc8AbNLUHn3spsYHkhIK7A+4c6Q5rGAb+8Ic/fF0zel7ZefJxm07qIxkxnKlxVserNE9IzpT54lJWbaU38TAhVcpmsz0TU1jTmrgE4ovObqler8uXkupLjgvOzs6kk7cuTq6vr4VrzH0Dl77NZlP2BjzQWEQACMVysVhIyhB/136/f+a/fzqdxNHvr3/9K4rFoizm+Hh7vR4Gg4EIuGq1mkjh+VzIgCEjgOIYOl3SSpodVjQalQ7LNE358lWrVQkXYRel67pQ28bjsQiKDMNAPp9HIBBAMpkUhSX9fGq1mrw37FAZ1cZRw2KxkH3KfD4Xw7ZIJIJoNCqGeBy/RSIRyVrlotvqdUPQt2W324m8f7PZyJezUCjA5/MhEAjg48ePIvajGZWu6ygWi3h4eBAGCNkgPGDD4TDq9TpOp5NYIrB7Y4Hh94Xc8MFgIGK8fr+Pfr+PUqmEeDwuplubzUacGnkr4tiCHH2OOxkCw+bH7Xbj7du36HQ6OB6PclPy+/3CPCGLjY+10+mg3W7Lvubvf/+7iPYYkn46ncR2od/vi4PkbvcpCKbdbiMcDqPRaGC73aJer8tBzsIdjUYRCARkWZxOp0UAxWaP3vo8WDiC5WiIt3krut2usKuAp0Afun92u12hQVNZTmJDsViUWzd9hADIZIG6IO6UrB26lUlIURl9+5n6xmXzfwcvpqPPZrPPvGvIavB4PHL15uyTQhr+m/QqcqkBiIkZC4tVwAR8Gv9QbcefTTEMu/39fi/zYnKUKaPn1Y15r+xkyZDgeINsFfKrR6ORWMhSccpRxHA4RDQalZAUHj68vRSLRSmKpVJJ5nX0kAc+eYBnMhk4HA4Mh0OxoaWwiiZd1m6Cs0arIMfn80lhOh6P4n/y448/itrweDxKl5XNZvH4+IhMJgPg076ATBkewPTR4ZV7NBrJKCoWi4nYZzqdik8656imaSKTyUiXT7dK4FP0oN1ul7n04XBAvV6Hruuyg/F6vWg0GtD1J2fPSqWCm5sbudVVKhUYhiFXbQBibkX7DACyDxoMBnC5XBI6kslkJIR7Op2KMySXbul0Gnd3d0JtdblcGI1GcrDn83lZxpZKJdk18CBmQL31lsVDhXbT1DRYNSUUbZ1OJ/R6PZyfn6NWqwmt0dphkv7L7xk/+1ZrCuDpBtntdmWHRMM5muglEgmZW3M8yM9eIpGQ59jr9ZDJZMRWm9qDdruNcrn8TIBIK2YAIjRkweUOiOMQNgMMIOJ3jXqOZDKJRqPx7L1er9df/Aw+d7LraGsRDoelsWPkJwC5gXAUczgcpJCzkWXzSrEjhZC8rfJmx+e82+1EtLjZbH5yR/9iCn2hUBBu7W63E08TLtc48mg2m8hms0IrJP+WV2p2duQx8/TlB58fXnqUcGnCjov0RX6IyWsGnm4HtVrtWTQZT+zJZCIzRy7mrEsejirm87nwuelXwqUSRRacW6ZSKRkzMFmK4PLVZrPJcpcffI4YyFThLJ0FkT4ynIUDT57rZD5wCUj2RaFQkG5oMBiIDJ8cbvLXKUGnqhJ4ouLxS2btbMm44GuXy+WER07+tfU1Xq2eQpmZN8pumNmynDVTdcz3m/+Nry9fO+v+JxAIyFhov9/LYcBFNWFVXLtcLvksMQQjHA6LWIn0Pmo3KNhhNi9fj06nI37+nKXrui5GZKS20k6As2I+J448eDBS+clULhbYSCQiBXq73aLf78vzjMfjiMfjmM/nSCQSItOnvQQ9Y+jHz3ETmxcAMuLr9/soFovw+/1y62UHzkOcY5DHx0c5CNlY8Ptot9tlKcxaYG3aqDHh94vzde5EOPrk2Ik7KlKfOVK1Mme43wOe/POtrD3qPqLRqLDU7Hb7MwsHMny47yKZhAJFNqeMxeR3lwQG2nC43W4YhiHLXrpYmqYpey0e3F9doec1mtxbfmApRLLmYvI0ZvAHFyoUsNB8iy8GjdL4by4urb4aVK1Sks7iz/mc3W7Hhw8fZORyPB6FecIllDX7k+OcRCIhplUcdXCex0MolUqh0Wg8iyWjUIyKSRYldhS8iXD3wA8a8CnE5XA4yONl8WQx4peWs2qOyEajEbbbJ59w/h0eRFSZ8n0pFoui0iQnmAWI3hykl3JJCQCPj48y7ySzhXNidmfsfsguoMqQy1TrQWS1SCDDh18yXuE5CqNRWKPRkPxOHposCq1WC9VqVRTIBDs8vr+c0Xc6HdEKAJAlK7szMq24BGVH5vV6he9+e3uL3e7JaTWfz8tnm6MFvse6ruPh4UGe02g0krhGUoPJ9uJSFIB4wHDcQw8bWgRQ2ZxOp6WA9ft9sdcg3ZcRhGTz8MBnd8qxCEditB9gR8uEKDKDSEsk1ZaHeKPRQCwWEx98dt/cGZxOT97wbPT4OGjrQAEUhWDtdhs+nw+RSEQOer4uHEN9voxlU0b1us1mk5sT3Vh56FDrw/FNt9sVhhxV11R/s4Hh0p43PTYFg8FAxoqkmf/2t7+VWxDwKZjlpxb6F6OM/cUvfoHr62uUy2UAkA8O5/CkR1UqFVFZsuiT9rjb7eSDyRktFyL8oPEqxd9BRSW/hCysXJw6HA5RTF5dXUnaDz2u+QHih4dLs2AwiMFgIPNsFkjOp/mFm81m0rWSScCxERWQ1m4+nU4jkUjg48eP0l1RbWrtzjkuodya1+HRaIR0Oi03DwqhHh4eZJYeiUSkm+UV0sqKsRap8XgsvPzNZiN8dY6IuHjkIU4a4OXlJY7Ho3DIuYgslUoyM+XthpYTZ2dnotDkiIajF2ofuNcYj8fCuyftjl0zKW0MXOaXjWNC0iWPx+OzMAormCpELxdesXkD7Pf7+Pbbb2VhzkOVtzaPxyPvG20Y2FVz1GO1zf2vUK1WJUeUzQaZPvy8WYviaDQSSi89c96/f49Go/Hsu0PK4Gw2w/v379HtdkXVnclk0O/3pVja7XYxZqPI6Pz8XGIiTdNEt9tFoVDA7e0tfv7zn4vFMG/JPp9PKLcOh0NEY1xA2mw2EUcxfpNL91gsJhGMbNZIyphMJvLaA5DGj+wjTguIQqEgf87XlWZ/VjsRKszZ1ff7fWHXkUzBUSotGJLJpHwXIpEIarUastks1uu1sHr4uYnH41I3JpOJjAAHgwFvQT9JGftiCv3/92P4KSgWi1KESfk7Hp+sZnVdFxYD8PTFu7u7k4JEC1y73S70LNIdyYs/nU4iVGFSPTuPTCYjiymOksj6YaEg4vG4dIW89pNCGo1GMRgMxH2P3WCxWMTl5SVyuZzcHmgMRXsELmbdbrcUQlImGQDBQzCXy8mti4cMD9JisYjf/e53+O677xAIBIQyaHVpDIVCz/7+arUSNkOpVJIFYDwelxsJu2raHZDOxh0JbwBkY1AUR3Wkw+GQIsgxyueuht988w2azaZ0VOzweWVnTi47abfbLfNk0vUSiQQ6nY68p0zd+p/A5XLJ+0IGGvctVMqyGeFexzRN5HI5UbIGg0F5Dfr9vhAQFosFstksDocDer2exAR+//338nkkxZYcex6i9FHiSJALXt48yDiizQajJlkQ+d3hn/P2ZqW62mw2XF1dAYB4VHGJzL0XWWsca5J9lEql8MMPP3zxevI20el0UK1W0W63pWjT4oLPibx47oZI1bYeFFZPqfV6LeMZvnd83GQb8jPFGzNvCqRqc1H7VRZ6h8OBb775RlJy+N94bdd1XTogptGQ1pbJZNBsNmU8Q5c+q58MO5r/DbBoswunERTFSOv1WuZ6rVYLx+MR5+fnImS5ubkRG2A+PhYesnxICWN3zHEO8HToABAmSy6Xw/X19f/Kc1NQUPg68FUW+ng8LjPl3W6HcrmM77//XtgAnL3vdjtUq1XprLh4Ap7mqNPp9JkPOa+Bq9UKhUIBLpcLNzc3oiwkbSsYDCIcDot17nK5/KKjIy4uLnB9fY1gMCjzb45XyCDgUul0OokpG+fIXBqRQfLw8IDpdIpcLiezeioWea3jIocHAK+dXOaRqcQRjoKCwuvGTy30LyZ4BIBwfqfTqcwqd7unuC8u16zWw/R+53IqkUjIIo2jhHQ6jcvLS0nsobNiKpUStR/ZEFymhUIhmUVaTbDIxAEgDni8ipJjfjgccHNzg0qlIgwdr9eLYDAo3Gou0CiQoFcI52/9fh/7/R6JROKZuZbL5UK5XBalIBe65OIy8EQVegUFBSteVEdPS1TOoDj3Pp1OwnnebrfCqaWCjqIC+l8AkOJKYRGVaGdnZ2i321K8i8UiarWaLDzJdecilgsg0vaISCSCxWKBTCaDTqcjXjYc3Vjn5sViUUzHuFji8pDSbDJNyDAhHZBKRfLMTdOUkRUl8fF4XIy2qLZTUFB4/fipHf2LUcYCkKUZ/bXJRDkej6I6dDqdGAwG6Ha7aDabGA6HsvXm9tzpdCKfz8uGnHxqujAy7d7lcklnTu/4XC6H6XT6LJ2JIyArGJlHP28AyOVy8vc6nQ6ATw6DsVhM2CzH4xGNRkMWjZzRH4/HZ8sqAGJyRjbS4XBAPB5/5p9ByhUFKwoKCgpWvJiOvlwuwzAMbLdbXFxcoNvtCoWKRky0PqC162QyEWl0MpmU1BjKnKnCOx6PePv2rRRHq2CEoQLkRlNQRCOvarWKq6urZzx1AEilUhJhRj44/79QKCSiIe4LKBE/Pz+HaZqiwqWDIWf9vJHwhkGaJ0c9ZIhYf/doNBKfGuvCVkFB4XXjq+voabJE/i3tUk3TxGAwEAsDClw4TmEEGimJ9KxgKDcTbj58+CBe1SygAMRHvtlsigKN7Bwq0bgLsILmZbQSBZ4UcFSzUrQynU6RTCaFksZ4MBZjumtSuESxFBWnVNOFQiERSAFP4yBK4CnMiEajIkpSUFBQIF5MoWfw9dnZmagKQ6GQJPhwRk5ByNXVlYwt0un0s3QqLkVZBK0WoIfDAclkUlg4VK2S09xut8VjIpFIoF6vi0DBikgkgnQ6Lb40mUxGnO0oSLq/vxep9Wq1ehY36PF4RCVLD/LxeCyeNfF4HKZpotVqoVKpyEKaC2BaDlD9NxqNJEpNQUFBwYoXM7qhXbDb7RZvE/pKpFIpTKdT8UrXNA3RaFTc+DiyoHCBggtmUdLSlUlLVJzm83k0m03EYjHxnKcHCfnxDOC+u7t79pgrlYpkTXo8HpGsBwIB1Go1FAoF8eimEdnxeJQEKUb35fN5PDw8yI4AgPjwuFwuZLNZmedzLMNcSSYD0WyKSlGr3aqCgsLrxVc3uuGylFJtWr8CTwvI9+/fCwWR0X2xWEzUfLSM9Xg8uL29lfAE4NPi1Ov1IhwOo9lswufzya2hWCwinU6LIx39xcmGIf/eivl8LurPw+GAUCgksnaKpKig1TQNqVRKWDSNRkO8OwCgXC6LhJzGUtwv1Go17Pd75HI5MQGj1zd/DgMKksnksz2CgoKCAvCCCj0TlSgTdjqdMAxDYsJ+85vfYLvdSpZnJpMRXxEaHWmaJsk7DocDl5eX8Pl8iMViqNfr4qVBbxI6FF5eXmK326Hf7yOfz+Pdu3divkTZ9+fcdHpuA08HyXQ6RaPRENkyLZCZH8mfTX+OcrkMj8eDbreLXq8noSu8YSUSCYTDYblR9Pt9CdGgNwj9TAzDgNvtRqvV+sJDXUFBQeHFFHoaBJFPvtvtkMlksF6v+h4uPAAAFApJREFU0Wq18N1332GxWMjcmjTG6+trMQCjfWcmk0GxWBR749PpJJ7Y0+lUnDDz+bxYhvLAOJ1OMqt3Op1ot9tirmQF7U2z2azYMgCQYIhqtSqHysPDg4x3stksisUiDocDwuEwisWiWCVQvEXrYsaYUV8AQIIONE2D1+uVmwYNkKyiLgUFBQXgBc3o6Ri4Xq/x9u3bZ5FfDNTtdDoSsxeLxWRByiVsIBCQUA6r+GowGKBQKGA+n4uwKJfLoV6viy0xGTqM5qPDIA2HHA7HsxSnYDAoLpp0PSSffzabiVKWHuqn0wmXl5cSvTcYDFCpVOTAenh4kBjB4XAI0zRRKpWw2WwwGAwQiUQkYYiOmqvVColEQkI2CoUC+v3+f2rboKCg8Lrw1c3oQ6EQotGoJNrTBoBMG87E4/E4otGoOLrl83mk02nE43GZXdOSlkk3FBQNBgMUi0WhMb5///6ZB/1oNEK73YbNZkM6nZboQWuiEEG3RIqXmGXq8XgQDocxmUzEKfLHH3/EYrFAMBhEMpnE4XDAxcWF5K4ej0cUi0XJ0aT1Kv+M4i0AwvunujYajQo98+PHj6rIKygofIEXU+i3261w2cfjsTBgaO9Ky93ZbCa2qvF4HADEC0bTNJRKJZTLZQkwMU0Tm80Gt7e3KJfLGA6HKJfLmE6n4jVOyqXdbheHyYeHB0ktYtrNvwJ92Rl+wvzOUqmEyWSCcrksNglMgGIuJ7n/9Kcm64hRfryN0K8deKKhknGkaRoeHh5kcU02koKCgoIVL2Z0w2iuzWYjHTEXprqu4927d2i1WpjP5xIGfH9/L4lJHLMwSKPf78Pj8SAUCsE0zWcBHplMBtvtFq1WC+fn59jv92g2mxJGMJlMEA6HEQqFYBgGCoUC9vu9mKcBkFQexruZpol8Po/1ei1RbsDT0jabzaLVagH4JHRiwjzzIFerlUQP0te82+1KWhTj/Zipy4AD+lTb7XaMx2O5nSgoKLx+fHWjG+Y/0t+GVEmOKu7u7iS5ye12SyBAJBKRuLv5fC7WvrlcTgq+y+VCoVCA0+kU10fGpfV6PUld2m63iMViEhm33W7FduFz2qLf74emaRKs4XQ60Ww2Zfa+3W4RDAYlfapYLEqaPQ+wfr8vxmbUAPBmQF8fXdefjbTotQ9AuPY0WAMg4dsKCgoKxIso9KVSCR6P55n/O31jVquVeLPT490wDCl+9/f3GI1Gki1qnW+HQiGJCaQvzGazQb/flzzU4/EIr9eLbreLXC4n46PVaoXhcIhSqSTUSys4S6fqlqycUqmEWq2GVColS2PDMCT6a7fbYTabodfrwe/3i4c89xDz+RymaQotkwHSbrcbZ2dn4uUDQKIM+f9XKhVMJpN/75unoKDw4vEi9PLWgO7j8YhYLCbcctIHaUnMiDOah9lsNrEhYGSdz+eToHAGd9AV8+3btxgMBrKoJQNG0zSx/PV6vUgmkxJ0MhwOJRqQYKSYNcmdfHkuVTVNw2g0wvn5udgVJBIJABDTs3w+L4ZtDA6maRlHNAxxZnany+UC8KmjbzabEnDCPFwFBQUF4kV09PSdJ1vm/v5exiy73Q7tdlt85XO5nChA6QfPtCYWQBZwh8OBUqkkzB1rbmmlUhH6Jpk6DOXlCMUwDNzf3/9LR0j64bMrn81miEajYim8WCzgdDqxWCxwe3uLH374AY+Pj5JHulwuhVVEf3zy4FerFcbjMQqFglgacBEdCoUkIPzs7AwOhwNerxelUgn1eh2RSOTf/v4pKCi8bLyIZazL5TqlUiksFgvs93uYpolKpQLDMGCz2aTjZUye3+8XwzJN0+DxeGS5abfbEQwGoWmajFd2ux28Xi82mw0Mw0AqlUKtVkMul0OtVpORx+FwkM44FAqJLz13AJyNA08HDvDEgkmlUri/v0cgEEAwGMR2u8V0OoXL5RJHyWaziUgkAo/HA6fTieFwiGKxiOl0ilQqhdPpJEtULmZ7vR7K5TLm87ksfO12OzRNw263E2/7arWK0+mEh4cHsStWUFB4/fiqlrGhUAjtdlu81qPRKHq9HiKRCJxOJ87Pz6FpGmw2mxTd4XAoaew+n09iATk2oTc9Z/ybzQY3NzcolUqw2Wwyj3/z5g2azSZ0XYfH44FpmggGgzidTnJDYIG2gv46uq6j2WwiEAggHo8/K7S8jez3ezgcDgyHQ6FDJhIJGIaBzWaD/X6P4XAoqVq0PCgUCmg0GjAMQ247nNMzYQr4ZILGg0ZBQUHBihdR6KfTKXRdl8zXWCyGeDwuxZy2CIFAQNgxmqaJffDj4yMOhwPm8zlub29xOBxgGIbcDg6HAxwOh4RwTyYTGIaBUCiE7XYrLJ7hcIhUKoVgMIhGoyFpTvzdVuz3e/R6PUSjUaFEtlot9Ho9oUK63W5st1uMRiO43W5Eo1FUKhWYponhcIjZbAZd19Fut+HxeOBwODAajdDpdCQvljP3RCKBaDQKwzDgdDplPg9AFrCMPVRQUFCw4kUUepp3sWANBgPc39+L78xkMpHRzPF4FHHTZDIRR0sKpEqlEtrtNuLxOGq1GjKZDEzThGEYGA6HoNVCqVTCarWSTv1nP/sZ0uk0DMPAbDZDIpFAIBDA8XgUJaoVnJc3Gg2JK8xms3LbWK1WsNvtMnsn9fP+/h6pVArL5RJer1fYQHa7HavVCpPJBMvlEoPBQIRjuq7DNE05fMgAshqYkeb5uW++goKCwouY0dvt9hO57fSF6XQ6uLi4wGAwkI5+uVxKcUskEphMJpjP58jn87DZbDKft/rkMJmKgeO73Q7z+Vw87q1pTwDw5s0bmKaJUCiEWq2GarWK+XwOt9v9jHXD+ToZN9vtVnzxJ5MJ/H6/BIl4vV5h5/T7fcTjcfmdfD42mw26rqNWq+FwOIiLJ/AUUrLdbvH4+AhN05DP52VhnU6nMR6P5d+73U4UvwoKCq8bX9WMPhAIiHMkQzRYrNfrtbBUNO3p4Z5OJ7EQqFarcDgcEjByfX0tMX82mw2TyUQEUqFQCKFQSEJD1us18vk8yuUywuEwyuUyGo0GwuEwrq+vUa1W0Wq10O12vyiejUZDUqv6/T78fj9msxnG4zEOhwPG4zHK5TIcDgdM00Sn08FoNILL5cLt7a2Ma7hwns/nWC6XYrjmdDoxm80wnU5lVJVKpXA4HHB3d4d0Oi23DT5PVeQVFBT+FV5ER59IJE673U6WsrFYDIFAAJqmodvtYrlcIhaLSaF0u93o9XqSMavrOrbbLRwOh3TWpVIJ/X4fuVwOm80Gk8kEXq8Xs9kMyWRSEqgYMMKQES5ZT6eTJDZFo1Esl0t0u115zKFQCPv9HrquIxgMotvtirqVo5xerycWBgAkgHy73eLdu3dYrVZySzkcDohEIuLgud/vkU6n0el05Dklk0mMx2NZWvOmQ5ZRJBLBaDRCr9f7/3orFRQU/o34qjp6GoHRG54zasMwoGkaAoEAXC4XMpkMVqsVDocDCoUC1us1lssljscj3G43vF4vcrkcXC4XRqMRbDYbrq6u0Gq1EI1GsV6v4fV6pQDTtKzZbCKdTiMcDmM4HAorZrVaicOktcgDEOVqLBYTz3oWeL/fj9FohEQiIcrcZDIJTdOw3W7h8XhkZh8MBiU1i6rg4/GIdDqNy8tLjMdj+Hw+GTXR6mA2m0niFPcVoVAIFxcX//b3T0FB4WXjRXT0xWLx1O12kU6n0Ww2YbPZxJBsPB7D4/FgvV6LQyODtGlk5nA4JNRjNBoJP56JUiykTqcT+/0e6/Uau90Ox+MR8XgczWYTqVQKXq8XjUYD8/kcZ2dnEin4X1n/UlAVi8VE2ETGTDKZFGbP8XiEpmlot9tiaNZutxEOh2VW7/P5sNlssFwu5XYxm82k43e5XJhMJmKtEAgEoOs63G63pFhZffMVFBReL76qjn4+n4upGfNXGRu43+9xPB5xOByQSqXgdrulG47H43C5XOIZTz75jz/+KDzzQCAg7pSmaYpR2Xq9lvl6pVKBx+PBhw8fkEql8ObNG8znczEf+1fw+XziPZ/P58WWgaZpx+MRNpsNvV4P8/kcq9UKs9kMjUZDtACDwQCZTAbL5VJ87SmwcjgcGI/HIvrKZrPyWq3Xa7TbbQk6GQ6Hwt1XRV5BQeFzvAivG3bp5IjTs4XzaNoURCIR/PGPf8Tvf/97rNdrPD4+wuv1YjweI5PJSOf97bff4urqCna7He12G9VqVZazpCZSXEQe+2w2k9Qoq4skF6SfZ8ZyhLJarbBarWCz2bBer8W3vlwui5/+fr/H6XTCZDJBPB6H0+lEOBzGbrcThs5ms4HNZkOj0YDNZhPv+u12K2wdHm6bzQY+n08YPxSK+f1+yY5VUFBQIF7E6CYUCp24yNztdiIeikajqNVq8Hq9Ern3/v17LJdL1Go1eDweBINBPD4+Yr/f4+zsDJ1OB5FIBH6/H81mE8ViEavVCt1uF7FYDJPJBJFIBMfjUWyR6aNTLBZxd3cnilpSNU+nE0zTfPaYK5UKlsuljFXW6zXoqT8ej8U33u/3y+I3kUjgdDo9U8GSq09e/WazQTKZxHq9hs/nk7QtKmM7nQ7K5TL2+z0eHx9RrVYBAO12G8vlUgzRFBQUXj9+6ujmRRT6TCZz6nQ6CIfDMv7gcnMymcjs/XQ6CW1xsVgIpVLXdUwmE+RyObkFaJomXXc8HpeFJQDEYjHh2E+nU+mmz8/PxYWy1+thMpnA5XIJT94KFtRYLIbD4QBN0zCbzXA8HhGJRGTGvtlskMvlMJ1O4Xa7YRgGIpGImJWFw2F4vV6cTifU63WxMEin08I64ujm8fFRnpPD4RD2UDqdxmq1wm63g67rXyyOFRQUXie+qhl9p9NBNpuFpmnIZDI4HA5YLBZwOBxYLpfIZrP49a9/LT7sHL/E43EUi0UMBgP4/X7U63Vks1mEw2FkMhnE43FUKhUAEP48Z+jL5VIolcDTzF3XdVxeXgpvn7479JSxgh33cDjEZDIRw7NkMonJZCImaNlsFqZpwuVyCYPG7/fLLWU0GsmhlkqlxP74eDyiXq8jFovB4XCg3W7DbrfD6XTK36ENxHQ6xZ/+9Cfkcrkvbh4KCgoKL6Kjv7i4OPV6PeGbc05PLnogEMAvf/lL/O1vf4PX60U0GkW9Xke5XMbNzQ0SiYTYD2+3W2QyGSwWCwnpoHMk6YzT6RSn00lsClarlSQ1sYDW63WZfcdiMXQ6nWeP2arAZfGdzWYAnm4MpFj+6le/wl/+8hecnZ3h4eFBRlAAUCgUMBwOoWkakskk6vU6AIhRG3cTrVYLiUQCuq5juVzidDrB6XRK6lQgEIDf75fAks9DUhQUFF4nvqqO/v7+HofDQQI6uFTdbDYSH/iPf/xD7H03mw2y2Sz6/T6+/fZbGbEEAgHkcjms12sRIbVaLUQiEZxOJ1mwMkw7mUxKIhXZKw8PD5LoxE5e1/UvHjNZLoFAAOv1WmyU/X4//H4/7u/vsdls8Oc//xl2ux02mw3ZbBZ2ux1v3rxBJpNBt9sVnj5HVNFoVHx/OMICnm4xhmHImIrPJx6Po9/vQ9d1OWgUFBQUrHgRhb5SqSCdTuOf//wn7HY73r17h/1+j/1+D7/fL4tGj8cjHjcUT41GIwQCATkE5vM5xuMxZrMZnE6nRAwOh0MJ8KhWq9L5BoNBOBwO5HI5Savyer3QNA37/V7cJj9HLBZDPp9Ht9uF3W6XoJHxeCwZstFoFAAkQMVqV0zfey5tyfqhhQLw5HXPkVC5XJaf7/V6YZomdF3Her1GIBBAu90Wt0wFBQUFK15Eob+6uhKLXYZ5c2HZ6XSQz+eFOcPRzGAwEHaK1+vFfD6Hy+WCrusIhUKywGW3TMuC+/t7zGYz8b7h4tUwDFnscmQTDoexWCz+ZQ5rs9kUaqTT6US5XBZXzdVqJfRQ4MmwrNPpYLVaIRqNyuOikKpcLkPTNPk9zLsNBALo9XrIZDLQNE1CTXw+H4rFIhaLhbwOPp8PmUwGXq/33/reKSgovHy8iELPhCTgyQ+GhmG73U4CwpnM5PV6JcFpPB5D0zTxZmfw936/F5MvqlbZJZN73ul0JAUqEAiIcvXh4QGLxUIshKfTKeLx+BePmbYMwBOnvl6vwzAMsUWgUyYAeS6TyQSr1QoAxL3S7/fj5uYG+Xwew+EQDodDdgeJRALJZBKDwUDm+xcXF+j3+1iv10gkEvD5fHh8fAQA1Ot1ZVOsoKDwBV7EMjaXy51sNhsMw0C1WpUF7OFwQLfbFZFQJBIRPjztEPb7PTqdDvb7PeLxOAaDAUqlEmazGebzOTabDfL5vDhf0oKAXjfsrOfzObbbrXTS/Dm73e6ZPbGCgoLCS8FXtYxttVowDAMAcHd3h9lsBq/XK3xw0zRFROV0OkWput/v0e124XK58ObNG+nM6fiYTCbld3De3+l0pOg3Gg2s12ssFgtomoa3b99iPp9jMpmIYZhacCooKHzteBEdvc1m+x8/CLvdjlKpJEEinN9Pp1PxjT+dThJFSDvkVquFb775BrPZTGbn+/0eTqcTlUoF9XpdLTgVFBReJL4qZayCgoKCwv8dXsToRkFBQUHh/w6q0CsoKCi8cqhCr6CgoPDKoQq9goKCwiuHKvQKCgoKrxyq0CsoKCi8cqhCr6CgoPDKoQq9goKCwiuHKvQKCgoKrxyq0CsoKCi8cqhCr6CgoPDKoQq9goKCwiuHKvQKCgoKrxyq0CsoKCi8cqhCr6CgoPDKoQq9goKCwiuHKvQKCgoKrxyq0CsoKCi8cqhCr6CgoPDKoQq9goKCwiuHKvQKCgoKrxyq0CsoKCi8cqhCr6CgoPDK8R8mGp2HNnk4zwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i_batch, sample_batched in enumerate(train_dl):\n",
    "    # observe 4th batch and stop.\n",
    "    if i_batch == 3:\n",
    "        plt.figure()\n",
    "        show_landmarks_batch(sample_batched)\n",
    "        plt.axis('off')\n",
    "        plt.ioff()\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esat forma hemos aumentado nuestro dataset y probaremos con una nueva red..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_end(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_end, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 6)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 6)\n",
    "        self.fc1 = nn.Linear(16 * 60 * 60, 100)\n",
    "        self.fc2 = nn.Linear(100, 60)\n",
    "        self.fc3 = nn.Linear(60, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 255, 255)\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 60 * 60)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.softmax(self.fc3(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(log_interval, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(epoch, model, device, test_loader, best_accuracy, best_epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    if best_accuracy <= correct:\n",
    "        best_accuracy = correct\n",
    "        best_epoch = epoch\n",
    "        print('This is the best test accuracy to date.')\n",
    "    print('The corresonding test accuracy : {}/{}'.format(correct, \n",
    "        len(test_loader.dataset)))\n",
    "    \n",
    "    return best_accuracy, best_epoch\n",
    "\n",
    "def validation(model, device, validation_loader, best_acc):\n",
    "    model.eval()\n",
    "    validation_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in validation_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            validation_loss += F.nll_loss(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    if best_acc <= correct:\n",
    "        best_acc = correct\n",
    "    validation_loss /= len(validation_loader.dataset)\n",
    "    print('The corresonding validation accuracy : {}/{}'.format(correct, \n",
    "        len(validation_loader.dataset)))\n",
    "    return best_acc\n",
    "\n",
    "def fit(model, params_net, epochs, schedule = False):\n",
    "    optimizer = optim.SGD(model.parameters(), **params_net)\n",
    "    best_accuracy = 0\n",
    "    best_acc = 0\n",
    "    best_epoch = 0\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        if schedule:\n",
    "            scheduler.step()\n",
    "        train(50, model, device, train_dl, optimizer, epoch)\n",
    "        best_accuracy, best_epoch = test(epoch, model, device, \n",
    "                                         test_dl, best_accuracy, best_epoch)\n",
    "        best_acc = validation(model,  device, validation_dl, best_acc)\n",
    "        \n",
    "        print('\\n')\n",
    "        \n",
    "    print(\"Finished training network.\")\n",
    "    print(\"Corresponding test accuracy of {}/{} obtained in epoch {}\".format(best_accuracy, \n",
    "                                                            len(test_dl.dataset), best_epoch))\n",
    "    print(\"Corresponding validation accuracy of {}/{}.\".format(best_acc, \n",
    "                                                            len(validation_dl.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/1200 (0%)]\tLoss: -0.333924\n",
      "Train Epoch: 1 [500/1200 (42%)]\tLoss: -0.332459\n",
      "Train Epoch: 1 [1000/1200 (83%)]\tLoss: -0.334818\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 38/100\n",
      "The corresonding validation accuracy : 25/100\n",
      "\n",
      "\n",
      "Train Epoch: 2 [0/1200 (0%)]\tLoss: -0.333835\n",
      "Train Epoch: 2 [500/1200 (42%)]\tLoss: -0.337237\n",
      "Train Epoch: 2 [1000/1200 (83%)]\tLoss: -0.334251\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 38/100\n",
      "The corresonding validation accuracy : 25/100\n",
      "\n",
      "\n",
      "Train Epoch: 3 [0/1200 (0%)]\tLoss: -0.339001\n",
      "Train Epoch: 3 [500/1200 (42%)]\tLoss: -0.334863\n",
      "Train Epoch: 3 [1000/1200 (83%)]\tLoss: -0.334923\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 38/100\n",
      "The corresonding validation accuracy : 25/100\n",
      "\n",
      "\n",
      "Train Epoch: 4 [0/1200 (0%)]\tLoss: -0.342108\n",
      "Train Epoch: 4 [500/1200 (42%)]\tLoss: -0.332740\n",
      "Train Epoch: 4 [1000/1200 (83%)]\tLoss: -0.339207\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 38/100\n",
      "The corresonding validation accuracy : 25/100\n",
      "\n",
      "\n",
      "Train Epoch: 5 [0/1200 (0%)]\tLoss: -0.332955\n",
      "Train Epoch: 5 [500/1200 (42%)]\tLoss: -0.333045\n",
      "Train Epoch: 5 [1000/1200 (83%)]\tLoss: -0.344639\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 38/100\n",
      "The corresonding validation accuracy : 25/100\n",
      "\n",
      "\n",
      "Train Epoch: 6 [0/1200 (0%)]\tLoss: -0.342536\n",
      "Train Epoch: 6 [500/1200 (42%)]\tLoss: -0.348523\n",
      "Train Epoch: 6 [1000/1200 (83%)]\tLoss: -0.336123\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 48/100\n",
      "The corresonding validation accuracy : 44/100\n",
      "\n",
      "\n",
      "Train Epoch: 7 [0/1200 (0%)]\tLoss: -0.343964\n",
      "Train Epoch: 7 [500/1200 (42%)]\tLoss: -0.339174\n",
      "Train Epoch: 7 [1000/1200 (83%)]\tLoss: -0.338385\n",
      "The corresonding test accuracy : 27/100\n",
      "The corresonding validation accuracy : 43/100\n",
      "\n",
      "\n",
      "Train Epoch: 8 [0/1200 (0%)]\tLoss: -0.336847\n",
      "Train Epoch: 8 [500/1200 (42%)]\tLoss: -0.376894\n",
      "Train Epoch: 8 [1000/1200 (83%)]\tLoss: -0.521273\n",
      "The corresonding test accuracy : 36/100\n",
      "The corresonding validation accuracy : 47/100\n",
      "\n",
      "\n",
      "Train Epoch: 9 [0/1200 (0%)]\tLoss: -0.442623\n",
      "Train Epoch: 9 [500/1200 (42%)]\tLoss: -0.444614\n",
      "Train Epoch: 9 [1000/1200 (83%)]\tLoss: -0.604778\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 60/100\n",
      "The corresonding validation accuracy : 56/100\n",
      "\n",
      "\n",
      "Train Epoch: 10 [0/1200 (0%)]\tLoss: -0.563127\n",
      "Train Epoch: 10 [500/1200 (42%)]\tLoss: -0.505599\n",
      "Train Epoch: 10 [1000/1200 (83%)]\tLoss: -0.651759\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 67/100\n",
      "The corresonding validation accuracy : 70/100\n",
      "\n",
      "\n",
      "Train Epoch: 11 [0/1200 (0%)]\tLoss: -0.810185\n",
      "Train Epoch: 11 [500/1200 (42%)]\tLoss: -0.777689\n",
      "Train Epoch: 11 [1000/1200 (83%)]\tLoss: -0.673203\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 69/100\n",
      "The corresonding validation accuracy : 64/100\n",
      "\n",
      "\n",
      "Train Epoch: 12 [0/1200 (0%)]\tLoss: -0.629381\n",
      "Train Epoch: 12 [500/1200 (42%)]\tLoss: -0.777107\n",
      "Train Epoch: 12 [1000/1200 (83%)]\tLoss: -0.700591\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 77/100\n",
      "The corresonding validation accuracy : 69/100\n",
      "\n",
      "\n",
      "Train Epoch: 13 [0/1200 (0%)]\tLoss: -0.820387\n",
      "Train Epoch: 13 [500/1200 (42%)]\tLoss: -0.692275\n",
      "Train Epoch: 13 [1000/1200 (83%)]\tLoss: -0.960841\n",
      "The corresonding test accuracy : 71/100\n",
      "The corresonding validation accuracy : 59/100\n",
      "\n",
      "\n",
      "Train Epoch: 14 [0/1200 (0%)]\tLoss: -0.801402\n",
      "Train Epoch: 14 [500/1200 (42%)]\tLoss: -0.862138\n",
      "Train Epoch: 14 [1000/1200 (83%)]\tLoss: -0.486934\n",
      "The corresonding test accuracy : 68/100\n",
      "The corresonding validation accuracy : 67/100\n",
      "\n",
      "\n",
      "Train Epoch: 15 [0/1200 (0%)]\tLoss: -0.832858\n",
      "Train Epoch: 15 [500/1200 (42%)]\tLoss: -0.764532\n",
      "Train Epoch: 15 [1000/1200 (83%)]\tLoss: -0.691121\n",
      "The corresonding test accuracy : 74/100\n",
      "The corresonding validation accuracy : 66/100\n",
      "\n",
      "\n",
      "Train Epoch: 16 [0/1200 (0%)]\tLoss: -0.737602\n",
      "Train Epoch: 16 [500/1200 (42%)]\tLoss: -0.600473\n",
      "Train Epoch: 16 [1000/1200 (83%)]\tLoss: -0.988606\n",
      "The corresonding test accuracy : 73/100\n",
      "The corresonding validation accuracy : 62/100\n",
      "\n",
      "\n",
      "Train Epoch: 17 [0/1200 (0%)]\tLoss: -0.803478\n",
      "Train Epoch: 17 [500/1200 (42%)]\tLoss: -0.886645\n",
      "Train Epoch: 17 [1000/1200 (83%)]\tLoss: -0.726445\n",
      "The corresonding test accuracy : 74/100\n",
      "The corresonding validation accuracy : 72/100\n",
      "\n",
      "\n",
      "Train Epoch: 18 [0/1200 (0%)]\tLoss: -0.897196\n",
      "Train Epoch: 18 [500/1200 (42%)]\tLoss: -0.969178\n",
      "Train Epoch: 18 [1000/1200 (83%)]\tLoss: -0.899831\n",
      "The corresonding test accuracy : 69/100\n",
      "The corresonding validation accuracy : 69/100\n",
      "\n",
      "\n",
      "Train Epoch: 19 [0/1200 (0%)]\tLoss: -0.800000\n",
      "Train Epoch: 19 [500/1200 (42%)]\tLoss: -0.832341\n",
      "Train Epoch: 19 [1000/1200 (83%)]\tLoss: -0.904219\n",
      "The corresonding test accuracy : 73/100\n",
      "The corresonding validation accuracy : 68/100\n",
      "\n",
      "\n",
      "Train Epoch: 20 [0/1200 (0%)]\tLoss: -0.768465\n",
      "Train Epoch: 20 [500/1200 (42%)]\tLoss: -0.700048\n",
      "Train Epoch: 20 [1000/1200 (83%)]\tLoss: -0.800860\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 78/100\n",
      "The corresonding validation accuracy : 68/100\n",
      "\n",
      "\n",
      "Train Epoch: 21 [0/1200 (0%)]\tLoss: -0.703664\n",
      "Train Epoch: 21 [500/1200 (42%)]\tLoss: -0.797940\n",
      "Train Epoch: 21 [1000/1200 (83%)]\tLoss: -0.831212\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 78/100\n",
      "The corresonding validation accuracy : 66/100\n",
      "\n",
      "\n",
      "Train Epoch: 22 [0/1200 (0%)]\tLoss: -0.791843\n",
      "Train Epoch: 22 [500/1200 (42%)]\tLoss: -0.800696\n",
      "Train Epoch: 22 [1000/1200 (83%)]\tLoss: -0.582770\n",
      "The corresonding test accuracy : 75/100\n",
      "The corresonding validation accuracy : 72/100\n",
      "\n",
      "\n",
      "Train Epoch: 23 [0/1200 (0%)]\tLoss: -0.699997\n",
      "Train Epoch: 23 [500/1200 (42%)]\tLoss: -0.699972\n",
      "Train Epoch: 23 [1000/1200 (83%)]\tLoss: -0.999449\n",
      "The corresonding test accuracy : 73/100\n",
      "The corresonding validation accuracy : 70/100\n",
      "\n",
      "\n",
      "Train Epoch: 24 [0/1200 (0%)]\tLoss: -0.606008\n",
      "Train Epoch: 24 [500/1200 (42%)]\tLoss: -0.896560\n",
      "Train Epoch: 24 [1000/1200 (83%)]\tLoss: -0.957691\n",
      "The corresonding test accuracy : 73/100\n",
      "The corresonding validation accuracy : 73/100\n",
      "\n",
      "\n",
      "Train Epoch: 25 [0/1200 (0%)]\tLoss: -0.966289\n",
      "Train Epoch: 25 [500/1200 (42%)]\tLoss: -0.699942\n",
      "Train Epoch: 25 [1000/1200 (83%)]\tLoss: -0.800817\n",
      "The corresonding test accuracy : 74/100\n",
      "The corresonding validation accuracy : 71/100\n",
      "\n",
      "\n",
      "Train Epoch: 26 [0/1200 (0%)]\tLoss: -0.892167\n",
      "Train Epoch: 26 [500/1200 (42%)]\tLoss: -0.867319\n",
      "Train Epoch: 26 [1000/1200 (83%)]\tLoss: -0.591957\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 79/100\n",
      "The corresonding validation accuracy : 69/100\n",
      "\n",
      "\n",
      "Train Epoch: 27 [0/1200 (0%)]\tLoss: -0.700000\n",
      "Train Epoch: 27 [500/1200 (42%)]\tLoss: -0.899826\n",
      "Train Epoch: 27 [1000/1200 (83%)]\tLoss: -0.686868\n",
      "The corresonding test accuracy : 70/100\n",
      "The corresonding validation accuracy : 57/100\n",
      "\n",
      "\n",
      "Train Epoch: 28 [0/1200 (0%)]\tLoss: -0.703118\n",
      "Train Epoch: 28 [500/1200 (42%)]\tLoss: -0.501632\n",
      "Train Epoch: 28 [1000/1200 (83%)]\tLoss: -0.503206\n",
      "The corresonding test accuracy : 77/100\n",
      "The corresonding validation accuracy : 72/100\n",
      "\n",
      "\n",
      "Train Epoch: 29 [0/1200 (0%)]\tLoss: -0.620585\n",
      "Train Epoch: 29 [500/1200 (42%)]\tLoss: -0.800001\n",
      "Train Epoch: 29 [1000/1200 (83%)]\tLoss: -0.803962\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 80/100\n",
      "The corresonding validation accuracy : 71/100\n",
      "\n",
      "\n",
      "Train Epoch: 30 [0/1200 (0%)]\tLoss: -0.591405\n",
      "Train Epoch: 30 [500/1200 (42%)]\tLoss: -0.898482\n",
      "Train Epoch: 30 [1000/1200 (83%)]\tLoss: -0.897195\n",
      "The corresonding test accuracy : 72/100\n",
      "The corresonding validation accuracy : 70/100\n",
      "\n",
      "\n",
      "Train Epoch: 31 [0/1200 (0%)]\tLoss: -0.683002\n",
      "Train Epoch: 31 [500/1200 (42%)]\tLoss: -0.698813\n",
      "Train Epoch: 31 [1000/1200 (83%)]\tLoss: -0.530940\n",
      "The corresonding test accuracy : 78/100\n",
      "The corresonding validation accuracy : 69/100\n",
      "\n",
      "\n",
      "Train Epoch: 32 [0/1200 (0%)]\tLoss: -0.799860\n",
      "Train Epoch: 32 [500/1200 (42%)]\tLoss: -0.885152\n",
      "Train Epoch: 32 [1000/1200 (83%)]\tLoss: -0.775729\n",
      "The corresonding test accuracy : 71/100\n",
      "The corresonding validation accuracy : 73/100\n",
      "\n",
      "\n",
      "Train Epoch: 33 [0/1200 (0%)]\tLoss: -0.796014\n",
      "Train Epoch: 33 [500/1200 (42%)]\tLoss: -0.880444\n",
      "Train Epoch: 33 [1000/1200 (83%)]\tLoss: -0.999967\n",
      "The corresonding test accuracy : 73/100\n",
      "The corresonding validation accuracy : 67/100\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 34 [0/1200 (0%)]\tLoss: -0.655153\n",
      "Train Epoch: 34 [500/1200 (42%)]\tLoss: -0.699993\n",
      "Train Epoch: 34 [1000/1200 (83%)]\tLoss: -0.699980\n",
      "The corresonding test accuracy : 71/100\n",
      "The corresonding validation accuracy : 67/100\n",
      "\n",
      "\n",
      "Train Epoch: 35 [0/1200 (0%)]\tLoss: -0.886640\n",
      "Train Epoch: 35 [500/1200 (42%)]\tLoss: -0.602681\n",
      "Train Epoch: 35 [1000/1200 (83%)]\tLoss: -0.700001\n",
      "The corresonding test accuracy : 66/100\n",
      "The corresonding validation accuracy : 47/100\n",
      "\n",
      "\n",
      "Train Epoch: 36 [0/1200 (0%)]\tLoss: -0.400000\n",
      "Train Epoch: 36 [500/1200 (42%)]\tLoss: -0.901402\n",
      "Train Epoch: 36 [1000/1200 (83%)]\tLoss: -0.644333\n",
      "The corresonding test accuracy : 75/100\n",
      "The corresonding validation accuracy : 61/100\n",
      "\n",
      "\n",
      "Train Epoch: 37 [0/1200 (0%)]\tLoss: -0.734326\n",
      "Train Epoch: 37 [500/1200 (42%)]\tLoss: -0.611235\n",
      "Train Epoch: 37 [1000/1200 (83%)]\tLoss: -0.799707\n",
      "This is the best test accuracy to date.\n",
      "The corresonding test accuracy : 81/100\n",
      "The corresonding validation accuracy : 67/100\n",
      "\n",
      "\n",
      "Train Epoch: 38 [0/1200 (0%)]\tLoss: -0.899996\n",
      "Train Epoch: 38 [500/1200 (42%)]\tLoss: -0.700170\n",
      "Train Epoch: 38 [1000/1200 (83%)]\tLoss: -0.802671\n",
      "The corresonding test accuracy : 78/100\n",
      "The corresonding validation accuracy : 70/100\n",
      "\n",
      "\n",
      "Train Epoch: 39 [0/1200 (0%)]\tLoss: -0.892794\n",
      "Train Epoch: 39 [500/1200 (42%)]\tLoss: -0.899991\n",
      "Train Epoch: 39 [1000/1200 (83%)]\tLoss: -0.699987\n",
      "The corresonding test accuracy : 76/100\n",
      "The corresonding validation accuracy : 72/100\n",
      "\n",
      "\n",
      "Train Epoch: 40 [0/1200 (0%)]\tLoss: -0.864506\n",
      "Train Epoch: 40 [500/1200 (42%)]\tLoss: -0.899911\n",
      "Train Epoch: 40 [1000/1200 (83%)]\tLoss: -0.844685\n",
      "The corresonding test accuracy : 72/100\n",
      "The corresonding validation accuracy : 67/100\n",
      "\n",
      "\n",
      "Train Epoch: 41 [0/1200 (0%)]\tLoss: -0.836091\n",
      "Train Epoch: 41 [500/1200 (42%)]\tLoss: -0.756424\n",
      "Train Epoch: 41 [1000/1200 (83%)]\tLoss: -0.800989\n",
      "The corresonding test accuracy : 73/100\n",
      "The corresonding validation accuracy : 68/100\n",
      "\n",
      "\n",
      "Train Epoch: 42 [0/1200 (0%)]\tLoss: -0.800000\n",
      "Train Epoch: 42 [500/1200 (42%)]\tLoss: -0.900000\n",
      "Train Epoch: 42 [1000/1200 (83%)]\tLoss: -0.599999\n",
      "The corresonding test accuracy : 72/100\n",
      "The corresonding validation accuracy : 70/100\n",
      "\n",
      "\n",
      "Train Epoch: 43 [0/1200 (0%)]\tLoss: -0.890647\n",
      "Train Epoch: 43 [500/1200 (42%)]\tLoss: -0.700024\n",
      "Train Epoch: 43 [1000/1200 (83%)]\tLoss: -0.700000\n",
      "The corresonding test accuracy : 74/100\n",
      "The corresonding validation accuracy : 75/100\n",
      "\n",
      "\n",
      "Train Epoch: 44 [0/1200 (0%)]\tLoss: -0.781458\n",
      "Train Epoch: 44 [500/1200 (42%)]\tLoss: -0.801056\n",
      "Train Epoch: 44 [1000/1200 (83%)]\tLoss: -0.689605\n",
      "The corresonding test accuracy : 79/100\n",
      "The corresonding validation accuracy : 72/100\n",
      "\n",
      "\n",
      "Train Epoch: 45 [0/1200 (0%)]\tLoss: -0.797733\n",
      "Train Epoch: 45 [500/1200 (42%)]\tLoss: -0.700009\n",
      "Train Epoch: 45 [1000/1200 (83%)]\tLoss: -0.500000\n",
      "The corresonding test accuracy : 72/100\n",
      "The corresonding validation accuracy : 70/100\n",
      "\n",
      "\n",
      "Train Epoch: 46 [0/1200 (0%)]\tLoss: -0.894680\n",
      "Train Epoch: 46 [500/1200 (42%)]\tLoss: -0.760731\n",
      "Train Epoch: 46 [1000/1200 (83%)]\tLoss: -0.899999\n",
      "The corresonding test accuracy : 78/100\n",
      "The corresonding validation accuracy : 66/100\n",
      "\n",
      "\n",
      "Train Epoch: 47 [0/1200 (0%)]\tLoss: -0.703417\n",
      "Train Epoch: 47 [500/1200 (42%)]\tLoss: -0.621774\n",
      "Train Epoch: 47 [1000/1200 (83%)]\tLoss: -0.700000\n",
      "The corresonding test accuracy : 76/100\n",
      "The corresonding validation accuracy : 65/100\n",
      "\n",
      "\n",
      "Train Epoch: 48 [0/1200 (0%)]\tLoss: -0.800000\n",
      "Train Epoch: 48 [500/1200 (42%)]\tLoss: -0.900000\n",
      "Train Epoch: 48 [1000/1200 (83%)]\tLoss: -0.900036\n",
      "The corresonding test accuracy : 73/100\n",
      "The corresonding validation accuracy : 71/100\n",
      "\n",
      "\n",
      "Train Epoch: 49 [0/1200 (0%)]\tLoss: -0.700294\n",
      "Train Epoch: 49 [500/1200 (42%)]\tLoss: -0.700636\n",
      "Train Epoch: 49 [1000/1200 (83%)]\tLoss: -0.899957\n",
      "The corresonding test accuracy : 74/100\n",
      "The corresonding validation accuracy : 62/100\n",
      "\n",
      "\n",
      "Train Epoch: 50 [0/1200 (0%)]\tLoss: -0.800000\n",
      "Train Epoch: 50 [500/1200 (42%)]\tLoss: -0.701481\n",
      "Train Epoch: 50 [1000/1200 (83%)]\tLoss: -0.885133\n",
      "The corresonding test accuracy : 62/100\n",
      "The corresonding validation accuracy : 64/100\n",
      "\n",
      "\n",
      "Train Epoch: 51 [0/1200 (0%)]\tLoss: -0.404300\n",
      "Train Epoch: 51 [500/1200 (42%)]\tLoss: -0.899976\n",
      "Train Epoch: 51 [1000/1200 (83%)]\tLoss: -0.609355\n",
      "The corresonding test accuracy : 75/100\n",
      "The corresonding validation accuracy : 59/100\n",
      "\n",
      "\n",
      "Train Epoch: 52 [0/1200 (0%)]\tLoss: -0.799999\n",
      "Train Epoch: 52 [500/1200 (42%)]\tLoss: -0.700223\n",
      "Train Epoch: 52 [1000/1200 (83%)]\tLoss: -0.849300\n",
      "The corresonding test accuracy : 72/100\n",
      "The corresonding validation accuracy : 69/100\n",
      "\n",
      "\n",
      "Train Epoch: 53 [0/1200 (0%)]\tLoss: -0.701478\n",
      "Train Epoch: 53 [500/1200 (42%)]\tLoss: -0.599964\n",
      "Train Epoch: 53 [1000/1200 (83%)]\tLoss: -0.899427\n",
      "The corresonding test accuracy : 72/100\n",
      "The corresonding validation accuracy : 69/100\n",
      "\n",
      "\n",
      "Train Epoch: 54 [0/1200 (0%)]\tLoss: -0.899998\n",
      "Train Epoch: 54 [500/1200 (42%)]\tLoss: -0.799985\n",
      "Train Epoch: 54 [1000/1200 (83%)]\tLoss: -0.899999\n",
      "The corresonding test accuracy : 80/100\n",
      "The corresonding validation accuracy : 65/100\n",
      "\n",
      "\n",
      "Train Epoch: 55 [0/1200 (0%)]\tLoss: -0.699997\n",
      "Train Epoch: 55 [500/1200 (42%)]\tLoss: -0.500001\n",
      "Train Epoch: 55 [1000/1200 (83%)]\tLoss: -0.900000\n",
      "The corresonding test accuracy : 60/100\n",
      "The corresonding validation accuracy : 68/100\n",
      "\n",
      "\n",
      "Train Epoch: 56 [0/1200 (0%)]\tLoss: -0.600000\n",
      "Train Epoch: 56 [500/1200 (42%)]\tLoss: -0.700000\n",
      "Train Epoch: 56 [1000/1200 (83%)]\tLoss: -0.900003\n",
      "The corresonding test accuracy : 76/100\n",
      "The corresonding validation accuracy : 73/100\n",
      "\n",
      "\n",
      "Train Epoch: 57 [0/1200 (0%)]\tLoss: -0.899289\n",
      "Train Epoch: 57 [500/1200 (42%)]\tLoss: -0.800076\n",
      "Train Epoch: 57 [1000/1200 (83%)]\tLoss: -0.578868\n",
      "The corresonding test accuracy : 70/100\n",
      "The corresonding validation accuracy : 60/100\n",
      "\n",
      "\n",
      "Train Epoch: 58 [0/1200 (0%)]\tLoss: -1.000000\n",
      "Train Epoch: 58 [500/1200 (42%)]\tLoss: -1.000000\n",
      "Train Epoch: 58 [1000/1200 (83%)]\tLoss: -0.797243\n",
      "The corresonding test accuracy : 74/100\n",
      "The corresonding validation accuracy : 71/100\n",
      "\n",
      "\n",
      "Train Epoch: 59 [0/1200 (0%)]\tLoss: -0.699733\n",
      "Train Epoch: 59 [500/1200 (42%)]\tLoss: -0.800334\n",
      "Train Epoch: 59 [1000/1200 (83%)]\tLoss: -0.881175\n",
      "The corresonding test accuracy : 60/100\n",
      "The corresonding validation accuracy : 60/100\n",
      "\n",
      "\n",
      "Train Epoch: 60 [0/1200 (0%)]\tLoss: -0.400000\n",
      "Train Epoch: 60 [500/1200 (42%)]\tLoss: -0.600000\n",
      "Train Epoch: 60 [1000/1200 (83%)]\tLoss: -1.000000\n",
      "The corresonding test accuracy : 53/100\n",
      "The corresonding validation accuracy : 43/100\n",
      "\n",
      "\n",
      "Finished training network.\n",
      "Corresponding test accuracy of 81/100 obtained in epoch 37\n",
      "Corresponding validation accuracy of 75/100.\n"
     ]
    }
   ],
   "source": [
    "params_net = {'lr':0.01, 'momentum':0.1}\n",
    "fit(Net_end().to(device), params_net, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
